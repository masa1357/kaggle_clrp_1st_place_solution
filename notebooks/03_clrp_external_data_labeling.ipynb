{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_clrp_external_data_labeling.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNocH8Qu3bl0DtfHJwPJ1j3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c4PRHM_swWFm"},"source":["# README\n","\n","This notebook is used to pseudo-label the additional data. Before running this notebook, you will need to run the external and competition data preparation notebooks.\n","\n","After this notebook, run the external relabeling notebook to label this data again.\n"]},{"cell_type":"markdown","metadata":{"id":"MtloB3Y39lTq"},"source":["# Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BH988BowZCE","executionInfo":{"status":"ok","timestamp":1628344115661,"user_tz":-120,"elapsed":19360,"user":{"displayName":"Mathis Lucka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhthkjGUPZoLLxPuKrqVqOhzkL6AX8O9OT0agPB=s64","userId":"17007389841511802481"}},"outputId":"bf88fad7-a61c-40f4-b443-bb270afaf585"},"source":["!pip install torch\n","!pip install transformers\n","!pip install numpy\n","!pip install pandas\n","!pip install sentence-transformers\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Collecting transformers\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 13.0 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 71.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 58.5 MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 67.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.9.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 27.3 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126709 sha256=6195049b1b8db33bd05e9e8047f761d9898e7e7a804e3d8e9fc20b6077f84fcf\n","  Stored in directory: /root/.cache/pip/wheels/d1/c1/0f/faafd427f705c4b012274ba60d9a91d75830306811e1355293\n","Successfully built sentence-transformers\n","Installing collected packages: sentencepiece, sentence-transformers\n","Successfully installed sentence-transformers-2.0.0 sentencepiece-0.1.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kLeTpzWrxWUN"},"source":["from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","from torch import nn\n","from sentence_transformers import SentenceTransformer, util\n","import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KeItphTc3x8F","executionInfo":{"status":"ok","timestamp":1628344145779,"user_tz":-120,"elapsed":22809,"user":{"displayName":"Mathis Lucka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhthkjGUPZoLLxPuKrqVqOhzkL6AX8O9OT0agPB=s64","userId":"17007389841511802481"}},"outputId":"7fc9929b-4265-4855-9e18-dc6abbf0b218"},"source":["from google.colab import drive\n","drive.mount('gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B2rfLqXC60hR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IR6vI0ykuc15"},"source":["# Constants"]},{"cell_type":"code","metadata":{"id":"jB__EVF-uenf"},"source":["BASE_PATH = 'gdrive/MyDrive/Lit'\n","PSEUDO_LABEL_MODEL_PATH = os.path.join(BASE_PATH, 'models/roberta-base')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRW0J5Y462Sb"},"source":["def seed_everything(seed=1234):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","SEED = 28\n","seed_everything(seed=SEED)\n","MAX_LENGTH = 256"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxlIQpd1un1x"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"VQG27hNjrL5l"},"source":["def search_similar_passages(queries, search_selection, top_k, model_name='paraphrase-TinyBERT-L6-v2', cuts=None):\n","  model = SentenceTransformer(model_name)\n","  bank = []\n","  sentences = []\n","  for dataset in search_selection:\n","    in_dir = os.path.join(BASE_PATH, 'embeddings/encoded-' + dataset + '-' + model_name + '.pt')\n","    if os.path.isfile(in_dir):\n","      encoded = torch.load(in_dir)\n","      bank.extend(encoded)\n","    else:\n","      raise FileNotFoundError(f'{dataset} embeddings could not be found.')\n","    data_dir = os.path.join(BASE_PATH, 'data/preprocessed/' + dataset + '.csv')\n","    if os.path.isfile(data_dir):\n","      sents = pd.read_csv(data_dir)\n","      sents = sents.text.values\n","      sentences.extend(sents)\n","    else:\n","      raise FileNotFoundError(f'{dataset} passages could not be found.')\n","    assert len(bank) == len(sentences)\n","\n","  print(f'Starting to search within {len(sentences)} text fragments...')\n","  \n","  encoded_queries = model.encode(queries, convert_to_tensor=True)\n","\n","  hits = util.semantic_search(encoded_queries, bank, top_k=top_k, corpus_chunk_size=80000)\n","  selected = []\n","  for hit in hits:\n","    sents = [sentences[h['corpus_id']] for h in hit]\n","    if cuts:\n","      sents = sents[cuts[0]:cuts[1]]\n","    selected.append(sents)\n","\n","  return selected\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lagv6uH3vo62"},"source":["def zip_hits_scores(hits, scores, stdev):\n","  zipped = []\n","  for idx, hit in enumerate(hits):\n","    current = [(h, scores[idx], stdev[idx]) for h in hit]\n","    zipped.extend(current)\n","  return zipped\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RCtumYzvrzz"},"source":["def filter_on_stdev(sentences, predictions, scores, stdev):\n","  pred_filtered = []\n","  sents_filtered = []\n","  for idx, pred in enumerate(predictions):\n","    dev = stdev[idx]\n","    gt = scores[idx]\n","    diff = abs(pred-gt)\n","    if diff < dev:\n","      pred_filtered.append(pred)\n","      sents_filtered.append(sentences[idx])\n","  \n","  return sents_filtered, pred_filtered"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgF-nPh6w5AD"},"source":["def generate_augmented_data(fold_dir, model_dir, out_dir, n_samples=5, kfolds=[0, 1, 2, 3, 4, 5]):\n","  for fold in kfolds:\n","    torch.cuda.empty_cache()\n","    train_fold = pd.read_csv(fold_dir + '/train_fold_' + str(fold) + '.csv')\n","    val_fold = pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '.csv')\n","    queries = [str(t) for t in train_fold.excerpt.values]\n","    scores = [float(t) for t in train_fold.target.values]\n","    stdev = [float(t) for t in train_fold.standard_error.values]\n","    corpora = ['simplewiki', 'cb_corpus', 'wiki_snippets', 'onestop', 'asb', 'kaggle_scraped', 'bookcorpus_02']\n","    hits = search_similar_passages(queries, corpora, n_samples)\n","    zipped = zip_hits_scores(hits, scores, stdev)\n","    sentences = [t[0] for t in zipped]\n","    scores = [t[1] for t in zipped]\n","    stdev = [t[2] for t in zipped]\n","    torch.cuda.empty_cache()\n","    predictions = predict_fast(model_dir + '/model_fold_' + str(fold) + '/best', sentences)\n","    print(len(predictions))\n","\n","    sents_filtered, preds_filtered = filter_on_stdev(sentences, predictions, scores, stdev)\n","    augmented_df = pd.DataFrame.from_dict({'excerpt': sents_filtered, 'target': preds_filtered})\n","    augmented_df.to_csv(out_dir + '/predicted.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wzHLOB9w5-X"},"source":["def predict_fast(model_name=None, data=None, init_model=None, tokenizer=None, num_labels=1, is_multilabel=False, output_logits=False, use_softmax=False):\n","  device = \"cuda:0\"\n","  tokenizer = AutoTokenizer.from_pretrained(model_name) if model_name else tokenizer\n","  config = AutoConfig.from_pretrained(model_name, num_labels=num_labels) if model_name else None\n","  model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config) if model_name else init_model\n","  model.to(device)\n","  model.eval()\n","  y_pred = []\n","  batches = chunks(data, 32)\n","  for batch in tqdm(batches):\n","    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n","    input_ids = inputs['input_ids'].to(device)\n","    attention = inputs['attention_mask'].to(device)\n","    inputs = {\n","        'input_ids': input_ids,\n","        'attention_mask': attention\n","    }\n","    with torch.no_grad():        \n","          outputs = model(**inputs)\n","    if not use_softmax:\n","      logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n","    else:\n","      logits = nn.functional.softmax(outputs.logits, dim=-1).detach().cpu().numpy().squeeze().tolist()\n","    if is_multilabel and not output_logits:\n","      logits = np.argmax(logits, axis=-1)\n","    y_pred.extend(logits)\n","\n","  return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gw_ED7B7x_F1"},"source":["def chunks(lst, n):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), n):\n","        yield lst[i:i + n]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZFTyzUGqyJxw"},"source":["# Labeling"]},{"cell_type":"code","metadata":{"id":"aQYnRHqqyL48"},"source":["fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n","model_dir = os.path.join(BASE_PATH, 'models/roberta-base')\n","out_dir = os.path.join(BASE_PATH, 'data/training/predicted/predicted.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KM2V4ydT3EWY","executionInfo":{"status":"ok","timestamp":1628345477525,"user_tz":-120,"elapsed":210264,"user":{"displayName":"Mathis Lucka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhthkjGUPZoLLxPuKrqVqOhzkL6AX8O9OT0agPB=s64","userId":"17007389841511802481"}},"outputId":"16c01813-94b9-46b9-a299-0344729fb4c6"},"source":["generate_augmented_data(fold_dir=fold_dir, model_dir=model_dir, out_dir=out_dir, kfolds=[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting to search within 2161598 text fragments...\n"],"name":"stdout"},{"output_type":"stream","text":["369it [00:56,  6.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["11805\n"],"name":"stdout"}]}]}