{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "final-rebel",
   "metadata": {
    "papermill": {
     "duration": 0.014943,
     "end_time": "2021-09-10T06:57:09.970767",
     "exception": false,
     "start_time": "2021-09-10T06:57:09.955824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook combines two models:\n",
    "\n",
    "Score 0.467: [https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch](https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch)\n",
    "\n",
    "Score 0.468: [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-fabric",
   "metadata": {
    "papermill": {
     "duration": 0.01362,
     "end_time": "2021-09-10T06:57:09.998193",
     "exception": false,
     "start_time": "2021-09-10T06:57:09.984573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handy-filling",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:10.030610Z",
     "iopub.status.busy": "2021-09-10T06:57:10.029088Z",
     "iopub.status.idle": "2021-09-10T06:57:12.634953Z",
     "shell.execute_reply": "2021-09-10T06:57:12.633899Z",
     "shell.execute_reply.started": "2021-08-16T23:15:37.341087Z"
    },
    "papermill": {
     "duration": 2.623157,
     "end_time": "2021-09-10T06:57:12.635120",
     "exception": false,
     "start_time": "2021-09-10T06:57:10.011963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coated-telephone",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:12.712631Z",
     "iopub.status.busy": "2021-09-10T06:57:12.711763Z",
     "iopub.status.idle": "2021-09-10T06:57:12.714721Z",
     "shell.execute_reply": "2021-09-10T06:57:12.715219Z",
     "shell.execute_reply.started": "2021-08-16T23:15:40.113082Z"
    },
    "papermill": {
     "duration": 0.06392,
     "end_time": "2021-09-10T06:57:12.715401",
     "exception": false,
     "start_time": "2021-09-10T06:57:12.651481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 248\n",
    "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "ROBERTA_PATH = \"/kaggle/input/roberta-base\"\n",
    "TOKENIZER_PATH = \"/kaggle/input/roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8d0d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/masa1357/git/kaggle_clrp_1st_place_solution\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!cd /home/masa1357/git/kaggle_clrp_1st_place_solution\n",
    "test_df = pd.read_csv(\"pred.csv\")\n",
    "submission_df = pd.read_csv(\"pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "growing-federation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:12.759377Z",
     "iopub.status.busy": "2021-09-10T06:57:12.758778Z",
     "iopub.status.idle": "2021-09-10T06:57:12.874652Z",
     "shell.execute_reply": "2021-09-10T06:57:12.873704Z",
     "shell.execute_reply.started": "2021-08-16T23:15:40.205131Z"
    },
    "papermill": {
     "duration": 0.142989,
     "end_time": "2021-09-10T06:57:12.874807",
     "exception": false,
     "start_time": "2021-09-10T06:57:12.731818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_df = pd.read_csv(\"../input/clrp-pseudo-labeled-data-v1/train_fold_1_corrected.csv\")\n",
    "#submission_df = pd.read_csv(\"../input/clrp-pseudo-labeled-data-v1/train_fold_1_corrected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "public-economics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:12.908784Z",
     "iopub.status.busy": "2021-09-10T06:57:12.908156Z",
     "iopub.status.idle": "2021-09-10T06:57:13.122340Z",
     "shell.execute_reply": "2021-09-10T06:57:13.121818Z",
     "shell.execute_reply.started": "2021-08-16T23:15:40.328023Z"
    },
    "papermill": {
     "duration": 0.232788,
     "end_time": "2021-09-10T06:57:13.122482",
     "exception": false,
     "start_time": "2021-09-10T06:57:12.889694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/roberta-base'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(TOKENIZER_PATH)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:642\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    641\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    643\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    644\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:486\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 486\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    487\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    488\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    489\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    490\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    491\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    492\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    493\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    494\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    495\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    496\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    497\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    499\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    500\u001b[0m )\n\u001b[1;32m    501\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:112\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[1;32m    108\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[1;32m    110\u001b[0m ):\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 112\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m    114\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRepo id must be a string, not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(repo_id)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    167\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/roberta-base'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-vinyl",
   "metadata": {
    "papermill": {
     "duration": 0.027892,
     "end_time": "2021-09-10T06:57:13.168015",
     "exception": false,
     "start_time": "2021-09-10T06:57:13.140123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-highlight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:13.241841Z",
     "iopub.status.busy": "2021-09-10T06:57:13.240888Z",
     "iopub.status.idle": "2021-09-10T06:57:13.245601Z",
     "shell.execute_reply": "2021-09-10T06:57:13.244942Z",
     "shell.execute_reply.started": "2021-08-16T23:15:40.532687Z"
    },
    "papermill": {
     "duration": 0.05268,
     "end_time": "2021-09-10T06:57:13.245783",
     "exception": false,
     "start_time": "2021-09-10T06:57:13.193103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-blast",
   "metadata": {
    "papermill": {
     "duration": 0.027942,
     "end_time": "2021-09-10T06:57:13.300483",
     "exception": false,
     "start_time": "2021-09-10T06:57:13.272541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-currency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:13.365600Z",
     "iopub.status.busy": "2021-09-10T06:57:13.364440Z",
     "iopub.status.idle": "2021-09-10T06:57:13.369415Z",
     "shell.execute_reply": "2021-09-10T06:57:13.370621Z",
     "shell.execute_reply.started": "2021-08-16T23:15:40.542607Z"
    },
    "papermill": {
     "duration": 0.048394,
     "end_time": "2021-09-10T06:57:13.370844",
     "exception": false,
     "start_time": "2021-09-10T06:57:13.322450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-individual",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:13.432554Z",
     "iopub.status.busy": "2021-09-10T06:57:13.431654Z",
     "iopub.status.idle": "2021-09-10T06:57:13.436197Z",
     "shell.execute_reply": "2021-09-10T06:57:13.437540Z",
     "shell.execute_reply.started": "2021-08-16T23:15:40.555089Z"
    },
    "papermill": {
     "duration": 0.040761,
     "end_time": "2021-09-10T06:57:13.437746",
     "exception": false,
     "start_time": "2021-09-10T06:57:13.396985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-analyst",
   "metadata": {
    "papermill": {
     "duration": 0.024111,
     "end_time": "2021-09-10T06:57:13.486683",
     "exception": false,
     "start_time": "2021-09-10T06:57:13.462572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-lesbian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:13.549310Z",
     "iopub.status.busy": "2021-09-10T06:57:13.543035Z",
     "iopub.status.idle": "2021-09-10T06:57:15.908498Z",
     "shell.execute_reply": "2021-09-10T06:57:15.907972Z",
     "shell.execute_reply.started": "2021-08-16T23:15:40.570765Z"
    },
    "papermill": {
     "duration": 2.397033,
     "end_time": "2021-09-10T06:57:15.908639",
     "exception": false,
     "start_time": "2021-09-10T06:57:13.511606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = LitDataset(test_df, inference_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-attempt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T06:57:15.944139Z",
     "iopub.status.busy": "2021-09-10T06:57:15.943353Z",
     "iopub.status.idle": "2021-09-10T07:01:09.362742Z",
     "shell.execute_reply": "2021-09-10T07:01:09.363849Z",
     "shell.execute_reply.started": "2021-08-16T23:15:42.762983Z"
    },
    "papermill": {
     "duration": 233.441393,
     "end_time": "2021-09-10T07:01:09.364074",
     "exception": false,
     "start_time": "2021-09-10T06:57:15.922681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_1.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_2.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_3.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_4.pth\n",
      "\n",
      "Using ../input/commonlit-roberta-0467/model_5.pth\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in range(NUM_MODELS):            \n",
    "    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-decision",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T07:01:09.422435Z",
     "iopub.status.busy": "2021-09-10T07:01:09.421582Z",
     "iopub.status.idle": "2021-09-10T07:01:09.423754Z",
     "shell.execute_reply": "2021-09-10T07:01:09.423112Z",
     "shell.execute_reply.started": "2021-08-16T23:19:39.656699Z"
    },
    "papermill": {
     "duration": 0.033873,
     "end_time": "2021-09-10T07:01:09.423915",
     "exception": false,
     "start_time": "2021-09-10T07:01:09.390042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-swimming",
   "metadata": {
    "papermill": {
     "duration": 0.024279,
     "end_time": "2021-09-10T07:01:09.473069",
     "exception": false,
     "start_time": "2021-09-10T07:01:09.448790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2\n",
    "Imported from [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-aside",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T07:01:09.534114Z",
     "iopub.status.busy": "2021-09-10T07:01:09.533161Z",
     "iopub.status.idle": "2021-09-10T07:06:17.699550Z",
     "shell.execute_reply": "2021-09-10T07:06:17.698125Z"
    },
    "papermill": {
     "duration": 308.201963,
     "end_time": "2021-09-10T07:06:17.699720",
     "exception": false,
     "start_time": "2021-09-10T07:01:09.497757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:02<04:10, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:03<03:04, 61.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [03:03<02:02, 61.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [04:03<01:00, 60.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:03<00:00, 60.79s/it]\n"
     ]
    }
   ],
   "source": [
    "test = test_df\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import RobertaConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt, label = self.excerpts[item], self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.double),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "def make_model(model_name, num_labels=1):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    config = RobertaConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "):\n",
    "    \n",
    "    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size // 2, \n",
    "        sampler=test_sampler, \n",
    "        pin_memory=False, \n",
    "        drop_last=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def evaluate(self, data_loader, tokenizer):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids']\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda()\n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids\n",
    "                    )\n",
    "                \n",
    "                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
    "                preds += logits\n",
    "        return preds\n",
    "\n",
    "def config(fold, model_name, load_model_path):\n",
    "    torch.manual_seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.cuda.manual_seed_all(2021)\n",
    "    \n",
    "    max_len = 250\n",
    "    batch_size = 8\n",
    "\n",
    "    model, tokenizer = make_model(\n",
    "        model_name=model_name, \n",
    "        num_labels=1\n",
    "    )\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{load_model_path}/model{fold}.bin')\n",
    "    )\n",
    "    test_loader = make_loader(\n",
    "        test, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        test_loader, scaler\n",
    "    )\n",
    "\n",
    "def run(fold=0, model_name=None, load_model_path=None):\n",
    "    model, tokenizer, \\\n",
    "        test_loader, scaler = config(fold, model_name, load_model_path)\n",
    "    \n",
    "    import time\n",
    "\n",
    "    evaluator = Evaluator(model, scaler)\n",
    "\n",
    "    test_time_list = []\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "\n",
    "    preds = evaluator.evaluate(test_loader, tokenizer)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time() \n",
    "    test_time_list.append(tic2 - tic1)\n",
    "    \n",
    "    del model, tokenizer, test_loader, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds\n",
    "\n",
    "pred_df1 = pd.DataFrame()\n",
    "for fold in tqdm(range(5)):\n",
    "    pred_df1[f'fold{fold}'] = run(fold, '../input/roberta-base/', '../input/commonlit-roberta-base-i/')\n",
    "    #pred_df2[f'fold{fold+5}'] = run(fold, '../input/robertalarge/', '../input/roberta-large-itptfit/')\n",
    "    #pred_df3[f'fold{fold+10}'] = run(fold, '../input/robertalarge/', '../input/commonlit-roberta-large-ii/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-mailing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T07:06:17.743376Z",
     "iopub.status.busy": "2021-09-10T07:06:17.742808Z",
     "iopub.status.idle": "2021-09-10T07:20:24.605755Z",
     "shell.execute_reply": "2021-09-10T07:20:24.604235Z"
    },
    "papermill": {
     "duration": 846.886123,
     "end_time": "2021-09-10T07:20:24.605890",
     "exception": false,
     "start_time": "2021-09-10T07:06:17.719767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [03:00<12:03, 180.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [05:47<08:36, 172.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [08:33<05:39, 169.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [11:20<02:48, 168.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [14:06<00:00, 169.37s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_df2 = pd.DataFrame()\n",
    "for fold in tqdm(range(5)):\n",
    "    pred_df2[f'fold{fold+5}'] = run(fold, '../input/robertalarge/', '../input/roberta-large-itptfit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-cache",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T07:20:24.654475Z",
     "iopub.status.busy": "2021-09-10T07:20:24.653920Z",
     "iopub.status.idle": "2021-09-10T07:34:18.153783Z",
     "shell.execute_reply": "2021-09-10T07:34:18.152188Z"
    },
    "papermill": {
     "duration": 833.526359,
     "end_time": "2021-09-10T07:34:18.153985",
     "exception": false,
     "start_time": "2021-09-10T07:20:24.627626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:45<11:01, 165.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [05:31<08:17, 165.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [08:19<05:33, 166.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [11:06<02:46, 166.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [13:53<00:00, 166.69s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_df3 = pd.DataFrame()\n",
    "for fold in tqdm(range(5)):\n",
    "    pred_df3[f'fold{fold+10}'] = run(fold, '../input/robertalarge/', '../input/commonlit-roberta-large-ii/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-lloyd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T07:34:18.213653Z",
     "iopub.status.busy": "2021-09-10T07:34:18.212535Z",
     "iopub.status.idle": "2021-09-10T07:34:18.217779Z",
     "shell.execute_reply": "2021-09-10T07:34:18.217323Z"
    },
    "papermill": {
     "duration": 0.037044,
     "end_time": "2021-09-10T07:34:18.217909",
     "exception": false,
     "start_time": "2021-09-10T07:34:18.180865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df1 = np.array(pred_df1)\n",
    "pred_df2 = np.array(pred_df2)\n",
    "pred_df3 = np.array(pred_df3)\n",
    "model2_predictions = (pred_df2.mean(axis=1)*0.5) + (pred_df1.mean(axis=1)*0.3) + (pred_df3.mean(axis=1) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-stadium",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T07:34:18.273287Z",
     "iopub.status.busy": "2021-09-10T07:34:18.272774Z",
     "iopub.status.idle": "2021-09-10T07:34:18.276844Z",
     "shell.execute_reply": "2021-09-10T07:34:18.276441Z"
    },
    "papermill": {
     "duration": 0.03344,
     "end_time": "2021-09-10T07:34:18.276974",
     "exception": false,
     "start_time": "2021-09-10T07:34:18.243534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model1_predictions * 0.5 + model2_predictions * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-capacity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T07:34:18.344131Z",
     "iopub.status.busy": "2021-09-10T07:34:18.343533Z",
     "iopub.status.idle": "2021-09-10T07:34:18.669301Z",
     "shell.execute_reply": "2021-09-10T07:34:18.668439Z"
    },
    "papermill": {
     "duration": 0.366794,
     "end_time": "2021-09-10T07:34:18.669479",
     "exception": false,
     "start_time": "2021-09-10T07:34:18.302685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 0.1  \\\n",
      "0              0             0   \n",
      "1              1             1   \n",
      "2              2             2   \n",
      "3              3             3   \n",
      "4              4             4   \n",
      "...          ...           ...   \n",
      "4497        4497          4522   \n",
      "4498        4498          4523   \n",
      "4499        4499          4524   \n",
      "4500        4500          4525   \n",
      "4501        4501          4526   \n",
      "\n",
      "                                                excerpt    target  \n",
      "0     known you to turn down my meatloaf.\" With a lo... -0.539207  \n",
      "1     bowl in front of her, placed a sandwich on a s... -0.914280  \n",
      "2     away. Cathy came back to the table. \"Is there ... -0.447072  \n",
      "3     hoping that would be enough to satisfy Susanna... -0.218283  \n",
      "4     ye up.\" She smiled at Dolly, now eating a bowl... -1.120444  \n",
      "...                                                 ...       ...  \n",
      "4497  In some parts of the Arctic the ground is cove...  0.413902  \n",
      "4498  Tundras do not get very much rain or snow. How...  0.158152  \n",
      "4499  Solid is one of the three common states of mat... -0.173139  \n",
      "4500  These are three-dimensional shapes. Their side... -0.166454  \n",
      "4501  Most animals are mobile, meaning they can move...  0.181251  \n",
      "\n",
      "[4502 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "submission_df.target = predictions\n",
    "print(submission_df)\n",
    "submission_df.to_csv(\"./predicted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2238.222206,
   "end_time": "2021-09-10T07:34:21.654098",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-10T06:57:03.431892",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
