{"cells":[{"cell_type":"markdown","metadata":{"id":"SKAX5C8gAJxT"},"source":["# Intro\n","\n","This notebook contains code to prepare any additional data that I used during the competition. For each dataset, I performed some preprocessing and then transformed the data to sentence embeddings."]},{"cell_type":"markdown","metadata":{"id":"fre87Ny_AJxW"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:25:55.975113Z","iopub.status.busy":"2021-08-06T19:25:55.974604Z","iopub.status.idle":"2021-08-06T19:30:21.616528Z","shell.execute_reply":"2021-08-06T19:30:21.615321Z","shell.execute_reply.started":"2021-08-06T19:25:55.974974Z"},"id":"ma5qUjE0AJxX","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.5.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n","Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n","Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.1)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n","Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.26.3)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.3.23)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (0.42.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.3.23)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.14.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.28.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.29.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (59.6.0)\n","Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.37.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.26.3)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.3.23)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.29.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Install dependencies\n","%pip install pandas\n","%pip install numpy\n","%pip install matplotlib\n","%pip install seaborn\n","%pip install h5py\n","%pip install torch\n","%pip install scipy\n","%pip install sacremoses\n","%pip install sentencepiece\n","%pip install jieba\n","%pip install numpy\n","%pip install nltk\n","%pip install sentence-transformers\n","%pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:21.620946Z","iopub.status.busy":"2021-08-06T19:30:21.620590Z","iopub.status.idle":"2021-08-06T19:30:30.517727Z","shell.execute_reply":"2021-08-06T19:30:30.516515Z","shell.execute_reply.started":"2021-08-06T19:30:21.620910Z"},"id":"76RWMJbeAJxY","trusted":true},"outputs":[],"source":["# Import dependencies\n","import gzip\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","plt.style.use('ggplot')\n","import seaborn as sns\n","import re\n","import math\n","import torch\n","from scipy.stats import truncnorm\n","from tqdm import tqdm\n","from sentence_transformers import SentenceTransformer, util\n","from pathlib import Path\n","from datasets import load_dataset, concatenate_datasets\n","import gc\n","gc.enable()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rZWvodEntfD8"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('gdrive')"]},{"cell_type":"markdown","metadata":{"id":"cdrg0ARyAJxY"},"source":["# Constants"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.520683Z","iopub.status.busy":"2021-08-06T19:30:30.520238Z","iopub.status.idle":"2021-08-06T19:30:30.526596Z","shell.execute_reply":"2021-08-06T19:30:30.525031Z","shell.execute_reply.started":"2021-08-06T19:30:30.520639Z"},"id":"EKydXLEiAJxZ","trusted":true},"outputs":[],"source":["BASE_INPUT = '/home/masa1357/git/kaggle_clrp_1st_place_solution'\n","BASE_OUTPUT = '/home/masa1357/git/kaggle_clrp_1st_place_solution'"]},{"cell_type":"markdown","metadata":{"id":"R8L-nIaeAJxZ"},"source":["# Functions"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.529739Z","iopub.status.busy":"2021-08-06T19:30:30.529129Z","iopub.status.idle":"2021-08-06T19:30:30.538626Z","shell.execute_reply":"2021-08-06T19:30:30.537620Z","shell.execute_reply.started":"2021-08-06T19:30:30.529694Z"},"id":"XNcrpinYAJxZ","trusted":true},"outputs":[],"source":["def create_dir_if_not_exist(out_dir):\n","    output_dir = Path(out_dir)\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","    return output_dir"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.540946Z","iopub.status.busy":"2021-08-06T19:30:30.540211Z","iopub.status.idle":"2021-08-06T19:30:30.550093Z","shell.execute_reply":"2021-08-06T19:30:30.548848Z","shell.execute_reply.started":"2021-08-06T19:30:30.540900Z"},"id":"zAUsYdcbAJxa","trusted":true},"outputs":[],"source":["# a utility function to save a pandas dataframe to csv\n","# it will create directories if they don't exist\n","def df_to_csv(df, out_dir, out_file):\n","    output_dir = create_dir_if_not_exist(os.path.join(BASE_OUTPUT, out_dir))\n","    df.to_csv(output_dir / out_file)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.552514Z","iopub.status.busy":"2021-08-06T19:30:30.551921Z","iopub.status.idle":"2021-08-06T19:30:30.563591Z","shell.execute_reply":"2021-08-06T19:30:30.561087Z","shell.execute_reply.started":"2021-08-06T19:30:30.552471Z"},"id":"7zsIY-0EAJxb","trusted":true},"outputs":[],"source":["def encode_and_save(sentences, out_dir, data_name, scores=None, model_name='paraphrase-TinyBERT-L6-v2'):\n","  model = SentenceTransformer(model_name)\n","\n","  encoded = model.encode(sentences, convert_to_tensor=True)\n","  output_dir = create_dir_if_not_exist(os.path.join(BASE_OUTPUT, out_dir))  \n","  out_file = os.path.join(output_dir, 'encoded-' + data_name + '-' + model_name + '.pt')\n","  pairs = []\n","  for idx, sent in enumerate(sentences):\n","    pair = [sent, encoded[idx]]\n","    if scores:\n","      pair.append(score[idx])\n","  with open(out_file, 'wb') as f:\n","    torch.save(encoded, f)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.565871Z","iopub.status.busy":"2021-08-06T19:30:30.565319Z","iopub.status.idle":"2021-08-06T19:30:30.573940Z","shell.execute_reply":"2021-08-06T19:30:30.572606Z","shell.execute_reply.started":"2021-08-06T19:30:30.565827Z"},"id":"htB1uaTVAJxc","trusted":true},"outputs":[],"source":["def get_simple_wiki():\n","    simplewiki_path = os.path.join(BASE_OUTPUT, 'data/external/simplewiki-2020-11-01.jsonl.gz')\n","    if not os.path.exists(simplewiki_path):\n","        util.http_get('https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/simplewiki-2020-11-01.jsonl.gz', simplewiki_path)\n","    passages = []\n","    with gzip.open(simplewiki_path, 'rt', encoding='utf8') as fIn:\n","        for line in fIn:\n","            data = json.loads(line.strip())\n","            passages.extend(data['paragraphs'])\n","    return passages"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.579790Z","iopub.status.busy":"2021-08-06T19:30:30.578660Z","iopub.status.idle":"2021-08-06T19:30:30.589398Z","shell.execute_reply":"2021-08-06T19:30:30.588186Z","shell.execute_reply.started":"2021-08-06T19:30:30.579741Z"},"id":"PD7meGCVAJxc","trusted":true},"outputs":[],"source":["def truncated_normal(mean=180, sd=17, low=135, high=205):\n","    \"\"\"\n","    Return a number that belong to a normal distribution\n","    \n","    Parameters:\n","    -----------\n","    \n","    mean: (int/float)\n","        Mean of the distribution\n","        \n","    sd: (int/float)\n","        Standard deviation of the distribution\n","        \n","    low: (int/float)\n","        Lowest number fo the distribution\n","        \n","    high: (int/float)\n","    \"\"\"\n","    return truncnorm( (low - mean) / sd, (high - mean) / sd, loc=mean, scale=sd ).rvs()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.593523Z","iopub.status.busy":"2021-08-06T19:30:30.592789Z","iopub.status.idle":"2021-08-06T19:30:30.601438Z","shell.execute_reply":"2021-08-06T19:30:30.600312Z","shell.execute_reply.started":"2021-08-06T19:30:30.593451Z"},"id":"bXL2yFzEAJxd","trusted":true},"outputs":[],"source":["def chunks(lst, n):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), n):\n","        yield lst[i:i + n]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.604096Z","iopub.status.busy":"2021-08-06T19:30:30.603329Z","iopub.status.idle":"2021-08-06T19:30:30.612268Z","shell.execute_reply":"2021-08-06T19:30:30.610962Z","shell.execute_reply.started":"2021-08-06T19:30:30.604049Z"},"id":"D5eeAXN1AJxd","trusted":true},"outputs":[],"source":["def get_trainset_word_distribution(text):\n","  words = text.split()\n","  cut = math.floor(truncated_normal())\n","  chunked = chunks(words, cut)\n","  texts = [' '.join(c) for c in chunked]\n","  return texts"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.614641Z","iopub.status.busy":"2021-08-06T19:30:30.614334Z","iopub.status.idle":"2021-08-06T19:30:30.624219Z","shell.execute_reply":"2021-08-06T19:30:30.622790Z","shell.execute_reply.started":"2021-08-06T19:30:30.614612Z"},"id":"q1qwwUtQAJxd","trusted":true},"outputs":[],"source":["def clean_file(file):\n","  attribution = ''\n","  texts = []\n","  attribution_start = False\n","  current_text = ''\n","  max_len = truncated_normal()\n","  for ln in file:\n","    line = ln.strip()\n","    if line != '':\n","      if re.search('free to download', line) or attribution_start:\n","        attribution = attribution + ' ' + line \n","        attribution_start = True\n","      else:\n","        if len(current_text) < max_len:\n","          current_text = current_text + ' ' + line\n","        else:\n","          texts.append(current_text)\n","          current_text = line\n","          max_len = truncated_normal()\n","  attributions = [attribution for _ in texts]\n","  return texts, attributions"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.626866Z","iopub.status.busy":"2021-08-06T19:30:30.625992Z","iopub.status.idle":"2021-08-06T19:30:30.637468Z","shell.execute_reply":"2021-08-06T19:30:30.636352Z","shell.execute_reply.started":"2021-08-06T19:30:30.626820Z"},"id":"lYe582JUAJxe","trusted":true},"outputs":[],"source":["def get_cb_corpus():\n","    in_dir = os.path.join(BASE_INPUT, 'data/external/cb_corpus.txt')\n","    chapters = []\n","    current_chapter = []\n","    \n","    with open(in_dir, 'r') as f:\n","        for line in tqdm(f):\n","          ln = line.strip()\n","          if ln[:7] == 'CHAPTER':\n","            chapters.append(current_chapter)\n","            current_chapter = []\n","          elif not re.match(r'_BOOK_TITLE_|-LCB-|-RCB-', ln) and ln != '':\n","            rand_div = truncated_normal()\n","            curr_len = len(' '.join(current_chapter).split(' '))\n","            if curr_len < rand_div:\n","              current_chapter.append(ln)\n","            else:\n","              chapters.append(current_chapter)\n","              current_chapter = []\n","    return chapters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-m803VNuAJxf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"n55xb-TkAJxf"},"source":["# Wikipedia data\n","\n","This data contains text snippets from Wikipedia. It was downloaded from https://huggingface.co/datasets/wikitext and some preprocessing was applied."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:30:30.641679Z","iopub.status.busy":"2021-08-06T19:30:30.641278Z","iopub.status.idle":"2021-08-06T19:46:41.094211Z","shell.execute_reply":"2021-08-06T19:46:41.093092Z","shell.execute_reply.started":"2021-08-06T19:30:30.641641Z"},"id":"qkKIHyRKAJxg","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7eab690a823428baaad29323bf37763","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-0de99981bdd52981.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e5aeeafa46a2d172.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-96efade93a1a7f5a.arrow\n"]},{"data":{"text/plain":["0"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# download the dataset\n","wikitext_dataset = load_dataset('wikitext', 'wikitext-103-v1')\n","\n","# apply some preprocessing\n","wikitext_train = wikitext_dataset['train']\n","wikitext_train = wikitext_train.filter(lambda example: len(example['text'])>100)\n","\n","def replace_n(example):\n","  example['text'] = example['text'].replace('\\n', ' ')\n","  return example\n","\n","wikitext_train = wikitext_train.map(replace_n)\n","\n","# we only want samples between 600 and 1100 characters\n","wikitext_train = wikitext_train.filter(lambda example: len(example['text']) < 1100 and len(example['text']) > 600)\n","\n","# convert the dataset to a dataframe and save it\n","wikitext_train_pd = wikitext_train.to_pandas()\n","df_to_csv(df=wikitext_train_pd, out_dir='data/preprocessed', out_file='wiki_snippets.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","wiki_snippets = wikitext_train_pd.text.tolist()\n","encode_and_save(sentences=wiki_snippets, out_dir='embeddings', data_name='wiki_snippets')\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"0UZQ-LFEAJxh"},"source":["# SimpleWiki data\n","\n","This data contains snippets from Simple Wiki. It was downloaded from https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/simplewiki-2020-11-01.jsonl.gz"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:46:41.097398Z","iopub.status.busy":"2021-08-06T19:46:41.097062Z","iopub.status.idle":"2021-08-06T19:56:43.826303Z","shell.execute_reply":"2021-08-06T19:56:43.825081Z","shell.execute_reply.started":"2021-08-06T19:46:41.097361Z"},"id":"evHHrlLAAJxh","trusted":true},"outputs":[],"source":["simplewiki_snippets = get_simple_wiki()\n","\n","# filter out snippets which are too long\n","simplewiki_filtered = [p for p in simplewiki_snippets if len(p) < 1200]\n","\n","# convert the dataset to a dataframe and save it\n","simple_df = pd.DataFrame(simplewiki_filtered, columns=['text'])\n","df_to_csv(df=simple_df, out_dir='data/preprocessed', out_file='simplewiki.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","encode_and_save(sentences=simplewiki_filtered, out_dir='embeddings', data_name='simplewiki') "]},{"cell_type":"markdown","metadata":{"id":"tiqi73kKAJxi"},"source":["# Bookcorpus data\n","This data contains part of the book corpus. It was downloaded from https://huggingface.co/datasets/bookcorpusopen\n","\n","**Please note:**\n","\n","Due to processing resource limitations, only 20% of the bookcorpus dataset were selected. I made the selection randomly. The code can still be used to see how I preprocessed the data, but the resulting selection may produce different results during model training."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2021-08-06T19:56:43.830939Z","iopub.status.busy":"2021-08-06T19:56:43.830445Z"},"id":"Kzh-uWzZAJxi","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset bookcorpusopen (/root/.cache/huggingface/datasets/bookcorpusopen/plain_text/1.0.0/98559c92eb612e150a676c5b5131f9f8f07d4cab88e7f3761fda266ad22ff2a7)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63e7c6a754b548bc846bd941c015056b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"882291eeb3544ebeaa176689ad68e3db","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/17868 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82aecebfd79b4a9193b8037724fc12cc","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/6736669 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# load the dataset\n","bookcorpus = load_dataset('bookcorpusopen')\n","\n","# apply some preprocessing\n","bookcorpus = bookcorpus['train'].remove_columns('title')\n","\n","def process_batch(batch):\n","  out = []\n","  for text in batch['text']:\n","    out.extend(get_trainset_word_distribution(text))\n","  return {'text': out}\n","\n","bookcorpus_chunked = bookcorpus.map(process_batch, batched=True)\n","bookcorpus_chunked = bookcorpus_chunked.filter(lambda example: len(example['text']) < 1200)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'dict'>\n"]}],"source":["\n","# convert to pandas, select 20% and save\n","bookcorpus_df = bookcorpus_chunked.to_pandas()\n","msk = np.random.rand(len(bookcorpus)) < 0.2\n","bookcorpus_02 = bookcorpus[msk]\n","print(type(bookcorpus_02))\n","bookcorpus_02 = pd.DataFrame(bookcorpus_02)\n","df = bookcorpus_02\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["\n","df_to_csv(df, out_dir='data/preprocessed', out_file='bookcorpus.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","bookcorpus_texts = bookcorpus_02.text.tolist()\n","encode_and_save(bookcorpus_texts, out_dir='embeddings', data_name='bookcorpus')\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"gDd4xEIuAJxi"},"source":["# African Storybooks data\n","\n","This data was downloaded manually from https://www.africanstorybook.org/ .\n","I downloaded all books starting from letter A up to and including letter D.\n","The downloaded books were converted from .epub to .txt using Calibre (`ebook-convert input.epub output.txt`).\n","\n","The full bash script used to convert the books:\n","```\n","#!/bin/bash\n","for filename in *.epub; do\n","        ebook-convert $filename \"$filename.txt\"\n","done\n","```\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ga6UklouAJxj","trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"stack expects a non-empty TensorList","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# convert the dataset to sentence embeddings and save the result\u001b[39;00m\n\u001b[1;32m     17\u001b[0m asb_sents \u001b[39m=\u001b[39m asb_df\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> 18\u001b[0m encode_and_save(sentences\u001b[39m=\u001b[39;49masb_sents, out_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m'\u001b[39;49m, data_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39masb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n","Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mencode_and_save\u001b[0;34m(sentences, out_dir, data_name, scores, model_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_and_save\u001b[39m(sentences, out_dir, data_name, scores\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, model_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mparaphrase-TinyBERT-L6-v2\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m   model \u001b[39m=\u001b[39m SentenceTransformer(model_name)\n\u001b[0;32m----> 4\u001b[0m   encoded \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(sentences, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m   output_dir \u001b[39m=\u001b[39m create_dir_if_not_exist(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(BASE_OUTPUT, out_dir))  \n\u001b[1;32m      6\u001b[0m   out_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, \u001b[39m'\u001b[39m\u001b[39mencoded-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m data_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py:195\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    192\u001b[0m all_embeddings \u001b[39m=\u001b[39m [all_embeddings[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39margsort(length_sorted_idx)]\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m convert_to_tensor:\n\u001b[0;32m--> 195\u001b[0m     all_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(all_embeddings)\n\u001b[1;32m    196\u001b[0m \u001b[39melif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m    197\u001b[0m     all_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([emb\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m all_embeddings])\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"]}],"source":["# read in the data and clean the texts\n","in_dir = os.path.join(BASE_INPUT, 'data/external/a_d_txt')\n","all_texts = []\n","all_attributions = []\n","for file in os.listdir(in_dir):\n","  with open(os.path.join(in_dir, file), 'r') as f:\n","    txt, attr = clean_file(f)\n","    if txt != '' and attr != '':    \n","      all_texts.extend(txt)\n","      all_attributions.extend(attr)\n","\n","# create and save as pandas dataframe\n","asb_df = pd.DataFrame.from_dict({'text': all_texts, 'attribution': all_attributions})\n","df_to_csv(df=asb_df, out_dir='data/preprocessed', out_file='asb.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","asb_sents = asb_df.text.tolist()\n","encode_and_save(sentences=asb_sents, out_dir='embeddings', data_name='asb')\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"sbhS_InuAJxj"},"source":["# Scraped data\n","This dataset contains scraped data from wikipedia, wikibooks, simplewiki and kids.frontiersin.org. It was taken from https://www.kaggle.com/teeyee314/readability-url-scrape."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vEk46ZmAJxk","trusted":true},"outputs":[],"source":["in_dir = os.path.join(BASE_INPUT, 'data/external/external_df.csv')\n","scraped_data = pd.read_csv(in_dir)\n","\n","txts = []\n","for txt in scraped_data.usable_external.values:\n","  txts.extend(get_trainset_word_distribution(txt))\n","\n","scraped_df = pd.DataFrame(txts, columns=['text'])\n","df_to_csv(df=scraped_df, out_dir='data/preprocessed', out_file='kaggle_scraped.csv')\n","\n","scraped_sents = scraped_df.text.tolist()\n","encode_and_save(sentences=scraped_sents, out_dir='embeddings', data_name='kaggle_scraped')\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"biCYWNXQAJxk"},"source":["# Onestop Corpus data\n","This dataset was downloaded from https://huggingface.co/datasets/onestop_english"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NNxfhHGAJxl","trusted":true},"outputs":[],"source":["onestop_data = load_dataset('onestop_english')\n","onestop_data = onestop_data['train']\n","onestop_df = onestop_data.to_pandas()\n","\n","df_to_csv(df=onestop_df, out_dir='data/preprocessed', out_file='onestop.csv')\n","\n","onestop_sents = onestop_df.text.tolist()\n","encode_and_save(sentences=onestop_sents, out_dir='embeddings', data_name='onestop')\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"Hl-rPWnuAJxl"},"source":["# CC News data\n","This dataset was downloaded from https://huggingface.co/datasets/cc_news"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gfXRfNUAJxl","trusted":true},"outputs":[],"source":["news_data = load_dataset('cc_news')\n","news_data = news_data['train']\n","news_data = news_data.filter(lambda example: len(example['text']) < 1200)\n","news_df = pd.DataFrame(news_data['text'], columns=['text'])\n","\n","df_to_csv(df=news_df, out_dir='data/preprocessed', out_file='news.csv')\n","\n","news_sents = news_df.text.tolist()\n","encode_and_save(sentences=news_sents, out_dir='embeddings', data_name='news')\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"pfD6GfmIAJxm"},"source":["# Children's book corpus data\n","This dataset was downloaded from https://research.fb.com/downloads/babi/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2gSUV5vAJxm","trusted":true},"outputs":[],"source":["cb_corpus = get_cb_corpus()\n","cb_corpus = [' '.join(c) for c in cb_corpus]\n","cb_corpus = pd.DataFrame(cb_corpus, columns=['text'])\n","cb_corpus.drop([0])\n","\n","df_to_csv(df=cb_corpus, out_dir='data/preprocessed', out_file='cb_corpus.csv')\n","\n","cb_sents = cb_corpus.text.tolist()\n","encode_and_save(sentences=cb_sents, out_dir='embeddings', data_name='cb_corpus')\n","gc.collect()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"02_clrp_external_data_prep.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
