{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"02_clrp_external_data_prep.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"SKAX5C8gAJxT"},"source":["# Intro\n","\n","This notebook contains code to prepare any additional data that I used during the competition. For each dataset, I performed some preprocessing and then transformed the data to sentence embeddings."]},{"cell_type":"markdown","metadata":{"id":"fre87Ny_AJxW"},"source":["# Setup"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:25:55.974604Z","iopub.execute_input":"2021-08-06T19:25:55.975113Z","iopub.status.idle":"2021-08-06T19:30:21.616528Z","shell.execute_reply.started":"2021-08-06T19:25:55.974974Z","shell.execute_reply":"2021-08-06T19:30:21.615321Z"},"trusted":true,"id":"ma5qUjE0AJxX"},"source":["# Install dependencies\n","!pip install pandas\n","!pip install numpy\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install h5py\n","!pip install torch\n","!pip install scipy\n","!pip install sacremoses\n","!pip install sentencepiece\n","!pip install jieba\n","!pip install numpy\n","!pip install nltk\n","!pip install sentence-transformers\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:21.620590Z","iopub.execute_input":"2021-08-06T19:30:21.620946Z","iopub.status.idle":"2021-08-06T19:30:30.517727Z","shell.execute_reply.started":"2021-08-06T19:30:21.620910Z","shell.execute_reply":"2021-08-06T19:30:30.516515Z"},"trusted":true,"id":"76RWMJbeAJxY"},"source":["# Import dependencies\n","import gzip\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","plt.style.use('ggplot')\n","import seaborn as sns\n","import re\n","import math\n","import torch\n","from scipy.stats import truncnorm\n","from tqdm import tqdm\n","from sentence_transformers import SentenceTransformer, util\n","from pathlib import Path\n","from datasets import load_dataset, concatenate_datasets\n","import gc\n","gc.enable()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZWvodEntfD8"},"source":["from google.colab import drive\n","drive.mount('gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cdrg0ARyAJxY"},"source":["# Constants"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.520238Z","iopub.execute_input":"2021-08-06T19:30:30.520683Z","iopub.status.idle":"2021-08-06T19:30:30.526596Z","shell.execute_reply.started":"2021-08-06T19:30:30.520639Z","shell.execute_reply":"2021-08-06T19:30:30.525031Z"},"trusted":true,"id":"EKydXLEiAJxZ"},"source":["BASE_INPUT = 'gdrive/MyDrive/Lit/Lit_Submission'\n","BASE_OUTPUT = 'gdrive/MyDrive/Lit/Lit_Submission'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8L-nIaeAJxZ"},"source":["# Functions"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.529129Z","iopub.execute_input":"2021-08-06T19:30:30.529739Z","iopub.status.idle":"2021-08-06T19:30:30.538626Z","shell.execute_reply.started":"2021-08-06T19:30:30.529694Z","shell.execute_reply":"2021-08-06T19:30:30.537620Z"},"trusted":true,"id":"XNcrpinYAJxZ"},"source":["def create_dir_if_not_exist(out_dir):\n","    output_dir = Path(out_dir)\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","    return output_dir"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.540211Z","iopub.execute_input":"2021-08-06T19:30:30.540946Z","iopub.status.idle":"2021-08-06T19:30:30.550093Z","shell.execute_reply.started":"2021-08-06T19:30:30.540900Z","shell.execute_reply":"2021-08-06T19:30:30.548848Z"},"trusted":true,"id":"zAUsYdcbAJxa"},"source":["# a utility function to save a pandas dataframe to csv\n","# it will create directories if they don't exist\n","def df_to_csv(df, out_dir, out_file):\n","    output_dir = create_dir_if_not_exist(os.path.join(BASE_OUTPUT, out_dir))\n","    df.to_csv(output_dir / out_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.551921Z","iopub.execute_input":"2021-08-06T19:30:30.552514Z","iopub.status.idle":"2021-08-06T19:30:30.563591Z","shell.execute_reply.started":"2021-08-06T19:30:30.552471Z","shell.execute_reply":"2021-08-06T19:30:30.561087Z"},"trusted":true,"id":"7zsIY-0EAJxb"},"source":["def encode_and_save(sentences, out_dir, data_name, scores=None, model_name='paraphrase-TinyBERT-L6-v2'):\n","  model = SentenceTransformer(model_name)\n","\n","  encoded = model.encode(sentences, convert_to_tensor=True)\n","  output_dir = create_dir_if_not_exist(os.path.join(BASE_OUTPUT, out_dir))  \n","  out_file = os.path.join(output_dir, 'encoded-' + data_name + '-' + model_name + '.pt')\n","  pairs = []\n","  for idx, sent in enumerate(sentences):\n","    pair = [sent, encoded[idx]]\n","    if scores:\n","      pair.append(score[idx])\n","  with open(out_file, 'wb') as f:\n","    torch.save(encoded, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.565319Z","iopub.execute_input":"2021-08-06T19:30:30.565871Z","iopub.status.idle":"2021-08-06T19:30:30.573940Z","shell.execute_reply.started":"2021-08-06T19:30:30.565827Z","shell.execute_reply":"2021-08-06T19:30:30.572606Z"},"trusted":true,"id":"htB1uaTVAJxc"},"source":["def get_simple_wiki():\n","    simplewiki_path = os.path.join(BASE_OUTPUT, 'data/external/simplewiki-2020-11-01.jsonl.gz')\n","    if not os.path.exists(simplewiki_path):\n","        util.http_get('https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/simplewiki-2020-11-01.jsonl.gz', simplewiki_path)\n","    passages = []\n","    with gzip.open(simplewiki_path, 'rt', encoding='utf8') as fIn:\n","        for line in fIn:\n","            data = json.loads(line.strip())\n","            passages.extend(data['paragraphs'])\n","    return passages"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.578660Z","iopub.execute_input":"2021-08-06T19:30:30.579790Z","iopub.status.idle":"2021-08-06T19:30:30.589398Z","shell.execute_reply.started":"2021-08-06T19:30:30.579741Z","shell.execute_reply":"2021-08-06T19:30:30.588186Z"},"trusted":true,"id":"PD7meGCVAJxc"},"source":["def truncated_normal(mean=180, sd=17, low=135, high=205):\n","    \"\"\"\n","    Return a number that belong to a normal distribution\n","    \n","    Parameters:\n","    -----------\n","    \n","    mean: (int/float)\n","        Mean of the distribution\n","        \n","    sd: (int/float)\n","        Standard deviation of the distribution\n","        \n","    low: (int/float)\n","        Lowest number fo the distribution\n","        \n","    high: (int/float)\n","    \"\"\"\n","    return truncnorm( (low - mean) / sd, (high - mean) / sd, loc=mean, scale=sd ).rvs()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.592789Z","iopub.execute_input":"2021-08-06T19:30:30.593523Z","iopub.status.idle":"2021-08-06T19:30:30.601438Z","shell.execute_reply.started":"2021-08-06T19:30:30.593451Z","shell.execute_reply":"2021-08-06T19:30:30.600312Z"},"trusted":true,"id":"bXL2yFzEAJxd"},"source":["def chunks(lst, n):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), n):\n","        yield lst[i:i + n]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.603329Z","iopub.execute_input":"2021-08-06T19:30:30.604096Z","iopub.status.idle":"2021-08-06T19:30:30.612268Z","shell.execute_reply.started":"2021-08-06T19:30:30.604049Z","shell.execute_reply":"2021-08-06T19:30:30.610962Z"},"trusted":true,"id":"D5eeAXN1AJxd"},"source":["def get_trainset_word_distribution(text):\n","  words = text.split()\n","  cut = math.floor(truncated_normal())\n","  chunked = chunks(words, cut)\n","  texts = [' '.join(c) for c in chunked]\n","  return texts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.614334Z","iopub.execute_input":"2021-08-06T19:30:30.614641Z","iopub.status.idle":"2021-08-06T19:30:30.624219Z","shell.execute_reply.started":"2021-08-06T19:30:30.614612Z","shell.execute_reply":"2021-08-06T19:30:30.622790Z"},"trusted":true,"id":"q1qwwUtQAJxd"},"source":["def clean_file(file):\n","  attribution = ''\n","  texts = []\n","  attribution_start = False\n","  current_text = ''\n","  max_len = truncated_normal()\n","  for ln in file:\n","    line = ln.strip()\n","    if line != '':\n","      if re.search('free to download', line) or attribution_start:\n","        attribution = attribution + ' ' + line \n","        attribution_start = True\n","      else:\n","        if len(current_text) < max_len:\n","          current_text = current_text + ' ' + line\n","        else:\n","          texts.append(current_text)\n","          current_text = line\n","          max_len = truncated_normal()\n","  attributions = [attribution for _ in texts]\n","  return texts, attributions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.625992Z","iopub.execute_input":"2021-08-06T19:30:30.626866Z","iopub.status.idle":"2021-08-06T19:30:30.637468Z","shell.execute_reply.started":"2021-08-06T19:30:30.626820Z","shell.execute_reply":"2021-08-06T19:30:30.636352Z"},"trusted":true,"id":"lYe582JUAJxe"},"source":["def get_cb_corpus():\n","    in_dir = os.path.join(BASE_INPUT, 'data/external/cb_corpus.txt')\n","    chapters = []\n","    current_chapter = []\n","    \n","    with open(in_dir, 'r') as f:\n","        for line in tqdm(f):\n","          ln = line.strip()\n","          if ln[:7] == 'CHAPTER':\n","            chapters.append(current_chapter)\n","            current_chapter = []\n","          elif not re.match(r'_BOOK_TITLE_|-LCB-|-RCB-', ln) and ln != '':\n","            rand_div = truncated_normal()\n","            curr_len = len(' '.join(current_chapter).split(' '))\n","            if curr_len < rand_div:\n","              current_chapter.append(ln)\n","            else:\n","              chapters.append(current_chapter)\n","              current_chapter = []\n","    return chapters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-m803VNuAJxf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n55xb-TkAJxf"},"source":["# Wikipedia data\n","\n","This data contains text snippets from Wikipedia. It was downloaded from https://huggingface.co/datasets/wikitext and some preprocessing was applied."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:30:30.641278Z","iopub.execute_input":"2021-08-06T19:30:30.641679Z","iopub.status.idle":"2021-08-06T19:46:41.094211Z","shell.execute_reply.started":"2021-08-06T19:30:30.641641Z","shell.execute_reply":"2021-08-06T19:46:41.093092Z"},"trusted":true,"id":"qkKIHyRKAJxg"},"source":["# download the dataset\n","wikitext_dataset = load_dataset('wikitext', 'wikitext-103-v1')\n","\n","# apply some preprocessing\n","wikitext_train = wikitext_dataset['train']\n","wikitext_train = wikitext_train.filter(lambda example: len(example['text'])>100)\n","\n","def replace_n(example):\n","  example['text'] = example['text'].replace('\\n', ' ')\n","  return example\n","\n","wikitext_train = wikitext_train.map(replace_n)\n","\n","# we only want samples between 600 and 1100 characters\n","wikitext_train = wikitext_train.filter(lambda example: len(example['text']) < 1100 and len(example['text']) > 600)\n","\n","# convert the dataset to a dataframe and save it\n","wikitext_train_pd = wikitext_train.to_pandas()\n","df_to_csv(df=wikitext_train_pd, out_dir='data/preprocessed', out_file='wiki_snippets.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","wiki_snippets = wikitext_train_pd.text.tolist()\n","encode_and_save(sentences=wiki_snippets, out_dir='embeddings', data_name='wiki_snippets')\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UZQ-LFEAJxh"},"source":["# SimpleWiki data\n","\n","This data contains snippets from Simple Wiki. It was downloaded from https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/simplewiki-2020-11-01.jsonl.gz"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:46:41.097062Z","iopub.execute_input":"2021-08-06T19:46:41.097398Z","iopub.status.idle":"2021-08-06T19:56:43.826303Z","shell.execute_reply.started":"2021-08-06T19:46:41.097361Z","shell.execute_reply":"2021-08-06T19:56:43.825081Z"},"trusted":true,"id":"evHHrlLAAJxh"},"source":["simplewiki_snippets = get_simple_wiki()\n","\n","# filter out snippets which are too long\n","simplewiki_filtered = [p for p in simplewiki_snippets if len(p) < 1200]\n","\n","# convert the dataset to a dataframe and save it\n","simple_df = pd.DataFrame(simplewiki_filtered, columns=['text'])\n","df_to_csv(df=simple_df, out_dir='data/preprocessed', out_file='simplewiki.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","encode_and_save(sentences=simplewiki_filtered, out_dir='embeddings', data_name='simplewiki') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tiqi73kKAJxi"},"source":["# Bookcorpus data\n","This data contains part of the book corpus. It was downloaded from https://huggingface.co/datasets/bookcorpusopen\n","\n","**Please note:**\n","\n","Due to processing resource limitations, only 20% of the bookcorpus dataset were selected. I made the selection randomly. The code can still be used to see how I preprocessed the data, but the resulting selection may produce different results during model training."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-06T19:56:43.830445Z","iopub.execute_input":"2021-08-06T19:56:43.830939Z"},"trusted":true,"id":"Kzh-uWzZAJxi"},"source":["# load the dataset\n","bookcorpus = load_dataset('bookcorpusopen')\n","\n","# apply some preprocessing\n","bookcorpus = bookcorpus['train'].remove_columns('title')\n","\n","def process_batch(batch):\n","  out = []\n","  for text in batch['text']:\n","    out.extend(get_trainset_word_distribution(text))\n","  return {'text': out}\n","\n","bookcorpus_chunked = bookcorpus.map(process_batch, batched=True)\n","bookcorpus_chunked = bookcorpus_chunked.filter(lambda example: len(example['text']) < 1200)\n","\n","# convert to pandas, select 20% and save\n","bookcorpus_df = bookcorpus_chunked.to_pandas()\n","msk = np.random.rand(len(bookcorpus)) < 0.2\n","bookcorpus_02 = bookcorpus[msk]\n","df_to_csv(df=bookcorpus_02, out_dir='data/preprocessed', out_file='bookcorpus.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","bookcorpus_texts = bookcorpus_02.text.tolist()\n","encode_and_save(sentences=bookcorpus_texts, out_dir='embeddings', data_name='bookcorpus')\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gDd4xEIuAJxi"},"source":["# African Storybooks data\n","\n","This data was downloaded manually from https://www.africanstorybook.org/ .\n","I downloaded all books starting from letter A up to and including letter D.\n","The downloaded books were converted from .epub to .txt using Calibre (`ebook-convert input.epub output.txt`).\n","\n","The full bash script used to convert the books:\n","```\n","#!/bin/bash\n","for filename in *.epub; do\n","        ebook-convert $filename \"$filename.txt\"\n","done\n","```\n"]},{"cell_type":"code","metadata":{"trusted":true,"id":"ga6UklouAJxj"},"source":["# read in the data and clean the texts\n","in_dir = os.path.join(BASE_INPUT, 'data/external/a_d_txt')\n","all_texts = []\n","all_attributions = []\n","for file in os.listdir(in_dir):\n","  with open(os.path.join(in_dir, file), 'r') as f:\n","    txt, attr = clean_file(f)\n","    if txt != '' and attr != '':    \n","      all_texts.extend(txt)\n","      all_attributions.extend(attr)\n","\n","# create and save as pandas dataframe\n","asb_df = pd.DataFrame.from_dict({'text': all_texts, 'attribution': all_attributions})\n","df_to_csv(df=asb_df, out_dir='data/preprocessed', out_file='asb.csv')\n","\n","# convert the dataset to sentence embeddings and save the result\n","asb_sents = asb_df.text.tolist()\n","encode_and_save(sentences=asb_sents, out_dir='embeddings', data_name='asb')\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbhS_InuAJxj"},"source":["# Scraped data\n","This dataset contains scraped data from wikipedia, wikibooks, simplewiki and kids.frontiersin.org. It was taken from https://www.kaggle.com/teeyee314/readability-url-scrape."]},{"cell_type":"code","metadata":{"trusted":true,"id":"5vEk46ZmAJxk"},"source":["in_dir = os.path.join(BASE_INPUT, 'data/external/external_df.csv')\n","scraped_data = pd.read_csv(in_dir)\n","\n","txts = []\n","for txt in scraped_data.usable_external.values:\n","  txts.extend(get_trainset_word_distribution(txt))\n","\n","scraped_df = pd.DataFrame(txts, columns=['text'])\n","df_to_csv(df=scraped_df, out_dir='data/preprocessed', out_file='kaggle_scraped.csv')\n","\n","scraped_sents = scraped_df.text.tolist()\n","encode_and_save(sentences=scraped_sents, out_dir='embeddings', data_name='kaggle_scraped')\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biCYWNXQAJxk"},"source":["# Onestop Corpus data\n","This dataset was downloaded from https://huggingface.co/datasets/onestop_english"]},{"cell_type":"code","metadata":{"trusted":true,"id":"_NNxfhHGAJxl"},"source":["onestop_data = load_dataset('onestop_english')\n","onestop_data = onestop_data['train']\n","onestop_df = onestop_data.to_pandas()\n","\n","df_to_csv(df=onestop_df, out_dir='data/preprocessed', out_file='onestop.csv')\n","\n","onestop_sents = onestop_df.text.tolist()\n","encode_and_save(sentences=onestop_sents, out_dir='embeddings', data_name='onestop')\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hl-rPWnuAJxl"},"source":["# CC News data\n","This dataset was downloaded from https://huggingface.co/datasets/cc_news"]},{"cell_type":"code","metadata":{"trusted":true,"id":"6gfXRfNUAJxl"},"source":["news_data = load_dataset('cc_news')\n","news_data = news_data['train']\n","news_data = news_data.filter(lambda example: len(example['text']) < 1200)\n","news_df = pd.DataFrame(news_data['text'], columns=['text'])\n","\n","df_to_csv(df=news_df, out_dir='data/preprocessed', out_file='news.csv')\n","\n","news_sents = news_df.text.tolist()\n","encode_and_save(sentences=news_sents, out_dir='embeddings', data_name='news')\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pfD6GfmIAJxm"},"source":["# Children's book corpus data\n","This dataset was downloaded from https://research.fb.com/downloads/babi/"]},{"cell_type":"code","metadata":{"trusted":true,"id":"l2gSUV5vAJxm"},"source":["cb_corpus = get_cb_corpus()\n","cb_corpus = [' '.join(c) for c in cb_corpus]\n","cb_corpus = pd.DataFrame(cb_corpus, columns=['text'])\n","cb_corpus.drop([0])\n","\n","df_to_csv(df=cb_corpus, out_dir='data/preprocessed', out_file='cb_corpus.csv')\n","\n","cb_sents = cb_corpus.text.tolist()\n","encode_and_save(sentences=cb_sents, out_dir='embeddings', data_name='cb_corpus')\n","gc.collect()"],"execution_count":null,"outputs":[]}]}