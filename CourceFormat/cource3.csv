userid,1,2,3,4,5,target
B-2020_U10,,"""Data fabrication, falsification, and plagiarism are illegal acts that should never be done.
When citing, the reference must be clearly stated. ""","""I fell asleep only a little on the way""",,,0
B-2020_U11,,,"""I vaguely understand the privacy policy, but I don't know the details.""",,"""Today, we learned something that we need to think about very carefully in the future. I think that research will start in the third year, and I think that science students need to be especially careful. .""",1
B-2020_U13,,"""I learned a lot about the ethics required for research.""","""The information ethics rules were difficult and there were parts I didn't understand.""",,,1
B-2020_U14,"""Rules to follow and ethics to have when doing research""",,"""I didn't know what kind of effect I could expect from a single blind.""",,,1
B-2020_U15,"""As a responsible researcher, I will conduct research objectively without committing fraud in accordance with research ethics. I will conduct peer review to increase objectivity. Various problems may arise in joint research with companies. Communication is important because there is a conflict of interest.Conflict of interest is a situation where objectivity may be lost by considering personal interests.Kyushu University has information ethics regulations and security policies. """,,,,,1
B-2020_U16,,"""Kyushu University must have an information ethics code and its content.""",,,,0
B-2020_U17,"""Not all rules for conducting research are written down, and much depends on individual ethics. In order to avoid emotional mistakes and friction with co-researchers and companies, we forget to maintain a sincere attitude. It is important to review each other with experts and researchers around you first.If you commit research misconduct that affects various fields, you may not be able to fulfill your responsibility to protect the interests of your clients. Measures and guidelines for each organization have been established, and it is necessary to check the security policy of the organization to which you belong.""",,,,,0
B-2020_U18,"""Sense of Responsibility in Research, Causes and Countermeasures of Research Misconduct, Problems of Conflicts of Interest, Information Ethics at Kyushu University""",,,,,1
B-2020_U19,"""When conducting research activities at universities and other research institutes, there are rules that must be followed, even though they are not necessarily written down. It is important to be aware of the social responsibility we have.In addition, research activities emphasize the protection of individuals and ethics, which contribute to the development of society.However, even in the current environment, research misconduct is happening, so there is room for improvement.""","""I learned that morals and ethics create an environment that facilitates research. I was able to understand the motives behind fraud. I was able to think about fraud in my own way.""",,,,0
B-2020_U21,,"""I learned about fraud in research and the ethics of conducting objective research.
It was a very meaningful class because I hadn't thought about it yet because it was something I had to protect and always keep in mind. ""","""I wasn't able to correctly apply what I learned today in Serious Games.""",,"""It was knowledge that would become necessary and important when I was in my fourth year writing a graduation thesis, so I decided to research it myself and understand it more deeply and correctly.""",-1
B-2020_U22,"""Individuals should pay attention to many attitudes towards research, such as research ethics and information ethics. It is also important for countries and organizations to stipulate and indicate norms to some extent.""",,,,"""In this class, I understood that I have to be careful about my attitude toward research and writing papers, so when I become a third or fourth year student and conduct research activities, I will pay close attention to these points. I felt like I should be conscious of it.""",1
B-2020_U23,,"""I want to learn morals and rules before doing research and writing papers so as not to put myself at a disadvantage or cause trouble to others.""",,,,0
B-2020_U24,"""In this lecture, I learned about ethics as a researcher and information ethics. Regarding research ethics, I learned about ethics as a researcher. In information ethics, we learned about information ethics regulations at Kyushu University, information management and classification, and physical security protection policies. is.""","""I was able to know what to think when I assumed that I was doing research. It was easy to create because I was in a hurry to falsify data, and because I was confident in my own assumptions. I thought I'd be careful when I decided to do it.""",,,"""Since I had to participate in the middle of the class due to a problem, I decided to come 10 minutes before the next class and deal with it quickly. I might be going to graduate school in science, so this time I felt closer to you. Also for research. I was surprised that there were rules that I should know, but I wanted to act with the thought that there are rules everywhere, like this time.""",0
B-2020_U26,"""Ethics and information handling in research""","""Research Misconduct/Research Ethics in Collaborative Research""",,,,0
B-2020_U31,,,,,"""Today's class was originally something I vaguely understood, but it will be very important for our own research three years from now, so we would like to take this opportunity to understand it thoroughly. Also, when we submit our report assignments, We would like to clarify the source and handle the information appropriately.""",1
B-2020_U32,""" Research activities must be conducted honestly, accurately, and objectively, and must be conducted efficiently in order to use limited resources without waste. Recognizing that individual researchers have different attitudes toward research, attitudes toward sharing results, and economic needs, when conducting joint research, It is necessary to establish proper communication, division of roles, authorship, and leadership.Conflicts of interest are those that give priority to personal interests and lose the objectivity that should be there, making research misconduct more likely to occur. It's a state of being, and it's often caused by feelings that don't have a form.""",,,,,0
B-2020_U33,,,"""There's nothing in particular that I didn't understand, but I wanted to know a few specific examples of conflicts of interest.""",,"""I felt it took too long to review.
I thought what I did today was a matter of course, but I was afraid that I wouldn't be able to get away with not knowing. """,0
B-2020_U34,"""During research activities, it is required to conduct research responsibly and in accordance with the code of conduct. In addition, research misconduct such as fabrication, falsification, and plagiarism must not be performed, and research data must be handled ethically. No. Peer review is usually performed by peer review.In joint research, we must understand each other's differences and be careful not to cause problems.By considering interests. A situation in which objectivity is lost is called a conflict of interest, and it is often due to academic obsession.
Kyushu University has established information ethics regulations and security policies regarding the use of information assets. ""","""I learned that I have to be careful when conducting research.""",,,,1
B-2020_U35,"""Research etiquette""",,,,,0
B-2020_U36,"""Various Points to Note in Research""","""I was able to understand what is important for my future research.""",,,"""I was able to develop a vague understanding of fraud in research.""",1
B-2020_U38,,"""It goes without saying, but of course there are ethics in research. Ethics are not something that can be firmly defined in writing, but I also learned that there are guidelines and norms to prevent them.""",,,,0
B-2020_U39,"""Research presentations must not be dishonest, and if they do, credibility will be lost in various ways. To prevent this from happening, always maintain a neutral standpoint and conduct thorough preliminary surveys and peer reviews."" is important.""","""Kyushu University also has an information ethics code.""",,,,1
B-2020_U4,,,,,"""This ethics was about social honor, so I realized the need to know the rules and guidelines without being lazy.
I felt that learning the fundamentals of cyber security is important not only as an academic subject, but also for living. """,1
B-2020_U40,,"""In research, there are many things that must be taken care of. In particular, we must be very careful in handling information used in research.""",,"""When using other people's data in my research, should I just write the source without their consent, or is it better to write the source with their consent as much as possible?""",,1
B-2020_U41,"""Research is carried out to create a better society and to elucidate how the world works. People involved in research are required to be honest and sincere. Research is neutral and objective. The content should have a sexual nature, so there should be no falsification, fabrication, or plagiarism.A situation of conflict of interest may occur due to results-based, financial problems, and biased opinions.Individual researchers and organizations may not share information. Getting the morals right is very important.""",,,,,-1
B-2020_U43,"""When doing research, you must be honest without being bound by your own wishes.""",,,,,1
B-2020_U44,,"""I learned how to avoid cheating when doing research and writing papers.""","""I forgot to draw a marker where I didn't know when I was preparing.""",,"""I would like to make use of what I learned today when conducting research in the future.""",-1
B-2020_U45,,"""I thought it would be better to get opinions from various points of view on papers, etc., so I was surprised to receive opinions from the same research experts in the peer review talks.""","""I think sponsors tend to have a stronger position, so I was curious as to what was done specifically to ensure that research was conducted ethically.""",,,0
B-2020_U46,"""When conducting research, we must observe research ethics and conduct research activities responsibly. Research ethics are ambiguous, but research should be published to contribute to the rank, be neutral, and be objective. Research misconduct includes fabrication, falsification, plagiarism, etc. It is a betrayal of society and affects the group to which one belongs and the individual. and the Code of Conduct for Members of the Chemical Society of Japan, etc. Peer review is a way to improve the quality of papers.People in the same field review papers before they are published.Reviewers are compulsory. There is an ethics of peer reviewers that must be observed even if they cannot do so, and to ensure fairness, it is good practice to set a single blind that hides the name of the reviewer and a double blind that hides the name of the author. It is important not to impede the progress of society.""",,,,"""Thank you very much.
I realized once again that I am a university student and that I will be doing research and writing papers in the future. """,1
B-2020_U47,,,"""Peer reviews are conducted by volunteers, but how many people are involved?""",,,0
B-2020_U5,"""Contents about morals that individuals should have when doing research or handling information.""",,,,"""The day after I attended last week's class, my smartphone broke and I couldn't use it at all. The day before, we had talked about backing up data in class, and we had confirmed it, so there is a possibility that the data may be restored to some extent, but I was afraid.
Therefore, through today's class, I once again thought that there is no loss in acquiring a lot of senses such as security and ethics from an early stage. """,0
B-2020_U50,,"""It turns out that there is no such thing as a rulebook for research.""",,,,0
B-2020_U51,,,,,"""Even in today's class, I was able to work hard from my preparation, and learned about my responsibilities and roles in research.""",-1
B-2020_U53,,,,,"""When I conduct research in my senior year, I want to have a sense of responsibility and not cheat. I was convinced when I learned that information would be withheld in order to maintain impartiality in the review of papers.""",1
B-2020_U54,"""I have learned proper research methods and conduct that constitutes research misconduct.""","""I was able to understand the attitudes that are important in research. Honesty, accuracy, and so on all seem natural to me, but when it comes to doing research, I worry about the evaluations of others and feel pressure. I also felt that it might be difficult to carry out proper research because I do.""","""The organizational structure of Kyushu University was difficult. I didn't even know such a thing existed, so I want to learn more.""",,,0
B-2020_U56,,,"""I couldn't confirm what I would do when I was in a research position.""",,,-2
B-2020_U57,"""When conducting research, it is necessary to be conscious of efficiency, and to honestly and faithfully summarize the content of the report based on the results obtained. Anonymous reviewers review and check for conflicts of interest.Information ethics is the code of conduct necessary for handling information.Information ethics is the way of thinking and attitudes in the information society.""",,,,,1
B-2020_U58,"""Understand research neutrality, objectivity, fraud, and conflicts of interest.""",,,,,0
B-2020_U59,,"""I learned the importance of not committing fraud in research. I learned that research entails trust in society, professional groups, and individual researchers.""","""I had a hard time with the physical security protection policy.""",,,1
B-2020_U6,"""What research misconduct is, what countermeasures are being taken, and information ethics""",,,,"""When I start doing research in the future, of course I will not commit research misconduct, and I have learned that the surrounding environment is also a major cause of research misconduct. I also wanted to be careful.""",1
B-2020_U60,"""Ethics in Research and Morals in Handling Information""","""Reliability of research depends to some extent on the morals of researchers.
Another cause of fraud""",,,,0
B-2020_U62,,,,"""Nothing in particular""",,0
B-2020_U63,,,"""What should I do if I want to cite other literature in presenting my research results?
""",,"""I was relieved to learn that Kyushu University is also managing its information network properly.""",-2
B-2020_U64,,"""What constitutes research misconduct?""",,,,-1
B-2020_U65,,"""I didn't know there was a way to prevent authors and reviewers from knowing each other when reviewing research.""","""The slide for managing information network connections was a little difficult.""",,,0
B-2020_U67,,"""On Research Logic and Information Ethics""",,,"""I'm getting used to online classes""",1
B-2020_U68,,"""I learned about the Rinri that should be protected when conducting information and research.""",,,"""I want to keep what I learned this time.""",1
B-2020_U69,,"""I hadn't really thought about the possibility of research misconduct that could not be prevented by individual researchers' attention, such as conflicts of interest. made me think""","""I couldn't understand exactly what the last slide was about""",,,0
B-2020_U70,"""About research ethics and information ethics.""",,,,,0
B-2020_U71,,"""I have found that there are different principles for conducting research.""",,,,-1
B-2020_U73,"""Confirmation of norms and ethical manners for research, and ethical rules for information handling and storage""","""I was able to learn about general norms, neutrality, and objectivity in research. I was also able to understand the meaning and content of the related word 'conflict of interest.' """,,,,0
B-2020_U74,,,"""Detailed terms such as backdoors on page 30""","""Nothing in particular""","""I've run out of time to prepare for class, so next time I'll try to do it on Saturdays and Sundays.""",-1
B-2020_U77,,,,,"""I became worried about my graduation research after I became a fourth year student.""",0
B-2020_U79,"""When conducting research, there are ethics and rules that must be followed. If you do not follow them, not only will the credibility of the research itself decline, but you will also lose the trust of the person himself. It is important to conduct research purely without losing out.""","""I was able to learn the minimum things to follow when conducting research.""",,,,0
B-2020_U8,"""Understand what research is and what is necessary and what is not good about research ethics necessary when conducting research.
Then, we will understand information ethics while looking at the example of Kyushu University. ""","""I learned what kind of measures were taken to prevent research misconduct, and I thought that they were a matter of course for human beings.
I found that Kyushu University has solid rules and policies. ""","""Various organizations and groups have issued norms and guidelines, and it has become confusing.""",,"""I learned that research misconduct occurs in situations that occur only in humans, such as conflicts of interest, what constitutes misconduct, and how to prevent it.
I learned that the norms are different depending on the administration, and I was wondering which country's norms were the laxest.
Since I am a student at Kyushu University, I would like to understand the rules and policies of Kyushu University. """,1
B-2020_U81,,,,,"""In today's class, I learned a lot of points to note and important things to know about research. I have no plans to become a researcher, but I would like to make use of what I learned today in a different field."" think.""",1
B-2020_U83,,,,,"""These ethical issues are often featured in the news and become a hot topic, so I would like to avoid becoming a party to such things.""",0
B-2020_U84,,,,,"""From now on, when I conduct research, research misconduct may occur, so I have to be careful.""",-3
B-2020_U85,"""When conducting research, it is necessary to comply with research ethics, strictly follow written norms, and also follow unwritten rules.
""",,,,"""I'm nervous because I feel that research is unexpectedly close to my heart.""",1
B-2020_U86,"""When conducting research, it is important to have a fair and objective attitude, without being bound by personal interests. In addition, Kyushu University's information ethics regulations are stipulated in detail for the proper handling of data within the university. ""","""I learned that Kyushu University has a documented information security policy.""","""I could only vaguely understand what was on page 30.""",,,1
B-2020_U87,"""Ethical thinking and attitude is necessary when conducting research.
If you commit fraud, it will cause inconvenience to the people involved and you will lose credibility. """,,,,"""Occasionally, fraudulent research is reported in the news, so I think we must always keep ethical considerations in mind.
Even so, it seems that a certain number of people will commit fraud without any malice. """,0
B-2020_U89,,,,,"""As I have said in past lectures, security and human psychology are closely related, and I thought that the fight against security is a fight against weakness.""",1
B-2020_U9,,"""There are rules that researchers must follow, and I understand my responsibilities as an organization and as a researcher.""","""I'm not sure what you mean by 'disclose a conflict of interest'. """,,,1
B-2020_U92,"""I realized that research is not just for me.""",,,,,1
B-2020_U93,,"""What matters in research""",,,"""This time was easier than last time and the time before last""",-1
B-2020_U94,,,"""About the information regulation at Kyushu University. Words are difficult.""",,"""In the future, there will be more opportunities for research and the publication of papers, so I would like to cherish this kind of information ethics.""",-1
B-2020_U95,,,,,"""Until now, I had only a vague image of researchers doing research, but now that I understand that they are actually being conducted on the basis of principles and standards, my perspective has changed.""",0
C-2021-1_U10,,,"""Proof of U(M) = log₂M""",,"""The content of this time was difficult because there were a lot of calculations and definitions, so I would like to review it through exercises.
I didn't know what the amount of information was, but I was surprised to learn that the ambiguity was reduced, and I was impressed that it could be expressed numerically. """,-2
C-2021-1_U101,,"""We found that the reduction in ambiguity (choices) by repeating questions is expressed as 'the amount of information obtained', and the ambiguity is expressed as U(M). We also found that knowing the occurrence of an event with probability P reduces the ambiguity to -logp. From this fact, knowing that a 1 came out by rolling the dice and knowing that a 1 came out out of the 1st place will know which one came out.""",,"""If U(S) = H(S), H(s) = expected value of information amount, then H(s) = expected value of information amount?""",,-2
C-2021-1_U103,,"""I now have a rough understanding of entropy""","""I didn't really understand the proof""","""Nothing in particular""",,-1
C-2021-1_U104,,,,,"""There was a lot of information in this lecture, so I would like to review it thoroughly together with the previous lecture.""",-1
C-2021-1_U105,"""By defining the amount of ambiguity and obtaining information, the ambiguity is reduced. The ambiguity is expressed by log2M. By knowing the occurrence of an event with probability p, you can obtain information log2p. In other words, the amount of information that can be obtained by knowing the occurrence of something that rarely happens is large.""","""The expected value of the amount of information obtained is equal to the entropy, and when the probability is 1/2, the value of entropy takes the maximum value of 1.""","""I couldn't understand examples and problems of mutual information of random variables. I also couldn't understand the proof that the definition of ambiguity is log2M.""",,"""I was surprised at the definition of ambiguity. Entropy and other information and calculations were difficult, so I wanted to do my best to solve them on my own.""",-1
C-2021-1_U106,,,"""About Mutual Information""",,,-2
C-2021-1_U11,"""The amount of information is represented by -log2p, and the smaller the probability of an event occurring, the larger the amount of information obtained.""","""I was able to deepen my understanding of how to calculate the amount of information.""","""The calculation formula was difficult, so I want to practice more.""",,"""I was late in starting the exercises, and it took me a while to remember what I had learned, so I would like to be able to start early.""",-3
C-2021-1_U12,"""Represents the reduction of ambiguity, the amount of information, etc. with a formula.""","""Ambiguity is additive. From this it can be proved that ambiguity can be expressed in log(2)M.
The amount of information is so large that things that rarely happen happen.
Conditional entropy = conditional information ambiguity""",,,"""I'd like to have them apply the proof themselves.""",-1
C-2021-1_U13,,"""How to find the amount of information and the expected value of the amount of information.""","""A mistake in a complicated calculation was conspicuous.""","""Nothing in particular.""",,-2
C-2021-1_U15,"""It was a content that seemed to have become a little complicated.""","""I think I've become able to calculate the relationship between two phenomena""",,,"""I want to keep learning""",-2
C-2021-1_U19,"""Information is gained by reducing ambiguity.""",,,,,-3
C-2021-1_U2,"""I learned how to reduce the ambiguity of information sources. Also, I thought about the expected value of the amount of information being equal to the entropy, and the amount of mutual information between two events.""",,,,,-3
C-2021-1_U23,,"""I learned that the reduction in ambiguity is the amount of information. In the entropy formula we learned in the previous lecture, I somehow understood the reason why the base of the logarithm is 2. Considering the expected value of the information amount as the average It was intuitively easier to understand.""",,,"""It was intuitively easier to understand when I considered the expected value of the amount of information to be the average. I felt strange that the ambiguity and the expected value of the amount of information corresponded to the value of entropy even though the definitions of the words were different. rice field.""",-3
C-2021-1_U24,"""I quantified ""ambiguity"" and thought about the relationship between ""ambiguity"" and the amount of information. I learned about expected value of information and mutual information. ""","""I couldn't understand the proof of U(M)=log_2(M) at the preparatory stage, but I was able to look back on the parts I stumbled on in class, such as the additivity of U and the monotonicity of functions, so I finally I was able to figure out the amount of information, its expected value, and the amount of mutual information.""",,,,-1
C-2021-1_U25,,"""I was able to quantify the amount of ambiguous information""","""Expected value meaning""",,"""I honestly don't know what I'm doing if I'm just listening, so I'll do my best to review.""",-2
C-2021-1_U26,"""I learned a lot of formulas for computing information.
""","""I learned the form of formulas using log.""","""I didn't understand the image of words of ambiguity and conditions""",,"""It was difficult.
I want to practice harder. """,0
C-2021-1_U27,,,"""I don't understand conditional entropy.
・The amount of mutual information is not well understood. """,,"""I'm starting to get lost in the mutual information area, so I'd like to review the textbook and solve the exercises.""",-3
C-2021-1_U28,,,"""I didn't quite understand the log proof of entropy.
I want to be able to guide this on my own. """,,"""I have a better understanding of entropy than I did last time.
However, I couldn't understand much about complex things such as mutual information in the first session, so I would like to review it.
""",-2
C-2021-1_U29,"""I learned about information and entropy.""","""I learned a little bit about ambiguity and entropy.""","""It was difficult to prove U(M) = log2M.""",,,-1
C-2021-1_U30,,,"""Ambiguity = proof of log""",,,-1
C-2021-1_U31,,,,"""I didn't understand the explanation of additivity in the case of two dice.""","""It was interesting because of the mathematical way of thinking.""",-2
C-2021-1_U32,"""The amount of information obtained is expressed as 'decrease in ambiguity', which is -log2(M). Also, the expected value matches the entropy. ""","""We found that events with high probability were approximated to 1 and not included in the average.""","""I decided to investigate how far approximation is possible and where it is not.""",,"""It was difficult, but I was able to understand it together with the contents of the last time.""",-1
C-2021-1_U33,,"""Entropy is related to the amount of information.
""","""When I heard the word entropy, I went into a crazy mode.""",,,-1
C-2021-1_U34,"""I learned how to calculate how the information obtained reduces the ambiguity of the event.
I learned how to calculate conditional entropy and how to write it. ""","""You can now do things like entropy, expectation, and how to calculate information ambiguity.""","""I didn't understand the conditional entropy calculation.""",,,-1
C-2021-1_U35,"""Through examples, we learned about the amount of information and the ambiguity of information sources. We saw the expected value of the amount of information, what the entropy function is, and how to compute it.""",,,,"""I was a little worried about things like entropy, but I'm glad I was able to confirm that in today's lecture.""",-1
C-2021-1_U36,"""・What is the amount of information?
・Relationship with entropy
・Joint Probability and Conditional Probability, Conditional Entropy""","""・Amount of information = reduction in ambiguity → U(M) = log2M
☆Monotonically increasing, additivity
☆M is the probability of the event that occurred, and the reduction in ambiguity due to knowing this is -log2M
・Expected value of the amount of information obtained = entropy""","""If you study again, you will understand.""",,"""There are still some things I'm not convinced about, but the relationship between entropy and probability is interesting. I'm glad that I was able to understand the proof better when I tried it myself. I'm going to review it regularly so that I don't forget it during Golden Week. think.""",-2
C-2021-1_U38,"""That ambiguity reduction is the same as the information obtained. Relationship between mutual information and entropy.""",,,,,0
C-2021-1_U39,"""I learned that the reduction in 'ambiguity' has something to do with the amount of information. ""","""It was easy to understand because the entropy function was drawn graphically.""","""The hard part was the entropy of conditional probability.""",,"""I want to study hard.""",-1
C-2021-1_U40,"""Entropy indicates the ambiguity of information. The rarer the information, the greater the amount of information.""","""I didn't understand entropy well last time, but this time the definition is clear. Information entropy is the essence of information probability.""",,,"""I understand the relationship between probability and entropy""",-2
C-2021-1_U41,"""Source ambiguity can be calculated using entropy.""",,,,,-1
C-2021-1_U42,,,"""On entropy.""",,"""I still don't know much about entropy, so I'd like to review it thoroughly.""",-1
C-2021-1_U43,,"""I was able to understand what entropy is.""",,,,-2
C-2021-1_U44,"""On entropy, probability distribution, information ambiguity and its reduction, and mutual information""","""I learned when the expected value of the amount of information and the probability distribution are used.
The reduction in information ambiguity was understandable. ""","""I didn't really understand the amount of mutual information.
""",,,0
C-2021-1_U45,"""The reduction in ambiguity is the amount of information, and the smaller the probability of occurrence of information, the greater the amount of information.
Also, entropy is the expected value of the amount of information, and the entropy becomes maximum when the probability is 0.5. ""","""I was able to calculate the amount of information. I'm not confident about the expected value, but I think I understand.""","""I couldn't understand the definition of mutual information well. Overall, the content was difficult.""",,"""I was able to understand the contents roughly, but I don't know the details well, so I would like to review thoroughly.
""",-1
C-2021-1_U46,"""The amount of information obtained by knowing the occurrence of an event with probability of occurrence p is defined as the reduction in ambiguity.""","""The amount of information that can be obtained by knowing the occurrence of an event that rarely occurs is large. Also, the amount of information and its expected value were obtained.""","""I still don't know much about entropy.""",,,-2
C-2021-1_U47,"""The ambiguity of information can be derived as a mathematical formula using theorems.""","""We were able to determine the amount of information obtained by calculating the ambiguity of information.""","""I didn't understand a bit about mutual information.""",,"""It was a lot of fun because there were many things I didn't know for the first time.""",-2
C-2021-1_U48,"""The amount of information is the reduction of ambiguity. The amount of information obtained from rare events is large. The expected value of the amount of information = entropy.""","""I learned how to calculate the amount of information. I was able to get an image of entropy.""","""I got confused when it came to conditionals. I didn't understand mutual information.""",,,-3
C-2021-1_U49,"""・Information ambiguity = expressed as log2(M)
・The amount of information obtained = the degree of ambiguity of information that decreased as the information was narrowed down
・Entropy = Expected value of the amount of information obtained ()
・There are several measures of the information obtained, and what is obtained in this way is called mutual information.","""・Information ambiguity = expressed as log2(M)
・The amount of information obtained = the degree of ambiguity of information that decreased as the information was narrowed down
・Entropy = expected value of the amount of information obtained
・Since information is represented by only two symbols, the base of the logarithm when showing the ambiguity or amount of information and entropy is 2.
・Amount of information obtained from an event with probability of occurrence p = -log2(p)
・The amount of information obtained is greater for events with lower probability of occurrence.""","""・I didn't quite understand the ambiguity theorem that appeared in the first half.""","""・How is the entropy that appeared in the previous average code length connected to the current one?""","""・There are a lot of formulas, and it's getting more difficult, but I also feel the joy of understanding.""",-2
C-2021-1_U50,,,,,"""There are a lot of mathematical elements, so I was a little weak, but when I actually tried to solve the problem, I was relieved that I could solve it unexpectedly. I would like to deepen my understanding through exercises.
""",0
C-2021-1_U51,"""I learned how to determine the amount of information that can be obtained, and how to determine the amount of ambiguity that accompanies it.""","""There is a correlation between the amount of information and the amount of ambiguity, and not only did we find that the amount of ambiguity decreased as the amount of information obtained increased,
I also found out that the value can be obtained by taking the logarithm. ""","""Nothing in particular.""","""Nothing in particular.""","""If you know how to calculate the amount of information, you will be able to obtain the information you want efficiently, so I thought it would be very convenient.
""",-2
C-2021-1_U52,"""expected value of information""","""Concept of Expected Value of Information""","""How to read the table""",,"""I felt like the content was getting more and more difficult. There were words in Japanese textbooks, such as the law of increasing entropy, but I couldn't really understand what they said.""",0
C-2021-1_U53,,,"""Conditional entropy was a bit difficult to understand.""",,"""The explanation using actual examples such as cats was very easy to understand. I still don't understand how ambiguity is expressed in logarithms, so I would like to review it again.""",-2
C-2021-1_U56,,"""In order to consider the case where information is interrelated, we need to deal with multiple indicators instead of one.""","""I couldn't understand why the mutual information formula is I[X,Y]=H[X]-H[X|Y].""",,"""In this lecture, I was able to confirm entropy, which I didn't understand very well last time. I would like to clarify points such as mutual information.""",-1
C-2021-1_U57,"""The expected value of information is roughly how much information you can get.""","""The expected value of the amount of information matches the entropy""","""mainly computation""",,"""Calculations were difficult in this class""",0
C-2021-1_U59,"""As ambiguity decreases, the amount of information obtained increases
""","""The value of entropy H(p) reaches its maximum when p=1/2
""","""Calculating Entropy (Mutual Information etc.)""",,"""Entropy was new and unfamiliar to me, so I found it difficult.
I don't understand the calculation very well, so I'd like to be able to understand it properly through exercises. """,-1
C-2021-1_U60,,,,,"""Was funny""",-1
C-2021-1_U61,"""Knowledge of an event reduces ambiguity.
Entropy is determined by using probability. ""","""The more information available, the less ambiguity.""","""The proof of U(M)=log₂M was difficult to understand. In the end, I only vaguely understood what entropy was.""",,"""The more times I go, the more difficult the content becomes, so I decided to cherish the review.
Looking at the calculation, it seems a little difficult, but when I thought about it carefully, I thought it wasn't that difficult. """,-2
C-2021-1_U62,"""In the process of narrowing down the number of dice rolls to one of the six, the process of reducing ambiguity was quantified to make it easier to analyze.""","""definition of ambiguity reduction, etc.""","""It became difficult around the expected value""",,,-1
C-2021-1_U63,"""When the ambiguity of information is captured as a numerical value, the ambiguity of information can be expressed as -log_2M, and it can be understood as the amount of information obtained by reducing the ambiguity of information. The expected value of the amount of information obtained matches the formula of entropy.In addition, when considering the expected value of the amount of information obtained depending on which event occurred under a certain conditional constraint, it can be said that it is the amount of mutual information between the condition and the event.""","""Reduction of information ambiguity = defined as the amount of information obtained. The average of the amount of information obtained is called the expected value of the information, and it must match the entropy.""",,,"""Defining the ambiguity of the amount of information was something new and surprising, perhaps because of the language. If you define the ambiguity of the amount of information, you can obtain a large amount of information when an event that rarely occurs occurs. It was convincing to me that what I had empirically understood as becoming was mathematically proven.""",-1
C-2021-1_U65,,"""It turned out that there was a number to the ambiguity.""",,,"""It was interesting to use log for fuzziness numbers.""",-1
C-2021-1_U66,"""Different entropy due to information ambiguity and obtained information""",,,,,-1
C-2021-1_U68,"""Entropy can be obtained by quantifying the ambiguity of information. The expected value of the amount of information obtained from the probability of occurrence is equal to the entropy.""",,,,,-2
C-2021-1_U69,"""Information is the amount of reduction in ambiguity. In the case of dice, the more numbers on the die, the greater the ambiguity. The numerical value of ambiguity is equal to entropy. The amount of information obtained from unlikely events is large, but the expected value of information is small.""","""I thought it was natural to understand something by reducing ambiguity when I got information, but in this class I learned that it can be obtained by calculation.""","""I don't understand why you use log.""","""is not""","""A lot of new things came out, but the examples were familiar and easy to understand.""",-3
C-2021-1_U7,,,"""I didn't understand the first proof of U(M)=log₂M""",,,0
C-2021-1_U70,"""Expansive feeling of mathematics, calculation of information amount and expected value""","""Calculations are basically done""",,,,-1
C-2021-1_U71,"Based on the idea that the amount of information obtained = reduction in ambiguity, it was formulated using the probability of generating the amount of information and a logarithmic function.
We learned that the expected value of the amount of information obtained can be calculated as entropy, which we learned last time.
We learned mutual information as a measure of the relationship between two events.
The mutual information indicates how much information about one event can be obtained from the information about the other event. If the two events are independent, the mutual information is 0. ""","""We found that we can formulate the amount of information and calculate it concretely for specific events.""","""Nothing in particular.""",,"""The method of formulating the amount of information was interesting. I thought it was consistent.""",-2
C-2021-1_U72,"""About information ambiguity, entropy.""","""I learned a lot about entropy that I didn't know last week.""","""Basically, there was nothing I didn't understand, and I was able to understand.""",,"""I didn't expect to know so much about the information.""",-3
C-2021-1_U73,,"""Since the reduction in ambiguity is expressed as -log2p, p, that is, when the probability of an event occurring is small, the information obtained is large. When there is no correlation between events X and Y, information about Y can be obtained. It is useless to get information on X.""","""I'm not familiar with the definition and concept of entropy, so I'll review it again. I don't understand what it means when the expected value of information and entropy match. I didn't really understand conditional entropy.""",,,0
C-2021-1_U74,,,"""Overall, I still feel like I don't fully understand it. I need to review it.""",,,-1
C-2021-1_U75,"""The amount of information increases as the ambiguity of information decreases. The ambiguity of information corresponds to entropy under the assumption that it is a continuous function of the occurrence probability of a certain event. Knowing the occurrence of an event with a low occurrence probability This increases the amount of information that can be obtained. The expected value of the amount of information is expressed as the sum of the product of the occurrence probability of each event and -log2 (occurrence probability), which is consistent with entropy.""",,,,"""The calculation formula came out, but it was easier to understand than last time. I was able to understand the definition and think about the formula.""",-1
C-2021-1_U76,"""About the amount of information, the expected value of the amount of information, and the amount of mutual information
""","""Knowing probabilities tells us about the phenomenon of ambiguity, how much information is available.
""","""The proof that the amount of ambiguity is expressed in log was a little confusing.
""",,,-2
C-2021-1_U77,"""We have entered the stage of actually calculating the amount of information. It was necessary to operate the calculation formula.""","""It is now possible to calculate the amount of information.""",,,"""Actually doing the math is tedious at first, but fun.""",-1
C-2021-1_U78,"""Calculating Entropy""",,,,,-1
C-2021-1_U79,"""Acquiring information reduces the ambiguity of a certain phenomenon. Obtaining information that rarely occurs reduces ambiguity.
Mutual information is the ambiguity about another phenomenon obtained from one piece of information.""",,"""We didn't know much about expected value of information or mutual information. Conditional entropy was the least clear.""",,"""I learned that you can learn a lot from one piece of information.""",-2
C-2021-1_U8,,"""I was able to do mathematics on seemingly unmathematical things like cat moods and tail states.""","""I didn't understand the math""",,"""I felt I needed a refresher""",-3
C-2021-1_U80,,,"""I didn't understand the joint probability and conditional probability of mutual information. I also didn't understand the meaning of conditional entropy, and I didn't understand how to use it in everyday life.""","""(3) part.""",,-2
C-2021-1_U82,"""Amount of information is related to ambiguity: (reduced ambiguity) = (amount of information obtained).
Information can be calculated using probability

""","""U(M) = proof of log-2 base M
The expected value of the information obtained matches the entropy
""",,,"""There were some mathematical proofs today, and I was able to understand the contents of the class more logically.""",-1
C-2021-1_U83,"""The source ambiguity U(S) is given by log2M. If an event with a low probability of occurrence occurs, the ambiguity is reduced. If there is information that interacts, the ambiguity is further reduced. These can be computed using entropy and expectations.""","""The analogy of the cat in a good mood was interesting. The explanation of the dice was easy to understand, and I was able to visualize the reduction in ambiguity expressed in log. The mutual information and expected value parts should be fully understood. Although it was not possible, by grasping the information that interacts, I interpreted it as being able to calculate that the possibility of a certain event increases.""",,,,-2
C-2021-1_U84,"""Probability and entropy tell us the degree of ambiguity of information.""","""I was able to understand how entropy works.""",,,"""It was difficult to calculate.""",-2
C-2021-1_U85,"""The amount of information is closely related to the ambiguity of that information. The more ambiguity is eliminated, the greater the amount of information. The rarer the event, the greater the amount of information. When calculating, it is necessary to narrow down and add like a conditional probability.""","""The amount of information can also be expressed as ambiguity. The expected value is the estimated amount of information that will be obtained from the information that will be obtained from now on. The amount of information can be obtained using a formula.""",,,"""I thought it was interesting to use the word expected value to express the estimation of the amount of information. It was very interesting because I didn't think that what I had been working on, such as probability problems, could be used for the amount of information. Entropy I regret that if I had a better grasp of the definition of , it would have been even more interesting.""",-1
C-2021-1_U87,"""The ambiguity of an information source corresponds to the entropy of the information source, and the amount of decrease is called the amount of information. The amount of information that can be obtained by knowing the occurrence of an event that rarely occurs is large.
The expected value of the amount of information, that is, the entropy, takes a maximum value of 1 when the probability is 1/2. ""","""I found that the amount of information and the expected value can be calculated by taking the logarithm.""","""I couldn't understand mutual information""",,"""It's become more mathematical and harder to understand.""",-1
C-2021-1_U88,"""The amount of ambiguity is represented by U(M), and the larger the number of M, the more ambiguous. The ambiguity of a uniformly distributed, memoryless stationary source is equal to the entropy of the source. Rarely occurs."" The amount of information is large.The value of entropy takes the maximum value when p1=0.5 when p1+p2=1.The denominator is different when calculating joint probability and conditional probability.Mutual information and indicates the strength of the relationship between two events.""","""I think I understood more about entropy than last week. It was difficult to quantify ambiguity during the preparatory class, but I was able to understand after listening to the class.""","""We still don't know much about the expected value of the amount of information.""",,"""I didn't do my homework last week, so I did my homework this week. I think it was easier for me to grasp the content of the class by doing it once.""",-3
C-2021-1_U89,"""The reduction in ambiguity is the amount of information.
And ambiguity is entropy. ""","""I understand a little bit more about entropy.
""",,,,-2
C-2021-1_U9,,"""The ambiguity increases as M increases, i.e. monotonically increasing with respect to M.
""",,,,-1
C-2021-1_U91,"""Representing probabilities in terms of entropy.""","""I learned how to do calculations using entropy to some extent.""","""I had a hard time understanding entropy.""",,"""Entropy is difficult to understand, and I'm a little worried about the future.""",-3
C-2021-1_U92,"""It was a class about entropy.
Various factors were connected to entropy. ""","""I'm thinking about entropy because I want to know the amount of information.""","""Honestly, I had no idea.""",,"""I thought I should do my homework and review.""",-1
C-2021-1_U93,"""About Entropy""","""The amount of information obtained increases as the ambiguity decreases""","""How to calculate""",,"""I was too stupid to keep up with class.""",-3
C-2021-1_U94,,"""We found that the amount of information ambiguity reduction represents the amount of information obtained, that the amount of information obtained is large when learning rare information, and that the expected value of information amount is consistent with entropy.
It was possible to obtain the amount of information and to obtain the expected value of the amount of information. ""","""I understand the formula I want, but I don't know why it works that way""",,,-2
C-2021-1_U96,"""What is the amount of information,"" the phenomenon of ambiguity = the amount of information obtained, ""was a game called Numeron, where one question narrowed down the number of opponents. ""","""The meaning of the definition of entropy.
""","""What to do when the expected value of the amount of information is not divisible.""","""What should I do when the expected value of the amount of information is not divisible?
""","""It was easy to understand.""",0
C-2021-1_U97,"""The less ambiguity you get, the less information you get.""","""By using log, the expected value of the amount of information is obtained.""","""I'm not good at calculating probability, so I want to master it as soon as possible.""",,,-1
C-2021-1_U98,"""I learned about entropy. I learned probability in high school mathematics, but this class was more practical.""","""I can now understand and calculate entropy, which I didn't understand much last time.""","""I felt the complicated calculations in the second half were a bit difficult, so I'll review them often.""",,"""It was fun because I had a lot to do and think for myself.""",0
C-2021-1_U99,"""Entropy is a measure of the degree of irregularity, or ambiguity. The higher the entropy, the more valuable the information provided by the data.""","""It gave me some understanding of what entropy is.""","""I still can't remember the meaning of the symbol when it's written, so I want to fix it through practice.""",,"""I want to be able to understand even difficult content by thorough preparation and review.""",-3
C-2021-2_U1,"""It can be defined as the ambiguity of entropy and information content, and the difference in ambiguity is the information content.""","""By relating ambiguity to entropy, I could understand that the amount of information can be represented by a formula, and that the magnitude of the expected value is maximized when it is halved.""","""I had a bit of trouble understanding how the conditional entropy formula came about.""",,"""I learned a lot about the background of formulas, so I want to review and understand them.""",-3
C-2021-2_U100,"""Information has ambiguity, and as the amount of information increases, the ambiguity decreases, so the amount of information is the amount of reduction in ambiguity. The calculated value is called the expected value.""","""I knew that it would become more certain as the amount of information increased, but I learned that I could define it as a reduction in ambiguity and calculate the degree. I knew the expected value itself, but I understood that it represents the possibility of obtaining that information by probability etc.""",,,"""I thought it was good to learn about the expected value, which I hadn't understood until now. I thought it was roundabout to define the accuracy of information after defining the ambiguity of information, but please explain. When I heard it, I thought it was a rational and very easy-to-understand way of thinking.""",-1
C-2021-2_U101,"""The amount of ambiguity is expressed as U(M)=log(2)M, and the larger M is, the greater the ambiguity. U(M) has the characteristic of being additive. It is also equal to the entropy H(S) under the assumption that U(S) is a continuous function of the probability of occurrence for an information source S. The reduction in information ambiguity is equal to the amount of information obtained, and the probability of occurrence p Then, it can be expressed as -log(2)p, so the value increases when information is obtained even though the probability of occurrence is low.Furthermore, the expected value of the amount of information obtained when being told which event occurred is the entropy match.
Conditional entropy is a calculation of the expected value of ambiguity in the state of obtaining certain information, and by subtracting the conditional entropy from the entropy before obtaining information, how much correlation there is between two things It is possible to clarify whether This value is called mutual information and is expressed as I(X,Y), but it remains the same even if X and Y are interchanged. Since the ambiguity did not change even if the information was obtained when the mutual information was 0, it can be said that the two matters are unrelated. ""","""In today's lecture, I learned more about entropy, which I had vaguely understood last time. I was able to sort out the keywords 'ambiguity', 'quantity of information' and 'entropy' and their relationships in my own mind. I feel like In addition, I felt that conditional entropy was a problem at the preparatory stage, but as I listened to the lecture, the information was organized and I was able to understand the content to a degree that I was satisfied with. ""","""Nothing in particular.""",,"""This time, I feel that I have deepened my understanding through the concrete examples. On the other hand, this time, I felt that I understood it somewhat when I was preparing for the lesson, but after listening to the lecture, I felt that I understood it. I realized that I had stumbled and that I had settled with a vague understanding. I would like to review the lessons thoroughly and establish the information in myself. Also, I would like to continue to make the class meaningful by properly checking my level of understanding in preparation for the class. """,-1
C-2021-2_U102,"""In today's lecture, we learned about the relationship between information ambiguity and entropy. Information ambiguity can be quantified by expressing it in terms of entropy, and a decrease in entropy means that information has been obtained.""","""I learned that the ambiguity of information can be quantified by entropy. I was able to understand that the amount of information obtained can be understood by defining the ambiguity of information in terms of entropy. I was able to understand how to use it to solve problems.""","""I didn't understand some parts like the entropy proof.""",,"""I was impressed by the ability to quantify abstract things such as the ambiguity of information. It was very interesting to know how much information could be obtained clearly by quantifying. It is still perfect. There are some parts that I haven't fully understood yet, so I want to review them thoroughly.""",0
C-2021-2_U103,"""It is defined as the amount of information that can be obtained to reduce the ambiguity of the information source, and the expected value of the amount of information obtained is equal to the entropy. We also apply the concept of the amount of information to multiple matters, and Mutual information is an indicator of the strength of the relationship.""","""I found out that information is concretely quantified by introducing indices of ambiguity and strength of relationships between multiple pieces of information based on mathematical thinking.""",,,"""After listening to today's lecture, it was interesting to see clearly that in the world of information, it is obvious that mathematical concepts such as expected values ​​and conditional probabilities are the foundation. There were many concrete examples. It was very easy to understand.""",-1
C-2021-2_U104,,,"""I was able to apply it to the formula and calculate, but it seems that I understood why the formula was used, so I was confused.""",,"""There were a lot of things that I didn't understand, so I wanted to review and deepen my understanding.""",0
C-2021-2_U105,"""The reduction in ambiguity is the amount of information obtained. The ambiguity of the source can be quantified. It is calculated. The expected value of the amount of information obtained is equal to the entropy.
. ""","""The amount of information obtained when an event with probability of occurrence p occurs can be expressed as -log2p.""","""The difficulty was in matching the ambiguity of the general source to the entropy of the source.""",,"""I got into a very serious content, and it got very confusing. Halfway through, I got confused, so I thought I'd review it thoroughly.""",-1
C-2021-2_U106,"""Quantification of ambiguity of information, nature of ambiguity, relationship between ambiguity and entropy and amount of information, how to obtain the expected value of information amount, and conditions associated with mutual information and conditional probability regarding two events I learned how to find the attached entropy.""","""In last week's class, I didn't fully understand why entropy was calculated using log2, but in today's class, I learned about the relationship between entropy and information ambiguity, and it cleared my mind.""","""I didn't really understand why the expected value of information and the entropy match.""",,"""Because I was able to learn about the amount of mutual information and the ambiguity of information, I thought that I was able to learn more about the correlation of multiple phenomena than ever before. Also, mathematics, physics, chemistry, etc. It was very interesting because I felt that different subjects were connected.""",-1
C-2021-2_U107,"""How to obtain the amount of information that can be obtained, and how to define and obtain its expected value""","""It was difficult to understand what the entropy function represents, and I could not visualize what the vertical and horizontal axes represent and their relationship, but I was able to understand because of the detailed explanation of the derivation procedure.""",,,"""Regarding the expected value of the amount of information, the idea that the more the probability of occurrence of each event is matched, the more ambiguous it becomes is fairly easy to understand using diagrams and examples.
My computer froze due to overheating during class, so it was helpful to have a recording of the class. """,-1
C-2021-2_U108,"""I learned how to display the 'ambiguity' of information as a numerical value of the amount of information. Information can be evaluated by making full use of mathematics and reducing it to probability, expected value, and entropy (the disorder and ambiguity of things). """,,,,,0
C-2021-2_U109,"""Definition of information content, relationship between ambiguity and entropy, definition of mutual information""","""Through the formula, I was able to understand that the amount of information (ambiguity) and entropy are deeply connected. I was able to grasp entropy and mutual information as concepts.""","""I could understand the relationship of mutual information I(X,Y)=I(Y,X) by looking at the formula, but I couldn't understand it intuitively.""",,"""I felt that the mathematical content had become quite large and the concepts were becoming difficult.""",-1
C-2021-2_U11,"""Proof that the amount of information can be measured by ambiguity, and that the ambiguity of information can be expressed using log2
That the expected value of the amount of information matches the entropy
Mutual information: ambiguity can be changed by adding other information""","""Determining the amount of information, the expected value of information, and the amount of mutual information
About the entropy function
""",,,"""At first, I was puzzled by the expression ambiguity, but as I looked at various examples, I was able to understand it.
I was surprised that the reduction in ambiguity was determined by the phenomenon that occurred.""",-1
C-2021-2_U110,,,"""Regarding mutual information, from preparation to class, I was able to calculate, but I felt like I didn't know what I was doing.""",,"""I'm at a point where I don't understand the amount of information and entropy. I want to review and understand more precisely.""",-1
C-2021-2_U111,"""The ambiguity of an information source is equal to the entropy. The amount of information = the reduction in ambiguity = -log2p, where p is equal to the probability p of the event occurring. The expected value of the amount of information is equal to the entropy. It agrees. Mutual information is one index that indicates the strength of the relationship between random variables X and Y, and when it is 0, X and Y are irrelevant.""","""I was able to understand what value entropy corresponds to and why it can be expressed in that formula.""","""I didn't understand the proof of U(M)=log2M.""",,,-1
C-2021-2_U112,"""As the amount of information increases, the ambiguity of information decreases. Also, if the amount of ambiguity is U(M), it monotonically increases with respect to M. The amount of decrease in ambiguity can be expressed as a probability. Inverse In addition, from the probability, the amount of information obtained can also be obtained.The expected value of the amount of information is equal to the entropy.Entropy is equal to the ambiguity.Mutual information indicates the strength of the relationship between X and Y. When the mutual information is 0, X and Y are irrelevant.In other words, even if you get information on X, the ambiguity of Y does not change.""","""The idea and meaning of entropy is not perfect, but I think I have come to understand it better than in the previous class. I was able to understand what it represents in terms of the amount of information. As for mutual information, I was able to understand that the relationship between X and Y can be expressed numerically, and I thought it was interesting.""","""I didn't learn about expected values ​​in high school, so I don't know the details, but I think I have a general idea. I thought it was necessary to understand and work on it.""","""Nothing in particular.""","""When I learned about what entropy is, I thought it was interesting because there are various ways of thinking and understanding it, and it piqued my interest. The fact that the ambiguity of information decreases as the amount of information increases is something that can be seen in everyday life as well. I think it's something we unconsciously recognize, but I think it's amazing that you can put what you unconsciously understand into words and use it in the field of information science. I thought it would be interesting to quantify the strength of the relationship between",-1
C-2021-2_U113,"""On how to calculate ambiguity and correlated information.""","""Through the proof, we now know why we use log for entropy.""","""About mutual information, I think that there is a part that I do not fully understand because the formula has become complicated.""","""is not.""","""It's finally getting more complicated. I want to not only listen to the lectures, but also do my homework and review.""",-2
C-2021-2_U114,"""The reduction in ambiguity is equal to the amount of information obtained. The rarer the information obtained, the less ambiguity. The expected value of the amount of information obtained is equal to the entropy, and entropy is a measure of ambiguity. It can be said that the maximum value of the entropy function is when the probability of two events is 1/2.In addition, the amount of reduction in ambiguity about the other information by knowing one information is called mutual information.""","""It turns out that the amount of ambiguity reduction by knowing the occurrence of an event with probability p is -log₂p. I'm glad I was able to understand the entropy that I didn't know last time.""","""I would like to review the mutual information calculation formula again.""",,"""It was my first time to know the concept of the expected value of the amount of information, but I think I understood it. The example is very easy to understand and it helps me every time. Thank you.""",-2
C-2021-2_U115,"""By quantifying the ambiguity and quantifying the amount of information obtained by reducing the amount of information, it became easier to compare different events. Also, by multiplying the probability, the expected value can be obtained.""","""I feel like I've grasped the overall picture of the amount of information.""","""I didn't do that much in high school, so it was hard to imagine.""",,"""I don't have much understanding of mutual information, so I'd like to focus on that.""",-1
C-2021-2_U117,"""As a means of measuring the amount of information, there is a way to quantify the ambiguity of the information source. This has the property of monotonically increasing and additive. The decrease in the strength is -log2p.In addition, the expected value of the amount of information is equal to the entropy.There is mutual information I(X,Y) in the degree that expresses the strength of the relationship between the random variables X and Y, and I( X,Y)=H(X)-H(X|Y).""","""I was able to understand a little bit of the concept of entropy, which I didn't understand at all until last time.""","""In the conditional entropy formula, I didn't understand why H(p) was multiplied by the probability of the presupposed condition.""","""I'm curious about how entropy is actually used and in what situations.""","""What I learned this time was a way of thinking and calculation that is unlikely to have been done in mathematics etc. If you can imagine the graph of the entropy function, even if you do not give a specific number, the size of the value from the distance from p = 1/2 I thought it would be useful to be able to compare the two. I couldn't keep up with my understanding when there were so many elements, so I'd like to sort them out and follow them carefully from the next time.""",-2
C-2021-2_U118,,"""I understood the definition of ambiguity and how to measure the reduction of ambiguity. I also understood that it automatically understands the amount of information.""","""I couldn't understand the proof on page 10 of the document halfway through, so I'd like to try it myself so that I can understand it properly.""",,"""There were some parts that were a little difficult to understand, such as proofs, so I would like to review them thoroughly so that I can understand them.""",0
C-2021-2_U119,"""It was a class with a lot of calculations, such as finding the amount of information, finding expected values, and so on.""","""I was able to understand the nature of ambiguity and the expected value of the amount of information.""",,,"""In this class, there were various calculations, but I was able to understand a little of the proofs that I didn't understand in the preparation stage. I was able to feel once again the importance of reviewing.""",-1
C-2021-2_U12,"""Phenomenon of ambiguity = increased amount of information
A large amount of information can be obtained by knowing the occurrence of an event that rarely occurs. ""","""I feel like I was able to find the amount of information I could obtain.
""","""I was able to solve the information volume exercises by referring to examples, but I was unable to fully understand the essence of the content.
The expected value could not be obtained even by referring to the example. I didn't understand the H (mood) part.
""",,,-1
C-2021-2_U120,"""Entropy has the aspect of the expected value of the amount of information. Also, the reduction in ambiguity is equal to the amount of information given.
""","""Computation of the expected value of the amount of information. How to compute the conditional entropy.""","""I know how to calculate the conditional entropy, but I don't understand why I added new probabilities in the calculation process.""",,"""I was able to learn about entropy from a different perspective. It was the first time I touched on the calculation of expected value, but I'm glad it wasn't difficult.""",0
C-2021-2_U121,,,"""I could hardly understand the explanation of the proof of U(M) = log2M.""",,"""There were a lot of calculation formulas, and it took me a while to understand them. Among them, I could hardly understand the part mentioned in (3), and I needed to organize what I understood. I think there is, so I thought I had to review.""",-2
C-2021-2_U122,"""Entropy: How to find the amount of information; How to find the expected value; How to find the mutual information
The amount of information is less ambiguous""","""How to find expected value, how to find mutual information""",,,"""It was interesting to see how the amount of information reduced the ambiguity. In high school, the problem that was treated as a problem of probability was treated as a problem that required the amount of information, which was fresh and fun.""",-3
C-2021-2_U124,"""The reduction in ambiguity is the same as the amount of information obtained.
The reduction in ambiguity due to knowing the occurrence of an event with probability p can be expressed as −log₂p.
The expected value of the amount of information matches the entropy. ""","""I now understand the concept of reducing ambiguity, and can now perform calculations to obtain it. In addition, we are now in a world where the expected value of information is required.""","""The proof of U(M)=log₂M was difficult and took me a long time to understand.""",,"""I'm glad I was able to learn something I didn't know today.
The idea that the reduction in ambiguity is the same as the amount of information obtained made sense to me and was interesting. """,-3
C-2021-2_U125,"""About the ambiguity of the amount of information, the amount of information that can be obtained. By obtaining certain information, the ambiguity of the amount of information is reduced. Ambiguity reduction is expressed as -log2(p).There is also mutual information, which expresses the mutual relationship between multiple information quantities.""","""The ambiguity of information has monotonically increasing and additive properties. The expected value of the ambiguity and the amount of information matches the entropy. The decrease in ambiguity matches the amount of information obtained. This is called the decrease in ambiguity. can be set to -log2(p) using the probability of occurrence p.""","""A proof of the ambiguity U(M)=log2(M). On conditional entropy.""",,"""There were things I didn't understand during the preparation, and things I vaguely understood, but after listening to the lectures, I was able to deepen my understanding, and there were parts I understood, but conditional entropy was still difficult. I think I'll move my hands and try to calculate until it becomes.""",-2
C-2021-2_U126,"""Increase in the amount of information = decrease in ambiguity of information, and the quantification of ""ambiguity of information"" is equal to entropy. The expected value of information obtained under certain conditions is also equal to entropy. The amount of information that can be obtained by combining two pieces of information is called the mutual information of the two pieces of information. """,,,,"""There are some mathematical formulas and proofs here and there, and I feel like I'm not good at it, but when I review, I'll try to write proofs myself again to deepen my understanding.""",-1
C-2021-2_U127,"""The amount of information obtained with an event is a decrease in ambiguity, and ambiguity has monotonically increasing and additive properties. In addition, if the ambiguity of the information source S is U(S), then U(S) is a continuous function of the probability p of any event, it agrees with the entropy H(S)=[sum of p*(-log2p)] of S. Furthermore, given the occurrence of an event with probability p The amount of information obtained can be expressed by the reduction in ambiguity -log2p, and the expected value of the amount of information can be obtained by summing q(-log2q) where q is the probability of an arbitrary event. Since is equal to the entropy, it may be useful to use the entropy function when considering the expected value of the amount of information.The mutual information of the random variables, calculated using the conditional entropy, is the relation of the random variables shows the strength of",,,"""In what kind of situations are the expected values ​​and mutual information of the amount of information learned this time actually used?""",,-2
C-2021-2_U128,,,,"""The fact that the information that a mouse has bitten a cat is large is that if you think about it in the same way as dice, something with a low probability has occurred, so you can think, 'Oh, this event can happen.'"" Is it because you can? ""","""This time, I was able to deepen my understanding by learning about the expected value that I had heard about in high school, but I didn't understand the points mentioned above, so I would appreciate it if you could open it.""",-1
C-2021-2_U13,"""The amount of information is the reduction of ambiguity, and the expected value of the obtained information is equal to the entropy. Also, the amount of information about other events obtained by observing a certain event is called mutual information, which is It represents the strength of the relationship between two events.""",,,,"""I tend to stop thinking when formulas are lined up, so I want to face it without fear.""",-3
C-2021-2_U130,"""I learned more about what entropy is, how to calculate its amount of information, and types of information.""","""The word entropy itself was not very familiar to me, so I felt it was a little difficult at first, but when I actually listened to the class, the concept of entropy and the basic method of calculating the amount of information were much simpler than I had imagined. It was a thing, so I think I understood the basics.""","""I think I understand the concept and basics of the language, but I would like to be able to apply it even if it is a little complicated.""",,"""I was able to learn more in detail than in the previous class, so I think I was able to understand things a little faster than before.""",0
C-2021-2_U131,,"""I was able to actually solve the problem by memorizing the formula for calculating the amount of information and the expected value.""",,,,-2
C-2021-2_U132,"""I was surprised that the vague concept of ambiguity could be expressed in a mathematical formula.""","""The amount of information can be known if the probability is obtained""","""I didn't quite understand why ambiguity was expressed using log""",,"""I should have listened more carefully to the proof.""",-1
C-2021-2_U133,"""In today's lecture, I used numerical values ​​to confirm the ambiguity of information and the relationship between entropy and expected value. I learned that the amount of information that can be obtained can be obtained stochastically.""","""I think you can understand from the calculation example that the reduction in ambiguity means that you are getting information.""","""I could only vaguely understand that I(X,Y)=0 when mutual information X and Y are irrelevant.""",,"""It was easy to understand because I took the class while trying to calculate the example problems. I felt it was a little difficult because there were some complicated parts such as conditional entropy. """,-2
C-2021-2_U134,"""I was able to go back and understand how to calculate entropy and the concept of probability.""","""I learned about the concepts of entropy, ambiguity, and amount of information, and was able to calculate them.""",,,"""I was able to concentrate on my activities. By learning about entropy, I learned about probability and expected value in high school, but I had a mechanical understanding of it, so I think it was very good to understand.""",-1
C-2021-2_U135,"""By quantifying and expressing the ambiguity of information, which is difficult to grasp, it is possible to make objective comparisons so that it can be generally understood.""","""Studying mathematical statistics has improved my outlook considerably. I felt that it was easier to understand entropy and expected values.""",,,"""I thought it was interesting to see the relevance to other subjects as soon as possible.""",-3
C-2021-2_U136,"""As the amount of information increases, the value becomes clearer.""","""The amount of information can now be solved by calculation.""","""Nothing in particular.""","""Nothing in particular.""","""I still don't understand how the formula works, so I thought I'd practice and get used to it.""",-1
C-2021-2_U137,"""Information sources have ambiguity, and the amount of information reduces that ambiguity. There is also an expected value for the amount of information, and it is possible to estimate how much ambiguity can be reduced. We can also combine the two pieces of information and analyze the sources as mutual information.""","""While learning about ambiguity, I was able to understand entropy, which I didn't understand last time. I was able to understand the amount of information and how to calculate the expected value of the amount of information.""","""There are parts of mutual information that I still don't understand, so I'd like to review and understand them by the next class.""","""I wondered in what kind of situations specifically mutual information is needed.""","""I learned about expected values ​​in today's mathematical statistics class, and I learned that the amount of information also has expected values, and I found it interesting that there are various expected values. I thought it was wonderful that you derived U(M)=log2M from the nature of the inequality.In today's class, I can't say that I fully understand mutual information, so I need to prepare more. I thought it was necessary.""",-2
C-2021-2_U138,,"""Continuing from the previous lecture, entropy came out and I was able to understand it well.""",,,,-1
C-2021-2_U139,,"""I was able to understand the principle of expected value and how to calculate it.""",,,,-1
C-2021-2_U14,"""The ambiguity decreases with the amount of information obtained. The ambiguity U(M) increases monotonically with respect to M and is additive. The entropy agrees, and the entropy agrees with the expected value of the information.The mutual information indicates the strength of the relationship between the two pieces of information.""","""I understood the relationship between the ambiguity of information and enthalpy. I understood that the ambiguity decreased as the amount of information obtained increased.""","""When I(X,Y)=0, X and Y are irrelevant, but I didn't know when H(X)=H(X|Y).""",,"""I thought it was very interesting to be able to express the ambiguity of information not as a feeling, but as a numerical value. I was confused by the alphabetical notation, so I would like to review and distinguish between them.""",-1
C-2021-2_U140,"""I learned about mathematical formulas for measuring the amount of information.""","""Expression of the amount of information given by a given piece of information by means of a formula.""","""Usage of Entropy""",,,-3
C-2021-2_U141,"""Introduced the meaning of information and amount of information, explanation and calculation method of information amount and expected value, and conditional entropy.""","""I understood the meaning of the amount of information and the expected value, and became able to calculate it.""","""The conditional entropy calculation was a bit confusing.""",,"""I understood the meaning of the expected value and how to calculate it, but I was vague about what I could understand from the numbers. I thought it was important not only to prepare before the lecture, but also to review and supplement the parts I did not understand. .""",-1
C-2021-2_U142,"""There is the idea of ​​quantifying information ambiguity, which is equal to the entropy of S, given the source of the information.
Information content is the reduction of ambiguity. Further, mutual information is an index representing the strength of the relationship, where X and Y are random variables. ""","""I was able to understand the meaning of the amount of information = reduction in ambiguity and how to obtain it. I also understood that the amount of mutual information represents the strength of the relationship between one event and another.""","""My understanding of how to calculate the expected value of the amount of information is half-finished from the story of the cat's mood.""",,"""Information and amount of information are words that we often hear, but I thought it would be interesting to think of the amount of information as a reduction in ambiguity.""",-1
C-2021-2_U143,"""Continuing from the last time, there was a detailed explanation about entropy. We learned about the relationship between the ambiguity of the information source and the amount of information obtained, and learned that the calculation method is the same as the entropy calculation. Also, the presence or absence of preconditions. learned the difference in expected values.""",,,,"""I tried to prepare for class, but as I progressed to the second half, I felt that my preparation was insufficient. Before the lecture, I understood what I didn't understand, and prepared more so that I could concentrate on listening to that part. I want to put in the effort.""",-2
C-2021-2_U144,"""Quantify the amount of information, the ambiguity, and the expected value of the amount of information.""",,,,,0
C-2021-2_U145,"""Definition and measurement of information content, expected value of information content and mutual information""","""The ambiguity of a general information limit is consistent with the entropy of that information limit.
Amount of information, how to obtain the expected value of the amount of information""","""Finding connections between the contents of the second class and this time""",,"""I was worried about answering the quiz, so I thought the second review was necessary.
I forgot to draw a marker in my homework. """,0
C-2021-2_U146,"""By representing the amount of information in numbers, it is possible to compare.""",,,,,0
C-2021-2_U147,"""Regarding the amount of information. Since the amount of information = reduction in ambiguity, we learn about information by calculating the ambiguity.""","""I was surprised that the amount of information can be expressed in a mathematical formula. I realized that the amount of information = reduction in ambiguity is certainly true.""",,,"""I wondered if the relationship between a cat's mood and its tail could be quantified.""",-2
C-2021-2_U148,"""Obtaining information reduces ambiguity. If the ambiguity of information is U(M), then U(M)=log2(M), and ambiguity and entropy are equal. Knowing the occurrence of an event with probability p The amount of information obtained is log2(P).""","""We found that the information ambiguity is consistent with the entropy, and we were able to actually calculate the information ambiguity.""","""I understood up to the derivation of the inequality in the proof of U(M)=log2M, but I didn't understand why the limit was used after that.""",,"""I'm glad I was able to deepen my understanding of the vague information endo rupee last time.""",-1
C-2021-2_U15,,,"""Calculation of entropy involving conditional probability""",,,-1
C-2021-2_U151,"""The amount of information obtained is equal to the amount of reduction in ambiguity. Also, the expected value of the amount of information is equal to the entropy.""",,,,"""Sometimes I can't concentrate properly and I miss things, so I want to concentrate more.""",-3
C-2021-2_U152,"""Information ambiguity""",,,,,-1
C-2021-2_U153,"""I learned how to quantify the ambiguity of information (amount of information) by formulating it. In addition, I learned the expected value of the amount of information and the amount of mutual information. I learned about quantification.""","""I think I've become able to firmly define, content, and calculate the amount of information, the expected value of information, the reduction of the amount of information, and the amount of mutual information.""","""Nothing in particular. The proof of U(M) = log2M was also difficult to understand in the preparation stage, but I was able to understand it in class and review.""",,"""Compared to the last class, I think I was able to understand the content better. If I can understand the content, I would like to be able to understand the meaning of definitions and formulas, because my calculations are at the level of high school mathematics.""",-3
C-2021-2_U154,,,,,"""I couldn't understand entropy properly in the last class, but I think I was able to understand it properly in this class.",-1
C-2021-2_U156,"""Reduced ambiguity = amount of information obtained""","""You can find entropy by thinking about the expected value""","""I still don't understand why you use log2""","""Please explain again why you use log2""","""You said, 'I explained it carefully,' but I feel that my understanding has not caught up yet. """,-3
C-2021-2_U158,"""Entropy theorem and how to calculate it.""","""Relationship between Entropy Calculation Method and Probability""","""Calculation when entropy parameters are large""",,"""I was able to do simple entropy calculations, but I couldn't do anything complicated.""",-1
C-2021-2_U159,"""The amount of information is the reduction in ambiguity, and can be derived by a formula using logarithms. The expected value of the amount of information is equal to the entropy.""","""How to find the amount of information, how to find the expected value""",,,"""I felt nostalgic to solve mathematical problems with logarithmic calculation after a long absence.""",-1
C-2021-2_U16,"""About the entropy function
The amount of information obtained reduces the ambiguity of the event.""","""I learned that if the given information is something that rarely happens and has a low probability, the value of the amount of information that can be obtained will be large.
Simple calculations using log can be done with understanding.""","""I didn't understand that the entropy function can be obtained by H(p), but I was able to understand the meaning of the graph by studying with my friends, so I understood it to some extent.
I can do the calculation itself, but I don't understand the proof of U(M)=log₂M""","""I don't know what Pr that appears in the expected value of the amount of information represents.""",,-2
C-2021-2_U160,"""Repeat questions to get information
Reduce ambiguity = get information



""","""Reduced ambiguity = amount of information obtained

The reduction in ambiguity due to knowing the occurrence of an event with probability p is -log2p

Expected value of information
I learned how to calculate mutual information. ""","""calculate log

division of log""",,"""Information is not obtained directly, but is obtained by reducing ambiguity.
I thought it was important to memorize and understand the calculations first. """,-3
C-2021-2_U161,,"""I was able to deepen my understanding of mutual information by listening to examples of the relationship between cat moods and tails in this class.""",,,"""I had a hard time deepening my understanding this time because there were many mathematical ways of thinking, but I was able to understand by solving examples and listening to explanations.""",-1
C-2021-2_U162,,"""I became able to calculate the amount of information by applying it to the formula for calculating the amount of information. As I looked at the examples over and over again, I began to understand what I was looking for. I didn't understand it, but after listening to the explanation, I understood the proof.""",,,,-2
C-2021-2_U163,"""The ambiguity of information decreases as the amount of information increases, and increases as the amount of information decreases.
Also, the ambiguity of an information source changes according to its occurrence probability.
""","""How to calculate the amount of information
How to calculate the expected value of the amount of information

""","""Mutual information""",,"""I was able to actually use the formula, and my understanding improved.""",-1
C-2021-2_U164,"""Learn more about entropy.""",,,,,-2
C-2021-2_U165,"""Mathematically represent the ambiguity of information and find the expected value of the amount of information.""","""I was able to learn how to express the ambiguity of information numerically and how to use entropy, which I learned in the previous class.""",,,,-2
C-2021-2_U166,"""How to check the amount of information""","""Entropy function increases the amount of information about events that occur with a probability of 1/2""",,,,-1
C-2021-2_U167,"""Assuming that ambiguity can be shown by increasing the amount of information, the ambiguity of information sources and the expected value of the amount of information obtained can be shown by entropy.""","""A decrease in ambiguity can be shown as an increase in the amount of information.
・The ambiguity of information sources and the expected value of the amount of information that can be obtained can be indicated by entropy.
・The properties of ambiguity are monotonically increasing and additive. I understood it with the example of the dice.
・Entropy is important",,"""・How to calculate [｜] that appears in the definition of mutual information""","""I'm not very good at mathematics, so there were a lot of calculations, so it was difficult, but I found it interesting when I listened to the explanations. However, I'm happy that I didn't do the calculations in the final exam.""",-2
C-2021-2_U168,"""General source ambiguity equals source entropy. More information means less ambiguity.""","""It is now possible to calculate the amount of information obtained, the expected value of the amount of information, and the amount of mutual information.""","""I didn't really understand how to find the conditional entropy, so I would like to solve it by referring to the solution shown in the example.""",,"""Today's class was mostly about memorizing formulas, so I think I was able to master it through examples and exercises. I didn't understand conditional entropy very well, so I want to review it and perfect it.""",-1
C-2021-2_U169,"""Ambiguity U(M) = log2M. The amount of information obtained by knowing the occurrence of an event with probability of occurrence p is defined as the amount of ambiguity reduction, that is, -log2p. p1 = p, p2 = 1- If p, the entropy value is H(p)=p(-log2p)+(1-p)(-log2(1-p)), which is called the entropy function, and H(p) is p= It reaches its maximum when it is 1/2.In addition, the mutual information of random variables X and Y is defined as I(X,Y) = H(X) - H(XlY).""","""I solved the exercise using the formula.""","""I don't quite understand the deeper meaning of the formula.""",,,-1
C-2021-2_U17,,"""I learned how to apply numbers to each formula and how to come up with the answer. Also, I understood why the formulas are the way they are, so I was able to actually do the calculations in the exercise.""",,"""I think I have understood it, but I would like to know if there is actually an answer for the assignment.""","""I was late in submitting my assignments and journals, so I'll try to submit them a little earlier.
In addition, I would like to deepen my understanding of the content a little more in preparation for the class. """,-1
C-2021-2_U170,,,,,"""Today, I thought it would be very difficult because it involved number 3, which is not within the scope of humanities, and at the same time, I thought it was amazing to have someone whose job is to handle information, such as a programmer.""",-2
C-2021-2_U171,,"""We found that the amount of information obtained is determined by the reduction in ambiguity. We now know how to quantify ambiguity.""",,,"""I forgot a little about logarithms, so I'm going to review the math content I learned in high school.""",-3
C-2021-2_U174,"""The meaning of the terms ambiguity and entropy and how they are calculated.""",,,,,-3
C-2021-2_U18,"""Entropy is something that expresses the ambiguity of information, and it is possible to calculate the decrease in the amount of information by calculating it.""","""I understand the entropy and the amount of information and the meaning of the formula.""",,,"""I learned a new way of thinking about the ambiguity and amount of information that I normally don't think about.""",-1
C-2021-2_U19,"""The ambiguity of information, the amount of information that can be obtained, and the expected value can be expressed numerically.""","""This time, I was able to prepare for the class, so I was able to take the class with a certain degree of understanding.""","""I was able to memorize the formula as the definition, but I doubt I fully understood why it happened.""",,"""It was easier to understand because it was more specific than last time.""",-2
C-2021-2_U20,,"""I know the definition. Somehow, I understand it to the extent that it is.""",,,,-2
C-2021-2_U21,"""About the ambiguity of information, I confirmed that 'the less ambiguous, the more information I could get,' and quantified the ambiguity and learned its properties and theorems. ""","""I understand that less ambiguity = amount of information obtained.""","""I didn't quite understand the amount of mutual information.""",,"""I was able to clarify what I understood and what I didn't understand. I realized once again the importance of preparation and review.""",-3
C-2021-2_U23,,,,,"""It was the most difficult class I have ever taken.""",-3
C-2021-2_U24,"""#NAME?""","""Computation of amount of information""","""Mutual information was a bit difficult to understand""",,"""It was a panic case with a lot of numbers coming out, but I think it wasn't complicated if you followed the steps carefully.""",-2
C-2021-2_U25,"""Relationship between amount of information, expected value of information, and ambiguity.""","""When the entropy function is maximized.""","""To fully understand entropy.""",,"""In the quiz, I made the worst mistake of choosing the wrong option even though I knew it. I will be careful not to do that again next time.""",-3
C-2021-2_U26,"""As the ambiguity of knowledge decreases, the amount of information obtained increases.
The fuzziness of a source S is given by log2(M), which is equal to S's entropy.
The amount of information obtained by knowing the occurrence of an event with probability of occurrence p is defined as -log2(p).
Therefore, the amount of information obtained by knowing the occurrence of rare events is greater than the amount of information obtained by knowing the occurrence of frequently occurring events. ""","""It turns out that the reduction in ambiguity can be quantified with a formula.
In addition, we were able to know when the expected value of the amount of information becomes large when the occurrence probability of the event is what.
""","""Nothing in particular.""","""Nothing in particular.""","""In the previous class, I didn't fully understand what entropy is, but through this class, I was able to understand it. I was surprised to be asked for the expected value of the amount. I will do my best in the next class as well.""",-2
C-2021-2_U29,"""The ambiguity U(M) becomes log2M.
Expected amount of information. Mutual information and calculation method. ""","""The amount of information given is a reduction in ambiguity.
I understood the concept and calculation method of the expected value of the amount of information and the amount of mutual information that I learned this time. ""","""There was something I didn't understand in the process of deriving U(M)=log2M, so I would like to review it.""",,"""I felt that it was more difficult than last time, so I want to be able to understand the preparation and review properly.
I wanted to know more about the entropy of information content.""",-3
C-2021-2_U3,"""・About ambiguity U(M)
Property: monotonically increasing/additive with respect to M (=1...same as throwing a die with MN)
Theorem: U(M) = log2M
　　　Entropy of information source S = ambiguity U(S)
　　　The reduction in ambiguity due to knowing the sanity of an event with probability p is -log2
・Entropy function
　Assuming p1=p, p2=1-p, the entropy value is H (p )=p (-log2p) + (1-p) (-log2(1-p))
　H(p) is maximized when p=1/2
・Mutual information
Definition: Define the mutual information of random variables X and Y as I(X, Y)=H(X)-H(X｜Y). ""","""I learned that ambiguity and the reduction of ambiguity, which cannot be quantified, can be made visible and calculable through informationization. Also, the probability that we have learned in classes so far works and can be used in this way. I also understand that it is","""I didn't get a very solid understanding of mutual information.""",,"""It was easy to understand because he slowly explained things that many people thought they didn't understand.""",-3
C-2021-2_U30,"""Expressing entropy numerically""","""The higher the ambiguity, the higher the entropy""",,,"""I didn't really understand what ambiguity was, but I learned that when the probability is biased towards one or the other, the ambiguity is small.""",-1
C-2021-2_U31,"""The ambiguity of the information source S is equal to the entropy of S. Also, by considering the amount of information obtained by knowing the event of the occurrence condition p as the amount of ambiguity reduction, we can use the formula. From the formula, we can see that the more rarely an event occurs, the more information can be obtained. The expected value of the amount of information matches the entropy.""","""We were able to quantify the amount of information that could be obtained.
Understand the importance of entropy. ""","""I couldn't really understand the meaning of the formula.""",,"""It was a little difficult because there were various formulas and the calculations were complicated.
I thought it would be interesting to be able to express the amount of information in numerical terms. """,-2
C-2021-2_U33,"""I learned the concept of source ambiguity and entropy. The entropies H(S) of sources S and S match.
Ambiguity reduction goes to -log2p. H(p) is maximized when p=1/2. ""","""Understood that the more narrow the candidates, the less ambiguous the source, and I accepted that it was -log2p. I was able to deepen my understanding through examples.""","""The final mutual information was difficult to understand. I will understand it through review.""",,"""It was a little difficult, but the example of the relationship between a cat's mood and its tail helped me understand. Thank you again for this time.""",-2
C-2021-2_U34,"""About the amount of information and the calculation method for obtaining the expected value of the amount of information.""","""Based on the formula, I was able to understand that the ambiguity of a common source of information corresponds to its entropy.""",,,,-1
C-2021-2_U35,"""The amount of information obtained by knowing the occurrence of an event with probability of occurrence p = the amount of reduction in ambiguity. The amount of reduction in ambiguity is expressed as −log₂p. The expected value of the amount of information corresponds to entropy.
""",,,,,-1
C-2021-2_U36,"""The amount of ambiguity reduction in general information is equal to the amount of information obtained by knowing the occurrence of an event with probability p, and can be expressed as -log₂p. The expected value of information is the value of entropy. matches.
""","""We were able to obtain the amount of information obtained by using the probability of occurrence of the event. In addition, the size of the expected value of the amount of information obtained can be determined by understanding the meaning of the formula that expresses it and using the entropy function. I was able to tell them apart.""","""I didn't understand why mutual information was defined that way.""",,"""I learned that the amount of information obtained can be quantified by calculating the log. The reason why the expected value of the amount of information obtained agrees with the entropy expression depends on the reduction in the ambiguity of each event. I thought it would make sense if we took it as the product of multiplying the probability of an event's occurrence and summing it up.""",-1
C-2021-2_U37,"""Entropy and information content""","""The meaning of entropy and the meaning of its definition""",,,"""I want to continue learning about information""",0
C-2021-2_U38,,,"""I didn't fully understand the proof on page 10.""",,,-2
C-2021-2_U39,,"""I understand the derivation of information ambiguity, the calculation of the amount of information obtained and the entropy.""","""is not.""","""Also, the time to submit this journal is late, but should I submit the journal only before the next class?""","""More and more numbers and letters came out, it became difficult and it felt like mathematics.""",-1
C-2021-2_U4,,"""entropy""",,,,0
C-2021-2_U40,,,,,"""I did something like mathematics for the first time in a while with the proof of U(M)=log[2]M. I can't prove it by myself, but I was a little relieved that I could understand it to the extent that I could understand it by looking at the proof. Today's lecture The math aspect was strong, so I want to review it thoroughly so that I won't be left behind.""",-1
C-2021-2_U41,"""The greater the amount of information obtained, the less ambiguity. Here, expressing ambiguity as U(M), U(M) is monotonically increasing, and U(MN) = U(M )+U(N), where when the probability is uniform in x, U(M) is equal to log2x (where 2 is the base) and is equal to the entropy of M. A memoryless stationary source The information ambiguity U(S) is also equal to the entropy of S for , where the reduced information ambiguity is −log2p when we know that probability p has happened, and the probabilities p1,p1,p3 ...pn, the expected value of the amount of information obtained by knowing which event occurred can be calculated as p1(-log2p1)+p2(-log2p2)+・・・pn(log2pn) , coincides with the entropy.At this time, when there are only two events, the entropy reaches its maximum value when the probability is 1/2.In addition, mutual information is an indicator of the strength of the relationship between two events. , which derives the conditional entropy (H(A/B)) from the entropy of one of the two events (let this be A and the other B) When the mutual information is 0, there is no relationship between A and B in these two events.""","""Log2p (base 2) expresses ambiguity, and it was found that -log2p indicates a reduction in information ambiguity. Also, ambiguity coincides with entropy, i.e. entropy is It turned out to be one indicator of ambiguity.""","""I didn't really understand the proof that the ambiguity U(S) is log2S (where 2 is the base). bottom.""","""Why does k<nlog2M<k+1 than the monotonicity of log2x?""","""I decided to write down the proof of the ambiguity U(M) = log2M (where 2 is the base) on paper and think about it.
""",-1
C-2021-2_U42,"""For knowledge ambiguity, we denote its quantity by U(M). This value is monotonically increasing with respect to M and has additive properties. The ambiguity U(S) of a uniformly distributed information source S is S is equal to the entropy of .""","""The amount of information represents the reduction of ambiguity, and the amount of information obtained by knowing the occurrence of an event with probability of occurrence p is expressed as −log2⁡𝑝.""",,,"""I learned again that the occurrence of ordinary phenomena is closely related to mathematics. I would like to study while being conscious of the connection with mathematical statistics, which I am currently taking.""",-1
C-2021-2_U43,"""The amount of information can be thought of as the reduction in ambiguity. There is also the concept of the expected value of the amount of information obtained.""","""The expected value of the amount of information obtained matches the entropy, and from the entropy function, it takes the maximum value when P = 1/2.""","""Conditional entropy was difficult.""","""Nothing in particular.""","""It was shocking that the expected value and the entropy matched.""",-1
C-2021-2_U44,"""What is important when measuring the amount of information is the amount of ambiguity, and the amount of information obtained can be measured by the amount of reduction in ambiguity. The expected value of the amount of information obtained is determined by entropy. match.""","""I was able to understand the method of deriving the expected value of information and the method of obtaining mutual information as an application.""",,,"""I didn't understand much at the preparatory stage, but after listening to the explanation, I was convinced.""",-1
C-2021-2_U46,"""The amount of ambiguity that is reduced depends on the quality of the information given.""","""You know how much the given information reduces ambiguity. When you have two sources of information, you know which one is of higher quality (reduces ambiguity).""",,,,-3
C-2021-2_U47,"""I want to be able to express ""ambiguity"" in quantity. Reducing ambiguity increases the amount of information. The expected value of information is equal to entropy. ""","""I understood the relationship between the expected value and the entropy function. I learned that problems like Exercise 2 can be answered once the probability of occurrence is known.""","""I can intuitively understand the relationship between the amount of information and ambiguity, but I'm not sure if I fully understand it.""","""I feel like I don't fully understand the content of (3).""",,-3
C-2021-2_U5,"""The information source of the event that rolls the dice or draws cards is ambiguous. Therefore, we theorize by using the fact that this ambiguity is equal to the entropy of the information source, and obtain the amount of information by substituting the probability of the event. Also, the expected value of this amount of information matches the entropy.""","""Using the ambiguity theorem, we were able to obtain the amount of information from random phenomena. Also, we were able to visually grasp the magnitude of entropy from the outline of the graph of the quadratic function.""","""I didn't feel comfortable with the limit part in the proof because I didn't touch on Math III.""",,"""It was fun because I was able to solve the problems at a high pace. I gained confidence that I could handle entropy myself.""",-3
C-2021-2_U51,"""I learned how to judge the amount of information as 1 or 0 and gradually narrow it down.""","""I was able to understand the expected value of ambiguity.""",,,"""There are quite a lot of numbers, and I felt that it was difficult for me, who is a humanities major, but I want to understand it properly because it can be the basis of my daily thinking if I learn it.""",-2
C-2021-2_U53,"""This time, I studied about the amount of information and the expected value of the amount of information. I learned that it is possible to calculate the amount of information using logs, etc. The method of outputting the expected value is very interesting. .""","""It's been a long time since I've done a lot of calculations, so it was a lot of fun. I didn't know you could calculate the amount of information, so I'm happy to add another seven.""","""I think the contents of this time are still easy, so there's nothing I didn't understand.""",,"""It was a very interesting lecture. The amount of information and calculations were very interesting. Log calculations were a little complicated, so I struggled, but I was able to get a sense of accomplishment. I will do my best again."" """,-3
C-2021-2_U54,"""Reduced ambiguity means increased amount of information obtained. That ambiguity reduction can be expressed as -log2p. Compute conditional entropy like conditional probability can be done.""",,,,"""I got a perfect score on the quiz, so I want to keep it up. I learned what I didn't understand last time by reviewing it at home, so I'm going to review it again this time. Calculating entropy will help you understand the expected value and ambiguity. I thought it was versatile because it was possible to obtain even the property and conditional entropy.""",-1
C-2021-2_U55,"""The reduction in ambiguity is the amount of information obtained, and when calculating this, if the probability of occurrence is P, it will be −log₂P. The expected value of the amount of information is p1(-log₂p1)+....pm(-log₂p ), which agrees with the entropy.Regarding the mutual information, the mutual information of the random variables X and Y is defined as I(X,Y)=H(X)-H(X|Y).When X and Y are unrelated , I(X,Y)=0.
""","""I was able to learn how to obtain a specific amount of information.""","""There was nothing in particular that I didn't understand today, so I want to review it thoroughly.""",,"""First of all, I was very satisfied when I heard the explanation that the amount of information obtained was consistent with the reduction of ambiguity. It was very interesting because it was related to the contents of the previous and the previous one.""",-1
C-2021-2_U56,"""The decrease in ambiguity is equivalent to the amount of information obtained. The ambiguity U(M) has the property that it increases monotonically with respect to M and is additive. The ambiguity U( S) agrees with the entropy H(S) of S under the assumption that U(S) is a continuous function of probabilities p1,p2, etc. The amount of information that can be obtained by knowing the occurrence of rare events is large. The expected value of the amount of information is equal to the entropy.If the mutual information is defined as I(X,Y)=H(X)-H(X | Y), when I(X,Y)=0, X and Y is irrelevant.""","""I was able to understand how to obtain the expected value of the amount of information. I also understood how to prove U(M) = log_2(M).""","""Mutual information was difficult to understand because conditional probability was also involved. I felt it would be difficult to figure it out on my own.""",,"""The tables and illustrations made it easy to visualize, and I was able to understand the things that were difficult to understand, so it was a very informative class.""",-2
C-2021-2_U57,"""If I were to describe today's lesson in one word, it would be 'expressing the accuracy and amount of information in numerical terms'. The ambiguity of the information source and the expected value of the amount of information obtained can be expressed by the entropy of the information source S. Mutual information is also defined, so it can be expressed numerically. ""","""Words such as 'This is too vague to understand' and 'I have obtained a considerable amount of information' appear in everyday life, but by listening to this class, I can think of them as concrete numbers. It became so. ""","""Nothing in particular.""","""Nothing in particular.""","""Because I was well prepared for the class this time, I felt that I was able to understand the content better. I will do my best without neglecting the preparation and review for the next class.""",-2
C-2021-2_U58,"""Fuzziness and entropy are equal.
The less likely an event is to occur, the greater the amount of information that can be obtained when it occurs.
""","""I know how to use entropy.
""","""I didn't understand the proof at all because it was a liberal arts course.""","""I still don't know where to submit.
""","""difficult.
Not sure if I can keep up. """,-1
C-2021-2_U59,,"""I was able to calculate the expected value of the amount of information using entropy, which I learned last time. Also, I was able to understand to some extent the proof on page 10 that I didn't understand in my preparation.""","""It was difficult to understand the term 'expected value' of the amount of information. ""","""Nothing in particular.""",,-2
C-2021-2_U6,"""Using the fact that ambiguity decreases as the number of trials (amount of information) increases, we know that the entropy formula is correct. And vice versa, we can obtain information by knowing the probability of occurrence of an event. We get more information from events that can be obtained and rarely occur.When there are two pieces of information, the conditional entropy can be obtained from the conditional probability of the two pieces of information.The mutual information can be obtained from the relationship between the two pieces of information. be done.""","""Because I was able to confirm why entropy becomes that formula, I was able to understand entropy somehow. I was also able to understand the meaning of mutual information.""","""Since the proof of the reduction of ambiguity required knowledge of number 3 (limits and pinching), the meaning suddenly became unclear. I also did not understand the meaning of the entropy function, so I would like to review it.""",,"""Overall, I was able to understand it somewhat, but on the contrary, I feel that I only understood it vaguely. If I proceed to the next step without a solid understanding, I will not be able to understand it even more, so this time, I will focus on exercises, etc. I want to make it firmly established.""",-2
C-2021-2_U60,"""Assuming that the ambiguity of the memoryless stationary information source S is U(S), when an event with probability p occurs, the amount of ambiguity reduction due to it is expressed as -log₂p.
Let pk(k=1~n) be the probability of occurrence of event Ak, and the expected value of the amount of information obtained can be expressed as ∑(k=1~n)(-log₂pk)/pk. This corresponds to the ambiguity U and its entropy H.
The mutual information of random variables X and Y can be expressed as I(X,Y)=H(X)-H(X|Y). Conditional entropy H(X|Y) is expressed by H(X|Y)=∑(k=1~n)pYk*H(pX(Yk )). ""","""It turns out that the entropy itself is consistent with the ambiguity of the source.""","""I didn't know what to find out by finding mutual information.""",,"""Since the content is getting more and more difficult, I thought I should prepare and review every day.""",0
C-2021-2_U61,"""Relationship between information content and entropy""","""Amount of information reduces ambiguity""","""I didn't know much about entropy, so I'll review it.""",,"""I feel like it's suddenly become difficult""",-1
C-2021-2_U62,"""Reduction of ambiguity, amount of information obtained, entropy, etc. can be quantified using logarithms.""",,,,,-3
C-2021-2_U63,,"""How to find the amount of information and mutual information""","""I didn't fully understand it, but I just vaguely understood it.""",,,-3
C-2021-2_U64,"""In the third class, we learned about the amount of information and the expected value of the amount of information. The amount of information obtained leads to a reduction in ambiguity, and this ambiguity is additive. ) is equal to the entropy H(S) of S, and the amount of information can also be calculated from this.In addition, the expected value of the amount of information is equal to the entropy, and the entropy function can be used to compare the expected value of the information amount. Mutual information can be obtained by obtaining the conditional entropy, which can reduce the ambiguity of the information.""","""First, knowing the amount of information reduces ambiguity, and this ambiguity is very important for information. Also, ambiguity is equal to entropy, and by using the entropy formula and entropy function, I was able to solve the problem of comparing the amount of information and the expected value of the amount of information.I also learned that mutual information can be obtained by conditional entropy and how to calculate it.""","""I found it difficult to prove the ambiguity U(M). Also, I had a vague understanding of how the expected value of the amount of information and the entropy are equal to the ambiguity, but I was not able to fully understand it. .""",,"""Today, I was able to learn about the amount of information in detail. It was also good that I was able to solve the ambiguity and calculation of the amount of information through practice problems. I want to.""",-2
C-2021-2_U65,"""Knowing the occurrence of an event with probability p = reduction in ambiguity -log2p
(You can get more information if you know that it rarely happens) ← Surprisingly

Expected value of amount of information = entropy
The value of entropy becomes maximum when p=1/2

Mutual information of random variables X,Y
I (X, Y) = H (X) - H (X | Y) ← 0 if irrelevant","""Calculating Entropy""","""It was difficult from the amount of mutual information.
I learned it after reviewing. The examples are very clear.""","""Nothing in particular""",,-1
C-2021-2_U66,"""The amount of information = reduction of ambiguity is expressed using a formula. The meaning of mutual information and its definition.""","""I became able to express the amount of information that can be obtained in concrete numbers. I was able to understand the expected value because it was the same feeling I had when I was in high school. How to calculate the amount of mutual information. And I know what that means.""","""I still don't understand log2 enough to teach people.""",,"""It is easy to understand in words that the amount of information that can be obtained by knowing the occurrence of an event that rarely occurs is large, but it is strange that it can be expressed by -log2P. I understand that the amount of information that can be obtained increases, but I still think that my understanding of log2 is still insufficient. I want to do it.""",-2
C-2021-2_U67,"""When specifying or quantifying the amount of ambiguous information, ambiguity and entropy coincide. In addition, the amount of information increases when it rarely occurs. And when obtaining the expected value of the amount of information, it also coincides with entropy. At this time, when there are only two, 1/2 is the maximum.In addition, when showing about four related information, it is possible to show what is called mutual information.""","""When I was in high school, I only thought of information in terms of probabilities, so I didn't know there was a way to show information using log. I was able to understand the contents well except for the mutual information part. """,,,,-1
C-2021-2_U68,"""The amount of information obtained from knowing that an event with probability p has occurred is expressed as -log2(p). This is equal to the reduction in ambiguity, and the amount of information obtained from knowing that a rare event has occurred. (and at the same time a large amount of ambiguity reduction).""","""I found that the concept of entropy, which was mentioned last time, matches the expected value of the amount of information.""",,,"""The amount of information was defined as -log2(p), which satisfies the fact that even if we know that an event that absolutely happens (p=1) will occur, we cannot obtain any information (-log21=0). I found it interesting.
The calculation of the expected value of the amount of information is the sum of (amount of information) * (probability), and the calculation method is similar to the average codeword length, so it was easy to imagine. """,-3
C-2021-2_U69,"""The main content of this lecture was about the amount of information. Among the amount of information, I learned about the ambiguity of information and the expected value of information, and learned how to actually express the ambiguity and expected value as numerical values.""",,,,,-1
C-2021-2_U7,"""How to measure the amount of information
Amount of information can be obtained by reducing ambiguity
Additiveness in the nature of ambiguity
If the probability of occurrence is P, the amount of information is -log2 P
The expected value of the amount of information matches the entropy
Sum of occurrence probability x (-log2 occurrence probability)
　Mutual information
""","""The fact that the amount of information can also be defined by a formula
""","""I didn't understand H""",,,-1
C-2021-2_U70,"""The amount of information is the amount of reduction in ambiguity, and the smaller the ambiguity, the greater the amount of information that can be obtained. From the probability of occurrence, the amount of information and its expected value can be calculated as a mathematical formula.""","""We found that if we know the probability of occurrence, we can calculate the amount of information as a formula and compare the magnitudes.""","""I didn't quite understand the explanation of the entropy function.""",,"""In this class, I did a lot of calculations by myself, so I was able to concentrate more than I had in previous classes.""",-1
C-2021-2_U71,"""With the theme of measuring the amount of information, we will learn about information ambiguity and information expected value. Information ambiguity can be quantified using log, and expected value is expressed using probability and log is possible, which agrees with entropy. I also studied mutual information.""","""In today's lecture, we learned about the quantification of information ambiguity and the expected value of information. Information ambiguity can be reduced by obtaining new information, and the amount can be defined using log. and the expected value can be expressed in terms of probabilities.""","""I couldn't understand the formula for deriving U(M)=log(2)M in today's lecture, so I'm going to use my own hands to understand it.""",,"""Today's class was the hardest in my class so far, so it was difficult. I will do my best to prepare and review.""",-1
C-2021-2_U72,"Added the concept of ""ambiguity"". For example, when you choose one of N numbers from 1 to M (M is a natural number of 2 or more), the information to know what it is is missing. It is defined as large. Considering the ambiguity as a function on M, this function is monotonically increasing and satisfies U(MN)=U(M)+U(N). This leads to U(M)=log2(M). The ambiguity S(M) of any memoryless stationary source S is equal to S's entropy. If the options from 1 to M are reduced from 1 to M', that is, if an event with probability M'/M occurs, M'/M=p and the reduction in ambiguity is expressed as -log2(p). Then the expected value of information obtained when an event with probability p occurs also agrees with the entropy of S. It was shown that the identity of entropy can also be expressed as the expected value when certain information is obtained. We also prepared the concept of mutual information. This is based on conditional probability, and quantifies how much the ambiguity of one event is reduced when the identity of another event is grasped. ""","""It was refreshing to grasp the actual state of entropy.""","""Nothing in particular""",,"""This time the content was pretty easy to understand.""",-2
C-2021-2_U74,"""Proving theorems and various uses of entropy""","""From finding entropy to thinking about expected value.""",,,,-2
C-2021-2_U75,"""In information, the amount of information obtained increases as the ambiguity decreases. The ambiguity is expressed as U(M), and assuming U(2) = 1, we obtain U(M) = log2M. The ambiguity U(S) matches the entropy of S.
The amount of information obtained by knowing the occurrence of an event with probability of occurrence p is defined as the decrease in ambiguity - log2p.
Also, the expected value of the information obtained when we are told about the occurrence of a certain event matches the entropy.
The mutual information I(X,Y) of some random variables X and Y is given by I(X,Y)=H(X)-H(X /Y).
""","""I understood the concept of ambiguity in information. I thought symbols and functions that express ambiguity would be difficult, but I was able to grasp the meaning and understand it, so I'm glad. .""","""There were a lot of calculations and some parts were a little confusing, so I would like to actually solve the exercises and become able to calculate by myself.""",,"""I felt that there were many things that seemed difficult to calculate, so I would like to practice so that I can get used to calculating before the next class.""",-2
C-2021-2_U76,"""For an event, obtaining the answer to the question reduces the ambiguity, which is equal to the amount of information obtained. Let U(M) be the amount of ambiguity, monotonically increasing with respect to M, U(M)=log2(M) holds because of the additivity.In addition, the ambiguity of the general information source U(S) corresponds to the entropy of S.The mutual information of the deterministic variable X Y is I(X, Y)=H(X)-H(X|Y), and when I(X,Y)=0, X and Y are treated as irrelevant and do not affect each other.""","""I was able to use the log to determine the amount of information given by the reduction in ambiguity in today's content.""","""It was very difficult to use the entropy function and calculate the mutual information.""",,"""Today, compared to the previous class, there is a lot more calculation content, so I want to review it thoroughly and solidify it. Also, there is no substance, such as 'amount of information that can be learned by listening to XX'. I found it very interesting to calculate things by predicting things. """,0
C-2021-2_U78,"""I learned about the amount of ambiguity. I also learned about the nature of ambiguity and the expected value of the amount of information.""","""I learned that the amount of ambiguity can be quantified and that the reduction in ambiguity is consistent with the amount of information obtained.""","""I couldn't figure out why the amount of ambiguity or the calculation of the expected value would be.""",,"""I was surprised to find that ambiguity can be quantified.""",-3
C-2021-2_U79,"Under the theme of measuring the ""amount"" of information, we learned how to quantify the ambiguity of information sources and derived the expected value of the amount of information. ""","""The amount of information means a reduction in ambiguity, and the rarer the event, the greater the amount of information that can be obtained by knowing the occurrence of that event.""",,,,-1
C-2021-2_U8,"""In measuring the amount of information, the reduction in ambiguity is equal to the amount of information obtained. The amount of ambiguity is denoted by U(M). The ambiguity U(M) increases monotonically with M and is additive The ambiguity U(S) of a source S agrees with the entropy of S under the assumption that it is a continuous function, given by U(S) = log2M.Therefore, the probability p The reduction in ambiguity due to knowing the occurrence of an event is −log2p.In other words, the amount of information obtained by knowing the occurrence of an event that rarely occurs is large.The probability of occurrence of events A1, ~, and Am is p1 The expected value of the information obtained when ,~,pm is p₁(-log₂p₁)+~+pm(-log₂pm), which is consistent with the entropy.In addition, the entropy function reaches its maximum when p=1/2 The amount of mutual information that takes into account the relationship between two events is called mutual information.Probabilities include joint probability and conditional probability.The mutual information of random variables X and Y is I(X,Y)=H(X )-H(X｜Y).At this time, even if X and Y are interchanged, the answer is the same, and when X and Y are unrelated, I(X,Y)=0.""","""It turns out that the entropy, which is the lower bound of the average codeword length, and the ambiguity of the information source are equal and can be visualized. It reminded me of the ideas of joint probability and conditional probability. A simple calculation of the amount of information is Now you can.""","""I didn't understand how to calculate the mutual information. Also, it took me a while to calculate the amount of information when there were multiple amounts of information.""","""Should the probabilities used in the mutual information calculation be conditional probabilities?""","""In this class, there were many scenes that required calculations, and I felt that knowledge of mathematics was also very important. I thought that I would take this opportunity to thoroughly review what I studied in high school.""",-3
C-2021-2_U80,"""There was an explanation about the amount of information and the nature and reduction of ambiguity. There was also an explanation about the relationship between the expected value of the amount of information and entropy, and the amount of mutual information.""","""I think I was able to acquire the concept of reducing ambiguity. I was also able to learn about the relationship between the expected value of the amount of information obtained and entropy.""","""My understanding of mutual information is a bit dubious.""","""Nothing in particular.""","""In today's class, I was most impressed by the concept of reducing ambiguity. There is a field of study that uses this kind of thinking with the sense of ``if you say it, it will be true.'' That's interesting. """,-1
C-2021-2_U81,"I learned to analyze and measure the ""quantity"" of information from the perspective of ambiguity, and learned the theorems related to it. ""","""I was able to understand how to measure the amount of information, specific calculations, expected values ​​and mutual information.""",,,"""I'm getting more and more into science. I spent a lot of time preparing for lessons, so I was able to learn smoothly.""",0
C-2021-2_U82,"""It was about the ambiguity of information and the expected value of the amount of information.""","""I was able to understand the expected value of the amount of information.""","""Nothing in particular.""","""Nothing in particular.""","""I was able to keep up with the class.""",-2
C-2021-2_U83,"""Entropy is the amount of information, the reduction of ambiguity""","""I found the relationship between the probability of occurrence and the decrease in entropy""",,,,-2
C-2021-2_U85,"""I learned what information content is and the relationship between information content and entropy.
In addition, I learned how to obtain the amount of information using calculations. ""","""A decrease in ambiguity is the amount of information obtained. The amount of ambiguity is represented by U(M). The ambiguity U(M) is monotonically increasing and additive with respect to M. Occurrence probability p The amount of information obtained by knowing the occurrence of an event is defined as the amount of ambiguity reduction, that is, -log2p.The amount of information obtained by knowing the occurrence of an event that rarely occurs is large.
Let the occurrence probabilities of events A1, . . . , Am be p1, . When asked which event occurred, the expected value of the amount of information obtained is p₁(－log₂p₁)＋…＋pM(－log₂pM)
becomes. This agrees with entropy.



""","""I stumbled a bit on my calculations.""",,"""It's been a while since I had to do log calculations, so it was a bit difficult, but I managed to understand it. I'll do my best to keep up with the class.""",-3
C-2021-2_U86,"""The amount of information = the reduction of ambiguity, and the amount of information obtained by knowing the occurrence of an event with probability of occurrence p is expressed as -log₂p.""","""I learned how to express the amount of information.""","""I didn't understand the proof of U(M)=log₂M.""",,"""I got confused with a lot of formulas, so I thought I'd do the math myself and figure it out.""",-2
C-2021-2_U87,,"""Both the ambiguity and the expected value of the information are the same as the calculation of entropy""","""How to calculate H(p) I don't know why textbooks do it in an instant""","""Do you check the value of log2p by yourself?""","""It's getting harder and harder""",-1
C-2021-2_U88,"""I mainly learned about the amount of information. Especially important was how to define the amount of information and how to obtain it.""","""I was able to grasp the outline of the concept of the amount of information. I was able to learn and practice the method of obtaining the amount of information.""","""I have a vague idea of ​​what the amount of information is, but it will take time to get used to the difficulty of imagining it.""",,"""Even though I use computers in my daily life, I noticed that there are behaviors that are optimized from the viewpoint of how to obtain information efficiently, like what I did this time. I make a lot of calculation mistakes, so I'm going to review them properly. I want to.""",-3
C-2021-2_U89,"""Reduction of ambiguity = amount of information obtained. If we define the amount of ambiguity with probability 1/M as U(M), then U(M) is monotonically increasing with respect to M and is additive U(MN) =U(M)+U(N) holds.U(M)=log2M is obtained when these properties and U(2)=1.Entropy=expected value of obtained information.Probability P The reduction in ambiguity due to knowing the occurrence of the event is -log2P.""","""Now I know why we use log base 2 when calculating entropy.""","""The mutual information part was a little complicated, and I had to spend a lot of time studying it to understand it.""",,"""Today's class was generally difficult, but I was able to understand it in my own way. I didn't understand the mutual information part during the class, but I was able to understand it after studying it later.""",-2
C-2021-2_U9,,,,"""Nothing in particular.""","""It was difficult. There were many parts where I got confused along the way, so I will review it regularly.""",0
C-2021-2_U92,,"""Entropy and conditional entropy can be obtained by applying the concept of probability and the concept of expected value learned in high school.
Proofs when expressing ambiguity mathematically""",,,"""There were many things that I didn't understand well at the preparatory stage, but I think I deepened my understanding by listening to the lectures and actually using my hands to prove the formulas.""",-3
C-2021-2_U93,"""I learned how to express mathematically the reduction in ambiguity that comes from knowing the outcome of trials for various probabilistic events.""","""We now know how to formulate the ambiguity reduction.""",,,,-1
C-2021-2_U94,,"""By knowing the nature of ambiguity, we are able to determine the amount of information. Furthermore, we are able to determine expected values ​​and understand interrelationships.""","""I couldn't understand the log calculation just by listening to it, so I wanted to actually try it.""",,"""It's a field where you can only vaguely understand something just by listening to it, and you'll feel like you've understood it, so I'd like to actually solve the exercises.""",-2
C-2021-2_U96,,"""I was able to understand the expected value of the amount of information. I was finally able to understand the entropy as well.""","""About mutual information. I didn't understand well.""",,"""I feel that the level of content is gradually rising, but I want to listen carefully and understand.""",-3
C-2021-2_U97,"""・About entropy ・About the expected value of the amount of information""",,,,,-3
C-2021-2_U98,"""I learned the concept of the amount of information that can quantitatively evaluate ambiguity, and studied how to calculate it.""","""Understanding the amount of information and the mechanism of calculation methods""",,,"""I had a little trouble understanding mutual information, but I will review it and understand it.""",-1
C-2021-2_U99,,"""I learned how to quantify and compare the amount of information. Last week I didn't really understand what entropy is, but this time I understood better.""","""I didn't quite understand the mutual information in the second half.""",,"""It's getting harder and harder to understand the numbers, so I thought I'd be a little more focused and take classes.""",-3
C-2022-1_U10,,,"""I understand that it is better to keep the codewords apart, but I couldn't immediately understand the image of 'when the codewords are separated by 3 or more' in the illustration on the slide, so I will study it again. """,,,-3
C-2022-1_U11,"Bit inversion occurs due to noise when bit strings are transmitted. Encoding is devised so that bit strings can be received correctly even if this inversion occurs. The Hamming distance is used to determine whether automatic error detection and correction are possible, and in order to automatically detect s errors at most, codewords must be separated by s+1 or more, and at most t In order to automatically correct errors in , codewords must be separated by at least 2t+1.""",,,,,-3
C-2022-1_U12,"""Automatic correction of errors depends on the distance between codes.
I want to increase the distance between codes so that they can be corrected, but increasing the distance takes time and money. Therefore, the channel coding theorem is used. ""","""In the example of hell, I understood the distance between rich people and the impossibility of automatic detection.""","""I didn't understand why the 2nd and 3rd rows of the table on page 47 are all 2.""","""Is the Hamming distance the number of bits that differ?
As a result, the table on page 47 onwards does not work well, and I am in trouble.
Please let me know if there are any discrepancies. ""","""Today, I was able to understand the class better by preparing for class.
I will continue to do so without forgetting it. """,-3
C-2022-1_U13,"""At most s errors can be automatically detected when the codewords are separated by s+1 or more.
At most t errors can be automatically corrected when the codewords are separated by more than 2t+1.
While the block error rate during transmission can be reduced by making the code redundant, the coding efficiency approaches zero because many codes are required to transmit a certain amount of information. ""","""Even if information is erroneously transmitted due to noise, the error can be automatically detected and corrected if certain conditions are met.
Redundancy is effective in preventing errors during transmission, but it has the disadvantage that the longer the length, the lower the coding efficiency and the lower the transmission speed.
Numerical determination of the block error rate. ""","""The channel coding theorem is ambiguous, so I want to review it thoroughly.""",,,-3
C-2022-1_U14,,,"""Hamming distance was difficult.""",,"""I was amazed at how many different methods have been devised to correct bit flipping.""",-3
C-2022-1_U15,"""When a bit string is transferred through a communication channel, bit inversion occurs due to the influence of electromagnetic waves, etc. The probability of this happening is called the inversion probability, and the error rate when transferring a block is called the block error rate. These countermeasures are It is possible to improve the communication channel and devise coding, but the former is difficult for individuals.As part of the latter, there is automatic error detection and automatic error correction, and if the codewords are separated by s+1 or more At most s errors can be automatically detected, and if the codewords are separated by 2t+1 or more, at most t errors can be automatically corrected. The difference in the number of bits in two equal bit strings is called the Hamming distance.In addition, there is also a means of error correction using repetition code, which converts 1-bit information into 2k + 1-bit information during encoding and transfers it. There is a method of decoding back to the original correct information by a majority vote when the number of bit inversions is k or less. This reduces the block error rate, but reduces the transfer speed.
""","""There are various methods to reduce the block error rate, and among them, the code redundancy method was easy to understand.""",,,,-3
C-2022-1_U16,,,,,"""Until now, I had a vague idea that computers work with binary numbers, but after integrating the classes I've had so far with today's class, I've come to understand a lot of things.""",-3
C-2022-1_U17,"""About error detection""","""Error detection is performed as a countermeasure against bit inversion. There are cases where automatic error detection is possible and there are cases where it is not possible.""",,,"""Since the story has become complicated, I thought I needed to review it thoroughly, so I'm glad I was able to review it.""",-3
C-2022-1_U19,"""Understand by theorem whether it is possible to detect or even correct certain errors in source coding.""","""In order to transmit accurate information, it is necessary to increase the distance between codes and enable error detection and correction, but the disadvantage is that the coding efficiency is reduced and the transmission speed is slowed down.""","""Unique for each channel. Does the difference between channels mean the difference in reversal probability of each channel?""",,"""I thought that the existence of the channel coding theorem guarantees the existence of the optimal solution, which is a very important point in the development of information technology.""",-3
C-2022-1_U20,,,"""It seems that sending many symbols to notice mistakes is incompatible with being able to communicate quickly.""",,,-3
C-2022-1_U21,"""Today, I learned about channel coding. I learned that improving the channel and making p in the formula smaller is a good way to reduce errors. In some cases, it is difficult both physically and economically, so I realized that it would be good to devise a method of encoding. I thought it was really amazing that an error-correcting computer could be made.""",,,,,-2
C-2022-1_U22,"""By inverting bits in channel coding, the transmitted information and the received information change. Since it is difficult to prevent bit inversion itself, efforts are made to ensure that even if bits are inverted, they can be properly decoded. .""","""I understand the inversion probability and block error rate. I understand the definition of Hamming distance.""","""I didn't really understand the technical term 'high'. ""","""Does automatic error detection detect possible errors, or does it detect certain errors?""","""I feel like the content has become quite difficult. There are some parts that I don't understand, so I would like someone who understands to explain it.""",-3
C-2022-1_U24,,,"""I still don't fully understand the communication channel capacity graphs and formulas, so I'll try searching the net myself.""",,,-3
C-2022-1_U25,"""Let the bit inversion probability be p. The block error rate when transmitting a block is p = 1 - (1 - p) ↑ k. Here, let the block size be k bits. Make p smaller. to increase the transmitted power to improve the signal-to-noise power ratio, to use a larger antenna to improve the received signal power, to cool the receiving amplifier to reduce thermal noise in the receiver, and to utilize a wider frequency band. and so on.
Automatic error detection is to detect bit inversion and have it retransmitted.
Automatic error correction is the correction of bit flips after automatic error detection.
Redundant codewords are made dissimilar so that automatic error detection and automatic error correction can be performed.
A Hamming distance is two equal length bit strings with different bits.
Automatic detection of at most s errors is possible when the codewords are separated by more than s+1.
Automatic correction of at most t errors is possible when the codewords are separated by more than 2t+1.
Error correction is possible when the number of bit inversions in each block (n bits) is k or less in n-th order repetition code (n=2k+1). Error correction is not possible when there are k+1 or more. ""","""Sending data in large blocks causes the average codeword length to approach entropy.""","""I didn't understand the channel coding theorem.""",,"""The conditions for automatic detection and automatic correction were difficult.""",-3
C-2022-1_U26,"""Even if you make a small mistake, you can fix it with error correction.
If 2t + 1 characters or more are wrong, at most t errors can be corrected
""","""Even if you make a mistake when converting the received signal, you can correct it.""",,,"""Each time, I was able to solve things that I didn't understand in the previous class, and it became easier to understand.""",-3
C-2022-1_U27,,,,,"""I learned about machine noises that we encounter in our daily life and their causes. I was able to solve the Hamming distance exercise and deepen my understanding of channel coding.""",-3
C-2022-1_U28,,"""Creating redundancy as a countermeasure against noise, etc.""","""Block Error Rate Calculation Method""",,,-3
C-2022-1_U29,,,,,"""I couldn't keep up with the second half, so I looked up the channel coding theorem, and I think I got a good idea of ​​it. By the way, I didn't really understand the proof.""",-3
C-2022-1_U3,"""When information is transmitted, errors can occur due to bit flips. Channel coding automatically detects such errors by designing each block redundant and codewords so that they do not resemble each other."" There is also a method of repeating the same code and correcting errors by majority vote, but in order to perform channel coding more efficiently, information bits and check bits are considered separately, and humming between code words is used. Make the distance as large as possible.""","""I was able to figure out how far the codewords needed to be separated from each other to detect and correct errors, and I was able to apply simple numbers to get a concrete picture of the situation.
In addition, the specific means of channel coding and their advantages and disadvantages were found. ""","""When thinking about detecting and correcting at most t errors, I was able to come up with a concrete example by substituting a simple number such as 1 for t.",,"""Knowing that error detection and correction are performed by encoding the communication channel, it is necessary to think about how to deal with errors in order to use communication equipment without any inconvenience, and to take various measures. When I have the opportunity to create a system in the future, I will try not to forget that point of view.
I missed answering only (5) in the previous study diary. I'm sorry. I added it, so I would appreciate it if you could see it. """,-3
C-2022-1_U30,"""In today's class, we learned about information source coding. When transmitting information sources, encoding can speed up communication and prevent information theft during communication. The effects of noise on communication channels The probability of this happening is called the inversion probability, and the error rate when transmitting a block is called the block error rate.The inversion probability and the error rate are not proportional.When the encoded information is restored, the automatic error rate is Detection and correction are performed.Error detection and correction are limited when possible, and do not always work.In general, when codewords are separated by s + 1 or more, at most s errors can be automatically detected. Also, in general, when codewords are separated by 2t + 1 or more, automatic correction is possible for at most t errors.In the block error rate and coding efficiency, as n increases, the error rate becomes becomes smaller, which is an advantage, but the encoding efficiency approaches 0, which is a disadvantage.""",,,,"""I learned for the first time that various systems such as automatic error detection and correction are used to transmit information accurately. Also, with such functions, no matter how good the machine is, mistakes cannot be reduced to 0. I think that it is a very important mechanism to reduce the block error rate by assuming mistakes and defects, and to find general terms and conditions for automatic error detection and correction. I would like to study efficiently while thinking about such things and devising them not only in the situation of communicating information by means of communication, but also in my daily life.""",-3
C-2022-1_U31,,,"""I didn't really understand the channel coding theorem and channel capacity.""",,,-3
C-2022-1_U32,,,,,"""I was able to learn something related to daily life, thinking that the communication channel coding that I learned today is similar to Kyushu University's student ID, so it was good.""",-3
C-2022-1_U33,"""About Channel Coding and Error Detection""","""I found that there are various error detection and correction methods""",,,,-3
C-2022-1_U34,,,,,"""The story about Ant Hell and the receptionist was very easy to understand. I couldn't find the link in the test, so I was pretty impatient. Next time, I will review the previous content with plenty of time to spare, so that I can answer in a short time. I wanted to keep it.""",-3
C-2022-1_U35,,,"""My understanding of the definition I learned today is vague, so I want to review it so that I can understand it.""",,,-3
C-2022-1_U37,,"""We found it important to separate the codewords for error detection and correction.""",,,,-3
C-2022-1_U39,"""When sending information, noise is generated, but the original information is rewritten. (When sending a bit string through a communication channel, the noise causes bit inversion. →Completely different information. (The communication channel is the path through which information passes.) If the probability of bit inversion is p, then when a block (a block of information) is transmitted, is wrong, pB=1-(1-p)^k (block size k bits) (probability other than the probability that all blocks are correctly delayed, 1-p is the probability that all blocks are sent correctly) Even if the inversion probability is small for 1 bit, it becomes quite large for 1000 bits.→The probability must be made smaller.→The noise is reduced by devising the method of encoding. →By dividing the bit string into blocks, each block is made redundant so that the codewords differ from each other by a certain amount or more, and even if there is a slight error, it is okay.(Automatic Error detection: Detect bit reversal and have it retransmitted / Automatic error correction: Correct bit reversal after automatic error detection) (Hamming distance: Bits at the same position in bit strings of equal length are different (The number of objects) <Regarding s error detection at most, automatic detection is possible if the codewords are separated by s+1 or more. , if there are two or more different codewords, it is either codeword 1 or 2, and at most error detection can be performed.)><For t error corrections, the codewords are separated by 2t+1 or more. (With regards to correction, it cannot be automatically corrected unless it is uniquely determined to which code the erroneous one applies!)> For example, if the codes you want to send are 〇〇〇 and ●●● ( Hamming distance 3), error correction is possible if the Hamming distance from 〇〇〇 and ●●● is less than 1. However, the nth repetition code is The block error rate can be reduced arbitrarily by increasing n, but the coding efficiency approaches 0 as n increases, so the transmission speed decreases and power is consumed, so it is not very efficient.→Communication channel There is an encoding theorem!!There is a more efficient way to do it than to repeat it!→Add as many check bits as needed to each block and separate them.
""",,,,"""Yamada Suzuki's story was easy to understand.""",-3
C-2022-1_U42,"""・What is Channel Coding?
・What is automatic error detection and correction?
・What is a repetition code?","""・In the middle of the communication path, noise can cause a reversal of 0 to 1 (and vice versa). There is a way to do it, but it is difficult for mobile phones, etc. Therefore, coding is devised.
・Automatic error detection is a function that detects that there was an inversion and asks for retransmission. If there is a Hamming distance of s+1 or more, at most s errors can be detected. Automatic error correction is a function that corrects errors after automatic error detection. At most s errors can be corrected with a Hamming distance of 2s+1 or more.
・Increasing the number of bits, such as sending 3 bits instead of 1, is called a repetition code. Although this reduces the error rate, it has the disadvantage of increasing the number of bits. """,,"""I think redundancy is a way to keep Hamming distance. Is this idea correct?""","""Until I entered university and took this information science course, I thought that I was just sending information, but I didn't know that there was automatic error detection and correction along the way. In class, I learned about automatic error detection and correction mechanisms, and it was very deep and I was very interested in it.It was thought to be difficult for mobile phones to deal with noise, such as the size of the transmission power and the cooling of the receiving amplifier, but the use of repeated codes is possible. I was impressed by the power of imagination in the world of information that was made possible by this.""",-3
C-2022-1_U43,,,"""Sometimes I didn't understand what the formula meant.""",,,-3
C-2022-1_U44,"""I learned how to convey information accurately.""",,,,,-3
C-2022-1_U45,,,"""Nothing in particular.""","""Is it better to have a higher coding efficiency value?""",,-3
C-2022-1_U46,"""・Automatic error detection... Detect bit inversion and have it retransmitted
→ If the codes are separated by s+1 or more (Hamming distance is s+1 or more), automatic error detection is possible for at most s errors
・Automatic error correction...Correct bit inversion after automatic error detection
→ If the codes are separated by 2t+1 or more (Hamming distance is 2t+1 or more), automatic error correction is possible for at most t errors.""","""I learned about automatic error detection and automatic error correction.""","""Since the channel coding theorem was difficult, I thought I would review it a lot.""","""Nothing in particular""","""Next time, I will do my best so that I can participate with time to spare.""",-3
C-2022-1_U47,,,"""I didn't understand the error correction by repetition code.""",,"""I made a mistake in the quiz, so I want to review it properly.""",-3
C-2022-1_U48,,"""I learned how to devise the sequence and number of codes to facilitate error detection and error correction.""","""The example of introducing a middle name didn't really resonate with me.""",,"""Continuing from the last time, I learned that the arrangement of codes makes it easier to do something. I understand well that error detection and error correction are useful, but I wonder where they are actually used in familiar situations. rice field.""",-3
C-2022-1_U49,"""When sending information using a communication channel, it is possible to correct and detect errors if the Hamming distance between the codes is large, that is, if the codes are different. Therefore, extra code is added to the code. (Redundancy) Lengthening the code reduces errors, but increases the data size and reduces the transmission speed.In addition, by adding check bits to the information bits, it is possible to reduce the block error rate without significantly reducing the transmission speed. .""","""In order to send information as error-free as possible, it is not just a matter of lengthening the code, but it is devised to consider the user and use check bits.""","""is not""","""is not""","""The ant hell example was easy to understand.""",-3
C-2022-1_U5,,,"""I didn't quite understand why the post-correction formula for the block error rate is what it is.""",,"""I felt that the story was getting harder and harder. I managed to understand it because the analogy of ant hell was easy to understand.""",-2
C-2022-1_U51,"""When communicating, the signal may be inverted for some reason.""","""Even if the probability of reversal is only about 0.001%, the probability of error is not 0.001%, but there is a much higher probability of error.""","""How do you find out why you can't send successfully after many attempts?""",,"""I found the automatic error detection feature very impressive.""",-3
C-2022-1_U52,,"""Understood above""",,,"""I want to do my best""",-3
C-2022-1_U53,,,,,"""I was impressed with the thoroughness of the countermeasures against errors due to noise, etc.""",-3
C-2022-1_U54,,"""We found that the degree of difficulty in detecting and correcting erroneous information also differs depending on the content. We found out how many errors can be detected at most depending on the Hamming distance of the code to be sent. Defense of error detection I found that it is possible to correct the error as long as the range does not overlap.In order to make the correction effective, for example, if it is an employee list, it is necessary to devise a method such as adding a middle name.""",,,"""I made a mistake in the scope of my homework, and because I didn't prepare enough, it was more difficult than usual to understand the class. I'll try not to make a mistake next time.""",-3
C-2022-1_U56,"""In the encoding stage, erroneous information such as bit reversal is often sent, and efforts are made to improve the communication channel to reduce the probability of reversal. If the code is separated by a certain distance or more, the Error detection and automatic error correction are activated. The upper limit of coding efficiency can be expressed as communication channel capacity using mathematical formulas and graphs.""","""The example of Ant Hell helped me understand the error detection and correction theorem.""",,,"""It was hard to understand because there were a lot of formulas, but I'm glad I was able to actually do the calculations and understand them.""",-3
C-2022-1_U58,"""In channel coding, improving the channel can reduce the probability of bit inversion and lower the block error rate. If it is difficult to improve the channel, it is necessary to devise coding. can be lowered by
There is automatic error detection and automatic error correction. If the codewords are separated by s+1 or more, at most s errors can be automatically detected. Also, if the codewords are separated by 2t+1 or more, at most t errors can be automatically corrected.
The block error rate of the n-order repetition code can be reduced as much as possible by increasing n, but the coding efficiency also decreases, resulting in a decrease in transmission speed. The channel capacity is a value unique to the channel and is the upper limit of coding efficiency, and can be obtained by calculation. ""","""During communication, bit inversion may occur and information errors may occur. Therefore, it is necessary to improve the communication channel and devise coding.""",,,,-3
C-2022-1_U59,,"""I understood the reason for lengthening the Hamming distance from 1 bit to 3 bits through the concept of middle names.""","""Communication capacity in the second half""",,,-3
C-2022-1_U6,"""Since noise occurs between the information source encoder and the information source decoder, there is channel coding as a means of devising coding to prevent bit inversion. The bit inversion rate and the inversion probability are Even if the size of the block is small, if the size of the block is large, the block error rate will increase.Therefore, physical cooling and increasing power are one means, but when these are difficult, the means of making blocks redundant is used. When codewords are separated by s+1 or more, at most s errors can be automatically detected, and when codewords are separated by 2t+1 or more, at most t errors can be automatically corrected. However, even if the block error rate is small, the amount of data is large and the coding efficiency is poor.","""I learned that the error rate can also be calculated using a formula. I learned that in channel coding, it is important to first divide each block into blocks and create redundancy. I also learned that a balance between coding efficiency and block error rate is important. have understood.""","""I made a mistake in the binary number calculation in the quiz. I will write it down on paper and solve it properly next time.""",,,-3
C-2022-1_U60,"""Error correction and Hamming distance.""","""Taichi Yamada's analogy was too easy to understand. By repeating it several times, I realized that I might have misunderstood the code.""","""I thought that lowering the block apology rate by devising encoding might not necessarily mean that the average codeword length was the shortest, but it turned out to be redundant and I was dumbfounded.""","""I forgot the deadline for my BRmap.""",,-3
C-2022-1_U61,"""Introduction of the concept of communication channel""","""When bit strings are transmitted through a communication channel, bit inversion occurs due to the effects of noise. Countermeasures can be taken by improving the communication channel and devising encoding.""",,,"""I lost concentration as the number increased.""",-3
C-2022-1_U62,,,,,"""The probability of bit inversion is small, but the probability of bit inversion is large when viewed in blocks, so I thought error correction was necessary.""",-3
C-2022-1_U63,,"""If the codewords are s+1 apart, s errors can be automatically detected, and if they are 2t+1 apart, t errors can be automatically corrected.
In addition to automatic detection, correction requires distance. """,,,,-3
C-2022-1_U64,"""Incorrect Information and Improvements""","""Sending information causes bit reversal
In order to improve it, channel coding and error detection""",,,,-3
C-2022-1_U65,"""About Channel Coding
When information is sent through white and black circles, bit inversion occurs due to noise. In this class, I mainly learned how to detect and correct such errors.
If the probability that the bit is reversed (reversal probability) is p (0<p<0.5), the block error rate is obtained as 1-(1-p)^k.
However, even if the value of p is small, if the block size is large, the block error rate will be large.
When encoding, first the bits are divided into blocks and each block is made redundant. A bit string for each block is called a codeword. The codewords are designed so that they are dissimilar to each other. The reason is that if the settings are not similar, errors can be automatically corrected when a reversal occurs.
From this point, when considering cases where errors can be detected and corrected, the term ""Hamming distance"" becomes a keyword. Hamming distance is simply the number of differences between two bit strings of equal length. Also, the Hamming distance is larger in error correction than in error detection. For example, when codewords are separated by 2 or more, automatic detection is possible for at most 1 error, whereas for autocorrection, when codewords are separated by 3 or more, at most 1 error is possible.
There is also error correction using repetition symbols. For example, one bit is tripled to set the Hamming distance between codewords to 3. Error correction is then possible when the inversion in the block is at most 1 bit. By doing so, the block error rate can be reduced, but the number of bits is tripled, so the coding efficiency is reduced.
In summary, the larger the minimum value of the Hamming distance, the more errors can be corrected, but the addition of redundant bits reduces the coding efficiency and the transmission speed.
""",,"""It was difficult to understand the formula for the block error rate due to the cubic repetition code.
I didn't understand even after listening to the explanation once, so I'll try to look it up again later.
""",,"""I was able to understand the lecture more smoothly than the last time by listening to the explanation while summarizing it on paper instead of using a computer.
Also, after the lecture, I was able to look back and review the content, so I will clarify what I understand and what I don't understand, and before the next lecture, I will ask my friends or check it out to deepen my understanding. """,-3
C-2022-1_U66,,"""The number of errors in a bit string is called the Hamming distance, and by setting the coverage range of each codeword in a cubic repetition code to a Hamming distance of 1 or less, an error of at most 1 bit can be corrected.
The channel capacity is a value unique to the channel and is the upper limit of coding efficiency.
According to the channel coding theorem, the block error rate can be made as small as possible, and the encoding efficiency can be brought closer to the channel capacity. ""","""The receiving side doesn't know whether the received bit string is correct or not, so I wondered how they could automatically detect the error. In the example of the employee list, similar information exists on the receiving side, so the error is detected. It may be detected, but I thought that if information such as photos were sent one-sidedly, there would be no room for verification on the receiving side.""",,"""It was good to know that errors can be detected and corrected in various ways, such as making blocks redundant and using repetition codes.""",-3
C-2022-1_U67,"""On how channel coding enables information to be transmitted accurately without being overwhelmed by noise.""","""When sending information, the sign is also inverted, but I found out that it was corrected in the middle of sending.
""","""Of the error rate formulas, I didn't quite understand the meaning and origin of the corrected formula.""","""Nothing in particular.""","""There were a lot of letters, and I was still unfamiliar with which letter meant what, so I had to check each letter one by one, so I had a little trouble understanding the content.
I wanted to get used to it through practice problems as soon as possible. """,-3
C-2022-1_U68,,"""In order to reduce the block error rate, it is possible to correct small errors by making blocks redundant, but we found that it is not just a matter of making the blocks longer.""",,,,-3
C-2022-1_U69,"""When data is transmitted over a communication channel, bits may be flipped and the wrong block may be sent. To prevent this, each block is encouraged when encoding data, and an automatic It would be nice to be able to correct errors, but in order not to reduce the communication speed too much, it is desirable to use the channel coding theorem.""","""I learned what kind of codewords can be used for automatic error correction, and what the Hamming distance is.""","""I didn't quite understand the difference between being able to correct errors and being able to detect errors.""",,,-3
C-2022-1_U7,"""Bit inversion occurs due to noise when transmitting information. Coding is devised to reduce the probability of inversion. Hamming distance determines whether errors can be detected or corrected. Cubic repetition codes also have advantages. However, it has the disadvantage of lowering the transmission speed. Therefore, we divide the message into locks and add check bits to each block to prevent the encoding efficiency from decreasing.""","""I was able to understand the pros and cons of sending information.""","""At most 〇 error detection/correction is still a bit vague.""",,,-3
C-2022-1_U70,,,"""The problem of how many errors can be detected at most was difficult. It was difficult to distinguish between automatic detection and automatic correction, but I was able to understand it in the latter half of the class.
""",,,-3
C-2022-1_U71,,,"""The larger the Hamming distance, the less data can be sent.""",,,-3
C-2022-1_U72,,"""I was able to understand the basic concept of error correction.""",,,,-3
C-2022-1_U73,,,"""Error detection (from page 21 onwards), correction of at most t errors is unknown. I also don't understand ant hell.""",,"""Even though I thought I understood the definition of the Hamming distance itself, I couldn't understand how it was actually used in the subsequent slides and what I was thinking about in Ant Hell.
I'm afraid that I can't keep up with it, so I want to research and understand properly. """,-3
C-2022-1_U74,"""About error detection Automatic detection, those that cannot be detected""","""If the code differs by three characters or more, the error can be detected and corrected immediately. If there is a one-letter difference, it will be difficult.
Hamming distance""",,,,-3
C-2022-1_U75,"""In channel coding, bits may be inverted, so we have to devise the channel and encoding. There are automatic error detection and automatic error correction, and the cases where each can be done are limited. For two bit strings , where the Hamming distance is defined.""","""I understand the difference between auto-detection and auto-correction. I understand when one is possible.""","""It was difficult from around the cubic repetition code. The calculation was complicated.""",,"""It's getting harder and harder, so I thought I'd have to do some preparation and review. I'm not catching up with my preparation, so I want to do my best.""",-3
C-2022-1_U76,,"""I understood the calculation of the block error rate. I understood why error detection was possible and in what cases error detection was not possible.""",,,"""Two concrete examples were easy to understand.
""",-3
C-2022-1_U77,,,,"""Nothing in particular""",,-3
C-2022-1_U78,,"""I found the conditions for error detection and correction.""","""I didn't quite understand the block error rate formula.""",,"""I understand.""",-3
C-2022-1_U79,,,"""I just remembered the formula, but I didn't understand the block error rate.""",,"""I didn't get it right the first time, but after reading the bookroll a few times, I think I was able to understand it a little more. It was helpful that the way of thinking using diagrams was easy to understand.""",-3
C-2022-1_U8,,"""I didn't quite understand the Hamming distance at the preparation stage, but when I heard the explanation about the number of combinations of different codes, I was able to understand the range of automatic detection and correction.""","""I was a little confused about the post-correction calculation of the block error rate.
I want to review again using my high school textbook. ""","""is not.""","""At the preparation stage, I didn't really understand the meaning of redundancy, but through this lecture, I learned that while it makes detection and correction easier, it also poses a problem of efficiency. I thought.""",-3
C-2022-1_U80,,,,,"""was fun.""",-3
C-2022-1_U81,"""Ambiguity, Entropy""",,"""Entropy Calculations and Tail Problems""",,"""was difficult""",-3
C-2022-1_U82,,,,,"""In today's lecture, I learned about channel coding after information source coding. It was difficult to understand and there were some things I was not familiar with, but it was easy to understand because I was shown concrete examples. In order for us to receive accurate information, we realized that various processes are necessary, and learning the mechanism has become more and more interesting.""",-3
C-2022-1_U83,"""Whether or not errors can be detected and corrected depends on how the data overlaps.The data is sent in units of blocks in which several grains are gathered.In order to prevent mistakes, we increase the number of pieces and send them.""","""How to correct information when it has changed and how to communicate to reduce information changes as much as possible""","""I didn't know the criteria for finding and correcting mistakes.""",,"""I think I didn't understand because I didn't prepare enough, so next time I want to prepare more carefully.
""",-3
C-2022-1_U84,"""Today's content is about how errors occur, how they are corrected, and how they are supposed to be corrected. It seems that error correction using repetition codes is quite effective.""",,"""Definition of communication channel capacity. Knowing that, I didn't know how it would affect other things.
I don't understand why error detection is easier when the Hamming distance is long. """,,,-3
C-2022-1_U85,,"""The block error rate can be improved by using residual events, and can be improved by increasing the transmission power, using a large antenna, or cooling the receiving amplifier. If there is only one letter difference, error correction is possible. , error detection is not possible if there are more than two characters or more than two candidates.""","""Why is it possible to automatically detect and correct by introducing a middle name?""",,"""Since I made a calculation mistake in the quiz, I would like to calculate it again from the next time to eliminate the mistake. Information science has become more like an academic subject, and I want to deepen it further.""",-3
C-2022-1_U86,"""About Channel Coding""","""Enables automatic error detection by making the Hamming distance greater than or equal to the condition""","""I couldn't quite understand the contents after P37""",,"""I thought you were still searching for a better way to communicate.""",-3
C-2022-1_U87,"""I learned that when detecting mistakes in code, it is easy to detect if you increase the difference between each by inserting the middle name as in the name analogy.""","""Bit flipping can occur when sending a communication code, which requires error detection and on-the-spot detection.""","""I didn't understand the meaning of Hamming distance.""",,,-3
C-2022-1_U88,,"""Understood terms such as channel coding, inversion rate, block error rate, automatic error detection, automatic error correction, Hamming distance, and coding efficiency.
I understood the basics of channel coding in noise.
""",,,,-3
C-2022-1_U89,,,"""I couldn't solve the computation of the channel coding theorem very quickly during class. Also, there were parts in the middle where I was confused about the Hamming distance, etc., and I had to try harder to understand the calculation system. I thought I should.As I repeatedly looked at the slides after the class, I gradually got them into my head, so I decided to continue my preparation and review.In addition, I also read high school math textbooks. I want to go back and review it.""",,,-3
C-2022-1_U9,"""Regarding the detection range of code errors""","""At most s errors can be detected if the codes are separated by s+1 or more.
If the codes are separated by more than 2t+1, at most t errors can be automatically corrected.
""","""About Hamming distance""",,"""I thought it was strange for a moment to increase the number of codes by myself to automatically correct errors.""",-3
C-2022-1_U90,,"""First you have to understand entropy.""",,,"""I couldn't understand it after going to the bathroom on the way. I'll review it later.""",-3
C-2022-1_U91,"""Minor mistakes can be automatically corrected""","""I learned about auto-correction with the example of the employee's name""",,"""Contents of (3) above""","""I want to deepen my understanding in the future.""",-3
C-2022-1_U92,"""In the relationship between cats, moods, and tails, I was able to understand some kind of relationship by expressing it in a table.""","""I understand the concept of entropy""",,,"""same as last time""",-3
C-2022-1_U93,"""By devising the shape of the codewords and making them easier to distinguish from each other, errors can be reduced.""","""I was able to understand the error correction.""","""I don't really understand the communication coding theorem""",,"""I was looking at the wrong slide the first time.""",-3
C-2022-1_U94,"""When information is sent, noise can cause inversion. This probability is called inversion probability. To reduce the block error rate, it is necessary to reduce the inversion probability. Automatic error detection and automatic error correction are also possible. what.""","""The block error rate can now be calculated from the reversal probability.""",,,,-3
C-2022-1_U96,"""We learned about the channel encoder and channel decoder, which are placed in front of the back of the source encoder and source decoder that we learned in the previous lesson. Noise when a train is transmitted through the channel. Bit inversion occurs due to the influence of .The probability of bit inversion is expressed using p.Even a small amount of p makes the block error rate quite large.It is costly to increase the hardware system for noise countermeasures It takes too long.The second countermeasure is to devise coding.This is channel coding.The content is automatic error detection and automatic error correction.Settings such that bit strings are not similar By doing this, errors are automatically detected and corrected.The number of bits that are not covered by the bit strings of x and y is called the Hamming distance.The bit string, that is, the interval between the codewords, determines whether errors can be detected.n In the case of the next repetition code, more than half of the errors cannot be corrected.In the nth repetition code, the error rate can be reduced by increasing n, but the communication speed can be reduced accordingly.""","""Exercises 1... Done!
Exercise 2, when the Hamming distance is 4, at most 3 error detections
　　　　　　　　　　　　　　　　　At most 1 error correction
　　　　　When the Hamming distance is 5, at most 4 error detections
At most 2 error corrections""","""I didn't know the communication capacity.""","""Why do we use distance when we say similarity?""","""I want to find out why it was defined.""",-3
D-2020_U10,,,,,"""I felt the need to review Fourier series""",0
D-2020_U13,"""I learned about periodic signals and Fourier series""",,,,"""There are a lot of calculation formulas from this time, so I want to work hard to understand them properly.""",1
D-2020_U14,"""Description of the Fourier Transform""","""Fourier is already learned in mathematics, so today's content was a review.""",,,,0
D-2020_U16,,"""Why Fourier series?""",,,,-2
D-2020_U18,,,"""What are the applications of complex Fourier series?""",,"""Fourier series, which is a seemingly complicated formula and difficult to approach, felt a little familiar.
I was also surprised to learn that all signals can be represented by summing sine waves. """,1
D-2020_U2,"""Any signal can be represented by the sum of sine waves of different frequencies. There are Fourier series representations of sine waves and complex Fourier series representations that use complex sine waves instead of sine waves. Complex Fourier series is calculated. has the advantage of simplifying","""It is now possible to express Fourier series and complex Fourier series.""","""Nothing in particular.""",,,1
D-2020_U21,"""Fourier transform review""","""I was able to remember the Fourier transform that I had almost forgotten.""",,,,1
D-2020_U22,,,,,"""I learned Fourier series in last year's math class, so I was able to understand it without any problems.""",1
D-2020_U23,,"""Understanding Fourier Series Overview""",,,"""It was organized in a short time, easy to concentrate, and easy to understand.""",-2
D-2020_U25,"""Periodic Signals and How to Represent them as Fourier Series""","""How to Represent Trigonometric Functions and Complex Fourier Series""",,,,1
D-2020_U27,,"""I understood the process of deriving the Fourier transform formula from the complex Fourier series expansion formula. I also understood functions with important properties such as the delta function.""",,,,-1
D-2020_U28,"""On periodic signals and Fourier series.""",,"""None.""","""Nothing in particular.""",,0
D-2020_U29,"""Description of Fourier series expansion, complex Fourier series expansion, and spectrum.""",,,,"""After all, the Fourier series expansion is really difficult to calculate. There are many + and - signs, and it is easy to make mistakes and it is too scary.""",1
D-2020_U3,,"""I learned that any waveform can be represented by superposition of sine and cosine waves.""",,,,1
D-2020_U31,"""A periodic signal can be represented by an infinite series of sine waves.
It can also be expressed using complex numbers using Euler's formula. """,,,,"""Fourier transform was not difficult because I learned it in two years.""",1
D-2020_U32,,"""How to find Fourier coefficients and complex Fourier coefficients. I understand what a spectrum is.""",,,,1
D-2020_U33,,"""The Meaning of Fourier Coefficients""",,,,0
D-2020_U34,"""Overview of Fourier Series Expansion""",,,,"""I feel like I'm about to forget the Fourier series formula, so I wanted to prove it and make it memorize it.""",-2
D-2020_U35,"""A periodic signal can be represented by a combination of sine waves with different periods""","""Complex Fourier series reduces the number of terms in the Fourier series""","""Spectrum Necessity""",,,-1
D-2020_U36,,"""Any periodic function can be represented by a Fourier series.""","""Physical Meaning of Fourier Series Expansion""",,"""Since the content is getting a little difficult, I would like to review Fourier series thoroughly.""",1
D-2020_U37,,,,,"""When I tried to solve the exercise, it took more time than I thought.""",1
D-2020_U38,"""All periodic signals can be represented by Fourier series""",,,,,0
D-2020_U39,"""Representing periodic functions using Fourier series.""","""The figure shows that the Fourier series can represent a periodic function.""",,,,1
D-2020_U4,"""On Fourier Series of Periodic Signals""",,,,,1
D-2020_U40,"""Fourier series transform of periodic signal and its method""","""How to find Fourier series and how to shorten the calculation depending on the case""",,,,1
D-2020_U41,"""Fourier Series and Complex Fourier Series""",,,,,0
D-2020_U42,,"""The fact that any periodic waveform can be represented in terms of sine and cosine waves, and that the use of complex numbers simplifies the equation and makes it easier to understand.""",,,,-1
D-2020_U43,"""How to express periodic signals.
A periodic signal can be represented by an infinite series of sine waves →Fourier series representation
If Euler's formula is used for this, it can be expressed as a complex number instead of a sine wave → complex Fourier series
The complex Fourier series has a simpler form than the Fourier series, making it easier to compute.""",,"""Nothing in particular.""",,"""I thought I had understood the lesson in preparation for the slides, but after listening to the teacher's explanation, I realized that there were many things that I could understand. """,1
D-2020_U44,"""Fourier transform""",,,,,1
D-2020_U45,,"""I learned how to find the Fourier series""","""Cases of Utilizing Spectrum""",,,0
D-2020_U46,,"""I was able to understand the relationship between signals and Fourier series.""",,,,0
D-2020_U47,"""Any wave can be reproduced by superimposing multiple waves""","""I had already learned about Fourier series and complex Fourier series, so it was a good review. Also, I was able to get a good idea of ​​how waves are synthesized because I had thoroughly prepared with videos.""","""As for the spectrum, I have understood the content and calculation method, but I have not yet understood how to use it, so this is my subject.""",,,0
D-2020_U48,"""How to express periodic signals mainly by Fourier series.""","""I learned that a periodic signal can be obtained by synthesizing a sine wave and a cosine wave. Also, I was able to learn about the Fourier series representation and the complex Fourier series representation of a periodic signal using this property.""","""I didn't understand how the Fourier coefficients were calculated in that way.""",,"""There were some missing parts (not so important parts), but I think I was able to understand the general idea and how to find the Fourier series. So I want to reflect.""",1
D-2020_U49,,"""The higher the harmonic order, the closer the shape of the original signal.""",,,,1
D-2020_U5,"""I learned about periodic signals and Fourier series.""","""A periodic signal can be represented by a combination of sine and cosine waves, and can be represented by a Fourier series.
Furthermore, by Euler's formula, it can be represented by a complex Fourier series. """,,,,-3
D-2020_U50,"""Periodic signals can be represented by Fourier series (or complex Fourier series). How to find Fourier series (complex Fourier series).""","""Any periodic function can be made by combining a sine wave and a cosine wave. It is easier to calculate if represented by a complex Fourier series.""",,,,0
D-2020_U52,"""I learned the basics of Fourier transforms and Fourier series used in signal processing.""",,,,"""I think I've already learned some things, but I think I've forgotten some, so I thought I'd check again by solving exercises and writing notes.""",1
D-2020_U53,,,"""I didn't really understand the spectrum""",,"""I feel like I've done the exercises. The calculations are complicated, so I want to be able to do them properly.""",-3
D-2020_U54,,"""A signal can be represented by the addition of sine waves, and a periodic signal can be represented by a Fourier series expansion""",,,"""Since it's what I've learned so far, I want to deepen my understanding while reviewing it.""",0
D-2020_U57,"""Any signal can be represented by adding sine waves
Periodic signals can be expanded into Fourier series
A complex Fourier series can also be used, which is easier to compute.""","""I understand the derivation process of Fourier series expansion""",,,,-1
D-2020_U59,,,,,"""I've studied Fourier series before, so there weren't many things that were difficult to understand.""",-2
D-2020_U62,"""Periodic signals can be represented using Fourier series.""","""A signal can be represented by the sum of different sine waves. A periodic signal can be represented using a Fourier series or a complex Fourier series. On the spectrum.""",,,"""Today I learned that signals can be represented using Fourier series. I would like to do a number of exercises to deepen my understanding of the Fourier transform of periodic signals.""",0
D-2020_U63,,"""How to find the Fourier series""","""Computation of complex Fourier series went wrong""",,"""I want to review the reflection points""",0
D-2020_U64,"""Any signal can be represented by superposition of sin and cos with different frequencies. If it is a periodic signal, it can be represented by an infinite series of sine waves.""","""I learned well how to Fourier transform a signal. I also found that using Euler's formula to put it in complex form made the calculations easier.""",,,,0
D-2020_U65,,,,,"""was difficult.""",-3
D-2020_U66,,"""All periodic functions can be expressed as series of sin and cos.""",,,,1
D-2020_U67,"""We took the limit that makes the period T of the Fourier series expansion infinite and made it possible to apply the Fourier series expansion to functions that are not periodic functions.
""",,,,,-2
D-2020_U69,"""Fourier Series, Processing as Complex Fourier Series""",,,,,-2
D-2020_U7,"""I learned how to represent signals with Fourier series expansions or complex Fourier series.
Any waveform can be expressed by superimposing sine waves, and the above method is useful as one of the means.
In addition, it is also effective to use Fourier series expansion and complex Fourier series properly as one of the devises of calculation, to use the property of even function/odd function, and to devise the interval of integration.
A spectrum is a quantity that has information about amplitude and information about phase. ""","""Correspondence between Fourier series expansion/complex Fourier series and waveforms""","""It was in the main point, 'Given a certain signal period, it is possible to obtain the magnitude of each sine wave in the Fourier series: Fourier coefficient,' but I could not understand what the Fourier coefficient meant. """,,"""Fourier Series Expansion/Complex Fourier Series has become revenge.
In addition to properly reviewing calculation methods, I would like to aim to intuitively understand the meaning of mathematical formulas in the process of reviewing. """,1
D-2020_U8,,,,,"""It was a short time, so I was able to concentrate.""",-1
D-2020_U9,,"""Any periodic waveform can be represented by a combination of sine and cosine waves of various frequencies and amplitudes.
It can also be expressed as a complex function based on Euler's formula. """,,,,-1
D-2021_U1,,,"""Image of convolution""",,"""I think I understood the Fourier transform by following the formula.""",-1
D-2021_U100,"""A periodic signal that repeats the same waveform can be represented by a Fourier series of a trigonometric function or a complex Fourier series. To be more specific about the Fourier series of a trigonometric function, the DC component of a signal can be obtained by obtaining the Fourier coefficient of the signal. and the n-th harmonic coefficients of the wave of the fundamental frequency are known and can be expressed by them.Any periodic function can be represented by a sine wave and a cosine wave.In the case of the complex Fourier, the periodic signal is exponentially can be expressed and is easier to obtain than the Fourier series of trigonometric functions.
The Fourier coefficients of the Fourier series are called spectra, and there are amplitude spectra and phase spectra. ""","""I understood how to obtain Fourier coefficients.""",,,"""It was good because I was able to understand the Fourier series of trigonometric functions graphically.""",1
D-2021_U101,,,,,"""Fourier is interesting when you know the mathematical way of thinking""",1
D-2021_U102,"""I learned about Fourier series representation of periodic signals. I also learned about the meaning of Fourier series.""",,"""I understood the meaning of the Fourier series, but it was difficult to actually calculate it.""",,"""The calculation of the Fourier series took a lot of time, so I want to speed up the calculation.""",1
D-2021_U103,"""Confirmation of how to do Fourier series was done.""","""I was able to derive and practice Fourier series and complex Fourier series.""",,"""I forgot to reload, and the automatic attendance has become absent. Please check.""","""The definition of Fourier series was different from when I learned it in technical college, and I was a little confused and delayed in understanding.
I felt the need to reconfirm. """,1
D-2021_U104,"""Today, we reviewed the basic concept of the Fourier transform and how to calculate it. Remember that the expression that any periodic function can be represented by a sine wave, a cosine wave, and a DC component is an essential part of the Fourier series. I want to.""","""It was good because I was able to find out how to do calculations through concrete exercises.""",,,"""I couldn't finish the exercises in time, so next time I'll take notes first and try to parallelize the lessons and exercises.""",1
D-2021_U13,,,"""How to process a periodic signal after Fourier series expansion
(how to process analog signals with smoother curves)""",,"""It was good because I was able to deepen my understanding of Fourier series expansion.""",1
D-2021_U14,"""Computation of Fourier series expansion, Fourier coefficients and complex Fourier series
By combining these, any signal can be made into a sine wave.""","""It reminds me of Fourier series expansion etc.""",,,,1
D-2021_U15,,"""I was able to reconfirm how to use Fourier series and Fourier transform""",,,"""Since the calculation is a little complicated, I thought I would try not to make any mistakes.""",1
D-2021_U16,,"""Periodical signals can now be represented by Fourier series.""","""There was nothing in particular, but I would be happy if there was an explanation about the DC component of the Fourier series.""",,,1
D-2021_U17,"""Fourier series""","""I just found out that cn is called spectrum.""",,,,1
D-2021_U18,"""Mathematical Representation of Signals""",,,,"""It's fun to become a professional field""",1
D-2021_U19,,,"""unit impulse""",,,1
D-2021_U20,,"""Finding Fourier Coefficients and Complex Fourier Coefficients""",,,,1
D-2021_U21,,"""There are two methods of Fourier series.""","""Complex Fourier Series""",,,1
D-2021_U22,"""I learned about periodic signals and Fourier series.""",,,,,1
D-2021_U23,"""Formula for expanding periodic signals with Fourier series and complex Fourier series.""","""Calculations for the expansion of Fourier series and complex Fourier series are often cumbersome involving complex numbers, integrals, sin and cox, so I learned that it is important to reduce the amount of calculation by identifying even functions and odd functions.""",,,,1
D-2021_U27,,"""Any signal can be expressed by adding two sine waves with different frequencies. A periodic signal can be expressed by an infinite series of sine waves = Fourier series expansion
From a given periodic signal, the magnitude of a Fourier series sine wave can be obtained = Fourier coefficient
Fourier series with complex sine waves instead of sine waves = complex Fourier series
Using a complex sine wave simplifies the calculations. ""","""spectrum""",,,1
D-2021_U28,"""Fourier Series Representation of Periodic Signals""",,,,"""Full-fledged calculation came out and it was a little difficult""",1
D-2021_U30,"""Fourier transform, Fourier series""",,"""I'm confused by the similarity of names
conversion by definition""",,,1
D-2021_U31,,,"""Part of the problem of finding Fourier series of trigonometric functions from complex Fourier series""",,"""I want to deepen my understanding of both Fourier series and complex Fourier series because they are often used.""",1
D-2021_U32,,,,,"""There were a lot of things I forgot, so I'm going to review them.""",1
D-2021_U41,"""Fourier Series Transform and Complex Fourier Series Transform of Periodic Signals""",,,,"""The explanation was concise and the lecture was over quickly, so I was glad that I had more time to research and study on my own.""",1
D-2021_U43,,"""Fourier transform""","""Inverse Fourier Transform""",,"""I didn't know what I didn't know""",1
D-2021_U45,"""Various Fourier Series""","""Using a complex sine wave simplifies the calculation""",,,,-3
D-2021_U46,"""I learned about Fourier series representation of periodic signals""","""Any signal can be represented by the sum of sine waves with different frequencies
Periodic signals can be represented by Fourier series expansion""",,,,1
D-2021_U49,"""Study of Fourier Transform, Complex Fourier Transform.
A periodic signal is expressed using a sine wave and a cosine wave. ""","""I was able to learn how to do Fourier transforms and complex Fourier transforms.
A periodic signal can be represented only by a sine wave and a cosine wave. ""","""The derivation was a little difficult, but I didn't understand anything.""",,,1
D-2021_U54,"""Enhance your understanding of Fourier series.""",,"""Complex Fourier series was difficult.""",,,0
D-2021_U57,,"""Typical Fourier Transform Calculations""",,,"""I thought there was a lot to remember.""",0
D-2021_U61,"""The Fourier transform is used to transform the signal, and there are trigonometric functions and complex number representations.""","""Understanding the Trigonometric Functions of the Fourier Transform and the Formulas of the Complex Fourier Transform""",,"""Should we be aware of the case division when n=0 in the complex Fourier transform?""","""I felt that I should approach my studies with more leeway.""",1
D-2021_U62,,"""Formulas for representing Fourier series and complex Fourier series of signals, how to find Fourier coefficients when the signal x(t) is an even function and an odd function. I also learned about the mathematical representation of amplitude and phase spectra.""",,,,1
D-2021_U63,"""I learned how to find Fourier series for Fourier transform and how to find complex Fourier series.""","""I learned how to obtain Fourier series and complex Fourier series and the principle.""","""I didn't quite understand what you were saying about the phase spectrum.""",,,1
D-2021_U64,"""Fourier Series/Complex Fourier Series""","""Formulas of Fourier Series and Complex Fourier Series""","""Derivation of Coefficients of Fourier Series""",,"""It turns out that it can be decomposed into sine waves""",1
D-2021_U67,,,,,"""It was difficult because there were a lot of calculations.""",1
D-2021_U69,,,,"""This is not part of the course content, but can I use BookRoll+LAD's chat function during the course?""",,1
D-2021_U70,,"""I was able to understand how to obtain the Fourier series and its relationship with periodic signals.""",,,,1
D-2021_U72,"""It is possible to represent periodic signals with sine waves.""",,"""I couldn't do the problem of finding the Fourier series with the specific numerical values ​​in the exercise.""","""I would like a commentary on exercises 2-3 and 2-4""",,1
D-2021_U73,"""Fourier Series Expansion for Periodic Signals""","""I was able to perform Fourier expansions of both trigonometric functions and complex numbers.
I was able to review what I learned a year ago.
I was able to research and describe the introduction of coefficients myself. ""","""Nothing in particular.
I misunderstood the deadline and was a little late in submitting the exercises, so I'll be more careful from next week. """,,,1
D-2021_U74,"""A periodic signal can be represented by an infinite series of sine waves. Fourier coefficients are used for this. A complex Fourier series can be obtained by applying Euler's formula to the Fourier series. Using a complex sine wave, Calculations become easier.""",,,,"""I learned how to calculate Fourier series in the second grade, so I was able to review it.""",1
D-2021_U75,"""A refresher on Fourier series.""",,,,"""Sometimes it took me a long time to solve the calculations, so I would like to review them regularly.""",1
D-2021_U76,,,,"""is not.""",,1
D-2021_U77,"""A periodic signal is a signal in which the same signal pattern is repeated at certain time intervals, and the minimum time interval at which the same signal pattern is repeated is called a period. If a signal is a periodic function, it is triangular. It can be represented by a Fourier series of functions.""","""I learned how to express periodic signals using Fourier series.""",,,"""I wanted to review a little of what I learned about the Fourier series last year.""",1
D-2021_U79,,,,,"""I felt that the knowledge of the Fourier series that I learned in my second year came alive. The calculations were complicated, but I was able to understand that they are important concepts for use in signal processing.""",1
D-2021_U81,,"""Representation of periodic signals by Fourier series and complex Fourier series.""","""What is the spectrum used for?
The calculation of the Fourier series was complicated and it was difficult to calculate. """,,"""I had seen the Fourier transform once in a second-year class, so I was familiar with it, but the calculations were complicated and I often made mistakes such as formula conversions, so be careful.""",1
D-2021_U82,,,"""What is a spectrum after all?""",,"""Fourier transform is a topic that comes up often after entering university, so I thought it was an important matter.""",1
D-2021_U83,"""Fourier Series, Complex Fourier Series, Spectrum""",,,,"""It was easy to understand because I was taking notes during class.""",1
D-2021_U84,"""I mainly learned about Fourier series and complex Fourier series
I also learned why we use Fourier series.""",,,,"""There was a lot of review and most of the content was easy to understand""",1
D-2021_U86,,,,,"""I forgot to write my impressions
""",1
D-2021_U89,,"""All signals can be represented by sine waves in the Fourier series representation
Calculations can be simplified depending on whether Fourier coefficients are even or odd periods
A complex Fourier series can be expressed more simply.""",,,"""I had learned about the Fourier transform before, but I didn't know much about the Fourier series that underlies it, so it was a good opportunity to review.""",1
D-2021_U90,"""Today, we reviewed how to find the Fourier series based on the content of the second graders.""",,,,,1
D-2021_U91,,,,,"""It seemed difficult to calculate.""",1
D-2021_U93,"""All periodic signals can be represented (Fourier transform) by combining sinusoids.
A complex sine wave can also be used instead of a sine wave. ""","""I figured out how to derive a complex Fourier series from a Fourier series.""",,,"""I had learned Fourier transforms in my second year, so this was a good opportunity to remind myself of that.""",1
D-2021_U94,"""I learned that periodic signals can be represented by Fourier series, and that periodic signals can be represented by superposition of sine and cosine waves.""","""I understood the Fourier series because there was a lot of refresher content.""","""There were some things I couldn't do because I forgot about specific calculations.""",,,1
D-2021_U96,,,,,"""I felt that it is necessary to be careful not to make a mistake in the calculation because the replacement integral is frequently used in the integral.""",1
D-2021_U97,,"""bn=0 for even period signal, a0=an=0 for odd period signal
Fourier series can be calculated with various periodic signals. """,,,,1
D-2021_U98,"""Introduction of Fourier transform from Fourier series""","""By viewing aperiodic functions as functions with an infinite period, I understood the desire to express aperiodic functions as sums of series of trigonometric functions.
I was able to feel what happens when the period is made infinite by expression transformation. ""","""Since the class was so fast, I found it difficult to understand while taking notes.""",,,-1
D-2021_U99,,"""I did my best to derive the Fourier coefficients.""",,,,1
D-2022_U1,,"""It was confirmed that Fourier coefficients can be obtained from orthogonality.""","""I only vaguely understood what the spectrum meant.""",,,0
D-2022_U10,"""Various Fourier transforms and their properties""","""Representative Fourier Transforms and Properties""",,,,0
D-2022_U14,,,"""On DC components of spectra and Fourier series""",,,1
D-2022_U20,"""How to Represent Periodic Functions Using Fourier Series
""",,"""How to find Fourier series for even and odd signals""",,,-2
D-2022_U21,"""Computation methods for Fourier series and complex Fourier series""",,,"""In cn, n may be a fraction, so I was wondering if c0 should be calculated separately and written before sigma. I wrote it in the assignment.""",,1
D-2022_U25,"""A periodic signal can be represented by a Fourier series. Fourier coefficients can be obtained by using the orthogonality of trigonometric functions. Any periodic waveform can be obtained by combining sine and cosine waves. Complex Fourier The series is represented by a complex sine wave instead of a sine wave.""","""Arbitrary waveforms are obtained by combining sinusoids and prophetic waves of different frequencies.""","""I don't know when the Fourier series is used and when the complex Fourier series is used""",,,1
D-2022_U26,""" Graphical Meaning of Fourier Series and How to Express Fourier Series""","""I understand that the signal wave is a collection of sine waves and cosine waves and how to express the Fourier series.""",,,,1
D-2022_U29,,"""The Meaning of Fourier Series Expansion, Derivation""",,,,1
D-2022_U30,,"""I learned how to calculate the Fourier series.""","""I had some trouble deriving the Fourier coefficients.""",,,1
D-2022_U31,"""A periodic signal can be represented by a Fourier series.
A Fourier series can be expressed as a complex Fourier series with a mathematical transformation. """,,"""I had some trouble with the exercise due to incorrect calculations.""",,,0
D-2022_U32,"""Representation of Periodic Signals by Fourier Series and Its Calculation""","""Periodic signals can be represented and computed by Fourier series, and the computation can be simplified by using complex Fourier series.""",,,,1
D-2022_U33,"""I reviewed Fourier series.""",,,,,0
D-2022_U35,"""When the signal x(t) is a periodic function, x(t) can be represented by the Fourier series of the trigonometric functions, and by applying Euler's formula, it can also be represented by the complex Fourier series.""",,,,,1
D-2022_U36,,,,,"""It was nostalgic.""",1
D-2022_U39,"""If you increase the period of a periodic signal to infinity, it becomes an aperiodic signal.
The Fourier transform has various properties.
Parseval's equation is an equation expressing that the energy in the time domain is equal to the energy in the frequency domain. ""","""Properties of the Fourier Transform""","""The process from the Fourier series to the Fourier transform was ambiguous for me.""",,,1
D-2022_U40,,"""An idea about periodic signals using Fourier series""","""About Spectral Properties""",,,1
D-2022_U41,"""Periodic signals can be represented by Fourier series""",,,,"""I couldn't do the Fourier transform, so I want to review it properly.
""",0
D-2022_U42,"""I learned about the content of the Fourier series and its relationship to the signal""",,"""It's an exercise, but I'd like to do some sort of proof of the Fourier series.""",,"""I learned about the Fourier series itself in other lectures and practiced it, but it was good that I was able to understand what it meant.""",1
D-2022_U43,,"""Fourier coefficients can now be obtained in complex form.""",,,"""It was relatively easy to understand because it was the content I had taken since my second year.""",1
D-2022_U45,"""Fourier Series Representation of Periodic Signals""",,,"""It's like writing class notes by hand.
I'm writing in a handwritten notebook on the ipad, is that okay? ""","""Sharing expression variants was useful
I want to calculate it myself and fix it.""",1
D-2022_U47,"""Methods of expressing waves by superposition of trigonometric and exponential functions""",,,,"""I was able to review the Fourier expansion that I dealt with in mathematics before.""",1
D-2022_U51,,,"""I still don't fully understand why the Fourier coefficients are represented by the formulas I just learned.""",,"""In today's class, the professor was actually proving the equation, and I wish I could have moved my hands instead of just looking at it.""",1
D-2022_U54,"""I learned Fourier series once before, but I forgot.""",,,,,1
D-2022_U55,"""A periodic signal can be approximated by a series by superposition of trigonometric functions. By handling complex numbers, it can be extended to complex Fourier series, which is easy to calculate.""","""I understood the content perfectly and was able to tackle the task.""",,,,1
D-2022_U56,,"""The meaning of Fourier series, how to find Fourier series.""",,,"""I had learned about Fourier series in other classes, so I was able to review it.""",1
D-2022_U57,"""I learned about Fourier series of periodic signals.""","""It turns out that all signals can be represented by a composition of sine waves. Also, using complex Fourier series makes the representation simpler. However, it is a bit cumbersome to do the calculation by hand.""",,,"""Continuing from last time, I was able to listen to the things that bothered me and important things during the class while taking notes. It's been a long time since I've calculated Fourier series, so I was able to remember them.""",1
D-2022_U58,"""Fourier Series Representation of Periodic Signals, Complex Fourier Series Representation and Spectrum""","""I learned how to represent periodic signals using Fourier series and complex Fourier series.""",,"""Nothing in particular""","""The Fourier series you learned in this class is very important, so I want you to understand it properly.""",1
D-2022_U60,"""Description of Fourier Series""","""I was able to review Fourier series.""","""I didn't really understand the meaning of the spectrum.""",,,1
D-2022_U61,"""I learned how to use Fourier series and complex Fourier series, and it became a concrete method for signal processing.""",,"""Complex Fourier series still didn't solve smoothly.""",,"""I want to be prepared to handle both Fourier series and complex Fourier series patterns.""",1
D-2022_U62,,"""How are Fourier series and complex Fourier series derived?""","""I understand how to find it, but I'm not good at calculations.
""",,,1
D-2022_U65,"""Understanding and Exercise of Fourier Series""","""I was able to answer the exercise using the formula""",,,,0
D-2022_U66,,,,,"""I learned about Fourier series in other classes, but there were some things I didn't remember, so I'm glad I was able to remember them.""",-1
D-2022_U67,"""Fourier transform can express complex sinusoidal amplitude contained in non-periodic signals""","""Expression of non-periodic signals by Fourier transform""",,,"""Fourier transform is difficult even though I learned it in other classes.""",0
D-2022_U68,,"""I understood that Fourier series expansion can be used for signal waveforms.
The reason why complex Fourier series is used in complex expression is because differentiation of exponential functions is faster than differentiation of trigonometric functions. """,,,,1
D-2022_U69,,"""How to find Fourier series and complex Fourier series""","""Fourier series and complex Fourier series were obtained with the same wave, but the values ​​did not match""","""I calculated the Fourier series in Exercises 2-3 to 2-5, but the values ​​of the complex Fourier series are all half the values ​​of the Fourier series.""","""I submitted the assignment at the last minute""",1
D-2022_U7,"""Fourier Series Representation of Periodic Signals""","""I was able to understand the meaning of the Fourier series.""","""I didn't know how to use calculations, formulas, etc.""",,"""I thought that the content was finally getting difficult.""",1
D-2022_U72,,,"""How to use spectrum""",,"""The explanation of Fourier series was concise and easy to understand""",1
D-2022_U73,,"""The signal should be expressed as an expression using the Fourier transform
Structure of the Fourier Transform""",,,,-2
D-2022_U74,,"""It reminded me a little of the Fourier series""",,,,1
D-2022_U77,,,"""I think I understand how to find Fourier coefficients, but I tend to get stuck in calculations.""",,"""It was roughly what I did in the first half of my second year, but there are many things I forgot, so I want to review it.""",1
D-2022_U8,,,"""I didn't quite understand the complex Fourier series.""",,,1
D-2022_U80,,,,,"""Okay""",0
D-2022_U81,,,,,"""I forgot how to find the Fourier series, which I did before, so I thought I'd like to study it again.""",-2
D-2022_U82,"""Any signal can be represented by the sum of sinusoids. Using the complex Fourier series simplifies the calculation.""",,,,,-1
D-2022_U83,,"""How to find Fourier series""",,,,1
D-2022_U84,"""About Fourier Series""",,,,,1
D-2022_U85,"""Fourier series, complex Fourier series, and the meaning of the word spectrum.""","""I was able to review Fourier series.""",,,,1
D-2022_U87,,"""I understood from the figure that even a square wave or a sawtooth wave can be roughly approximated by synthesizing trigonometric functions.
""","""I don't understand theoretically the proofs and derivations of formulas, so I would like to review them.""","""I attended the class from the beginning to the end, but the reloading of the yellow moodle was delayed, and it was treated as being late.
It seems to be processed correctly in the blue moodle, so please refer to this when evaluating attendance.
We apologize for the inconvenience, but thank you for your consideration.
""",,1
D-2022_U89,"""This time I learned about Fourier series. I learned how to find Fourier series and the main points of Fourier series.""",,,,"""In this class, we learned about the Fourier series. The Fourier series expresses an arbitrary signal by superposing sine waves, and I think it is widely used in the field of engineering. I wanted to thoroughly review the contents of this lecture.""",1
D-2022_U9,"""Today I was able to learn about the Fourier transform.""","""Fourier transforms came in easily because I learned them in math.""","""I felt that my understanding of the complex Fourier transform was not sufficient.""",,,1
D-2022_U90,"""A periodic signal can be represented by a Fourier series. That is, it can be represented by the sum of various sinusoids. A Fourier series can be transformed into a complex Fourier series using Euler's formula. A complex Fourier series is better. It can be expressed more concisely than the Fourier series.""",,,,,1
D-2022_U92,"""How to obtain the Fourier series of periodic signals, in other words, easy-to-understand signals. How to express Fourier series using complex numbers and trigonometric functions.""","""I understand how to find the Fourier series.""","""I didn't quite understand the Fourier series of complex numbers and trigonometric functions.""",,,1
D-2022_U93,"""Fourier Series Expansion Display of Periodic Signals""",,,,,1
